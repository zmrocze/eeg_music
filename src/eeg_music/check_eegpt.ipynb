{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "markdown-header",
   "metadata": {},
   "source": [
    "# EEGPT Model Loading and Usage Demo\n",
    "\n",
    "This notebook demonstrates how to load and use the pretrained EEGPT model for EEG feature extraction.\n",
    "EEGPT is a 10-million-parameter transformer designed for universal EEG representation learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7711f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmrocze/studia/uwr/magisterka/.devenv/state/venv/lib/python3.12/site-packages/downstream/Modules/models/EEGPT_mcae_finetune.py:679: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(True)\n",
      "/home/zmrocze/studia/uwr/magisterka/.devenv/state/venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/home/zmrocze/studia/uwr/magisterka/.devenv/state/venv/lib/python3.12/site-packages/downstream/Modules/models/EEGPT_mcae_finetune.py:693: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGPTClassifier(\n",
       "  (chan_conv): Sequential(\n",
       "    (0): Conv1dWithConstraint(28, 28, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (target_encoder): EEGTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 512, kernel_size=(1, 64), stride=(1, 64))\n",
       "    )\n",
       "    (chan_embed): Embedding(62, 512)\n",
       "    (blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (predictor): EEGTransformerPredictor(\n",
       "    (predictor_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (time_embed): RotaryEmbedding()\n",
       "    (predictor_blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predictor_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    (predictor_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): LinearWithConstraint(in_features=512, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eegpt\n",
    "\n",
    "# Make sure to replace this with the actual path to your checkpoint\n",
    "checkpoint_path = \"./model_checkpoints/25866970/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt\" \n",
    "\n",
    "model = eegpt.load_model(checkpoint_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the network: 33024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Define the network\n",
    "class ResidualLinear(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "      super(ResidualLinear, self).__init__()\n",
    "      self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "      self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "      residual = x # 128\n",
    "      x = torch.relu(self.linear1(x)) # 128\n",
    "      x += residual # 128\n",
    "      x = self.linear2(x) # 128\n",
    "      return x\n",
    "\n",
    "# Create an instance with the given dimensions\n",
    "model = ResidualLinear(128, 128, 128)\n",
    "\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the network: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1ef3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmrocze/studia/uwr/magisterka/.devenv/state/venv/lib/python3.12/site-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  available_backends = torchaudio.list_audio_backends()\n",
      "/home/zmrocze/studia/uwr/magisterka/.devenv/state/venv/lib/python3.12/site-packages/downstream/Modules/models/EEGPT_mcae_finetune.py:679: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(True)\n",
      "/home/zmrocze/studia/uwr/magisterka/.devenv/state/venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/home/zmrocze/studia/uwr/magisterka/.devenv/state/venv/lib/python3.12/site-packages/downstream/Modules/models/EEGPT_mcae_finetune.py:693: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51135660\n"
     ]
    }
   ],
   "source": [
    "from src.training import EegptConfig, EegptLightning, config, count_n_params\n",
    "\n",
    "eegpt_config = EegptConfig(\n",
    "    chpt_path=config.eegpt_chpt_path, lr_config=config.lr_config\n",
    "  )\n",
    "model = EegptLightning(eegpt_config)\n",
    "print(count_n_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702d53f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINABLE PARAMETERS:\n",
      "================================================================================\n",
      "chan_conv                     :        812 /        812 params  ✓ TRAINABLE\n",
      "head                          :     65,664 /     65,664 params  ✓ TRAINABLE\n",
      "linear (ResidualLinear)       :     33,024 /     33,024 params  ✓ TRAINABLE\n",
      "target_encoder                :          0 / 25,287,168 params  ✗ FROZEN\n",
      "reconstructor/predictor       :          0 / 25,747,984 params  ✗ FROZEN\n",
      "--------------------------------------------------------------------------------\n",
      "TOTAL TRAINABLE               :     99,500 params\n",
      "TOTAL FROZEN                  : 51,035,152 params\n",
      "TOTAL                         : 51,134,652 params\n",
      "Trainable ratio               : 0.19%\n",
      "================================================================================\n",
      "\n",
      "WHAT IS TRAINABLE:\n",
      "  ✓ chan_conv: Channel-wise 1D convolution (adapts input channels to model)\n",
      "  ✓ head: Final linear layer in EEGPTClassifier (maps embeddings to num_classes)\n",
      "  ✓ linear (ResidualLinear): Two-layer residual network (linear1 + linear2)\n",
      "    - linear1: input_dim -> input_dim with residual connection\n",
      "    - linear2: input_dim -> output_dim (final projection)\n",
      "\n",
      "WHAT IS FROZEN:\n",
      "  ✗ target_encoder: Pretrained EEG encoder (patch embedding, transformer blocks)\n",
      "  ✗ reconstructor/predictor: Pretrained reconstruction/prediction head\n",
      "\n",
      "NOTE: You might also want to make trainable:\n",
      "  - norm/fc_norm: Layer normalization before the head\n",
      "  - chan_embed: Channel embedding layer in target_encoder\n",
      "  - cls_token/summary_token: Learnable tokens\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from freeze_utils import freeze_all_except_head_and_adapters\n",
    "freeze_all_except_head_and_adapters(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62f3d56",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1758644730715 failed, reason: connect ECONNREFUSED 127.0.0.1:8888).)."
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = './datasets/bcmi/bcmi-calibration/stimuli/hvha10.wav'\n",
    "audio, sr = librosa.load(audio_path, sr=None)\n",
    "print(f'Audio shape: {audio.shape}')\n",
    "print(f'Sample rate: {sr} Hz')\n",
    "print(f'Duration: {len(audio) / sr:.2f} seconds') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e117b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "24\n",
      "58\n",
      "58\n",
      "{'FZ', 'FCZ', 'OZ', 'FPZ', 'CPZ', 'POZ', 'PZ', 'CZ'}\n",
      "{'Pz', 'Oz', 'FPz', 'CPz', 'Fz', 'FCz', 'POz', 'Cz'}\n",
      "     \n",
      "['FT9', 'FT10', 'TP9', 'TP10']\n",
      "['FT9', 'FT10', 'TP9', 'TP10']\n",
      "['AF7', 'AF8', 'PO5', 'PO6']\n",
      "[]\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:53: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:53: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/tmp/ipykernel_8319/3578042516.py:53: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(channels_calibration == channels_training, \"Calibration and training channels differ!\")\n"
     ]
    }
   ],
   "source": [
    "channels_training = [\"FP1\", \"FPz\", \"FP2\", \"F7\", \"F3\", \"Fz\", \"F4\", \"F8\", \"FT9\", \"FC5\", \"FC1\", \"FC2\", \n",
    "  \"FC6\", \"FT10\", \"T7\", \"C3\", \"Cz\", \"C4\", \"T8\", \"TP9\", \"CP5\", \"CP1\", \"CP2\", \"CP6\", \"TP10\", \"P7\", \n",
    "  \"P3\", \"Pz\", \"P4\", \"P8\", \"O1\", \"O2\",\n",
    "  ]\n",
    "\n",
    "finetuning_all_ch = [      'FP1', 'FPZ', 'FP2', \n",
    "                        \"AF7\", 'AF3', 'AF4', \"AF8\", \n",
    "            'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', \n",
    "                      'PO7', \"PO5\", 'PO3', 'POZ', 'PO4', \"PO6\", 'PO8', \n",
    "                               'O1', 'OZ', 'O2'\n",
    "                    ]\n",
    "\n",
    "\n",
    "channels_calibration = [\"FP1\" ,\"FPz\" ,\"FP2\" ,\"F7\" ,\"F3\" ,\"Fz\" ,\"F4\" ,\"F8\" ,\"FT9\" ,\"FC5\" ,\"FC1\" ,\"FC2\",\n",
    "  \"FC6\" ,\"FT10\" ,\"T7\" ,\"C3\" ,\"Cz\" ,\"C4\" ,\"T8\" ,\"TP9\" ,\"CP5\" ,\"CP1\" ,\"CP2\" ,\"CP6\" ,\"TP10\" ,\"P7\" \n",
    "  ,\"P3\" ,\"Pz\" ,\"P4\" ,\"P8\" ,\"O1\" ,\"O2\"\n",
    "]\n",
    "\n",
    "all_in_pretraining = [\n",
    "  \"FP2\", \"FPz\", \"FP1\", \"AF4\", \"AF3\", \"F7\", \"F5\", \"F3\", \"F6\", \"F1\", \"Fz\", \"F2\", \"F4\", \"F8\", \"FT7\", \"FC5\", \"FC3\", \n",
    "  \"FC6\", \"FC1\", \"FCz\", \"FC2\", \"FC4\", \"FT8\", \"T7\", \"C5\", \"C3\", \"C6\", \"C1\", \"Cz\", \"C2\", \n",
    "  \"C4\", \"T8\", \"TP7\", \"CP5\", \"CP3\", \"CP6\", \"CP1\", \"CPz\", \"CP2\", \"CP4\", \"TP8\", \"P7\", \"P5\", \"P3\", \"P6\", \n",
    "  \"P1\", \"Pz\", \"P2\", \"P4\", \"P8\", \"O1\", \"PO7\", \"PO3\", \"O2\", \"Oz\", \"PO4\", \"PO8\", \"POz\"\n",
    "]\n",
    "\n",
    "all_3 = [  'FP1', 'FPZ', 'FP2',\n",
    "                               'AF3', 'AF4', \n",
    "            'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', \n",
    "                      'PO7', 'PO3', 'POZ',  'PO4', 'PO8', \n",
    "                               'O1', 'OZ', 'O2', ]\n",
    "\n",
    "all_4 =                     [      'FP1', 'FPZ', 'FP2', \n",
    "                        \"AF7\", 'AF3', 'AF4', \"AF8\", \n",
    "            'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', \n",
    "                      'PO7', \"PO5\", 'PO3', 'POZ', 'PO4', \"PO6\", 'PO8', \n",
    "                               'O1', 'OZ', 'O2', ]\n",
    "\n",
    "# these are extra:   #  \"GSR\", \"ECG\", \"VA1\", \"VA2\", \"VAtarg\"]\n",
    "print(len(channels_calibration)) # 37\n",
    "print(len(channels_training)) # 37\n",
    "assert(channels_calibration == channels_training, \"Calibration and training channels differ!\")\n",
    "using_channels = set(channels_calibration).intersection(set(finetuning_all_ch))\n",
    "print(len(using_channels))\n",
    "print(len(all_3))\n",
    "print(len(all_in_pretraining))\n",
    "print(set(all_3) - set(all_in_pretraining))\n",
    "print(set(all_in_pretraining) - set(all_3))\n",
    "print(\"     \")\n",
    "print(list(filter( lambda i : i.upper().strip('.') not in all_4, channels_calibration)))\n",
    "print(list(filter( lambda i : i.upper().strip('.') not in all_3, channels_calibration)))\n",
    "\n",
    "print(list(filter( lambda i : i.upper().strip('.') not in all_3, all_4)))\n",
    "print(list(filter( lambda i : i.upper().strip('.') not in all_4, all_3)))\n",
    "print(set(all_4) == set(all_3))\n",
    "print(set(all_4) == set(all_in_pretraining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbc28cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 'FT9'), (13, 'FT10'), (19, 'TP9'), (24, 'TP10')]\n"
     ]
    }
   ],
   "source": [
    "not_in_finetuned_but_in_ours = {'TP9', 'TP10', 'FT9', 'FT10'}\n",
    "print([(i, ch) for i, ch in enumerate(channels_calibration) if ch.upper() not in finetuning_all_ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d33b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PO5', 'AF7', 'AF8', 'PO6'}\n",
      "set()\n",
      "62\n",
      "58\n",
      "\n",
      "{'FT10', 'FT9', 'TP10', 'TP9'}\n"
     ]
    }
   ],
   "source": [
    "finetuning_all_ch = [      'FP1', 'FPZ', 'FP2', \n",
    "                        \"AF7\", 'AF3', 'AF4', \"AF8\", \n",
    "            'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', \n",
    "                      'PO7', \"PO5\", 'PO3', 'POZ', 'PO4', \"PO6\", 'PO8', \n",
    "                               'O1', 'OZ', 'O2']\n",
    "\n",
    "pretraining_all_ch = [      'FP1', 'FPZ', 'FP2', \n",
    "                               'AF3', 'AF4', \n",
    "            'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', \n",
    "                      'PO7', 'PO3', 'POZ',  'PO4', 'PO8', \n",
    "                               'O1', 'OZ', 'O2' ]\n",
    "\n",
    "print(set(finetuning_all_ch) - set(pretraining_all_ch))\n",
    "print(set(pretraining_all_ch) - set(finetuning_all_ch))\n",
    "print(len(finetuning_all_ch))\n",
    "print(len(pretraining_all_ch))\n",
    "\n",
    "print(\"\")\n",
    "print(f\"{set([ ch.upper() for ch in channels_calibration]) - set(finetuning_all_ch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749e7e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "FT9",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m channels_calibration\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mEEGPTClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchannels_calibration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchannels_calibration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_channels_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_calibration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_chan_conv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_predictor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# mising: target_encoder, predictor, fc, head\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load the pretrained weights\u001b[39;00m\n\u001b[32m     14\u001b[39m checkpoint = torch.load(\u001b[33m\"\u001b[39m\u001b[33m./model_checkpoints/25866970/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     15\u001b[39m                         map_location=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/magisterka/submodules/EEGPT/downstream_tueg/Modules/models/EEGPT_mcae_finetune_change.py:694\u001b[39m, in \u001b[36mEEGPTClassifier.__init__\u001b[39m\u001b[34m(self, num_classes, in_channels, img_size, patch_stride, use_channels_names, use_mean_pooling, norm_layer, use_chan_conv, max_norm_chan_conv, **kwargs)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28mself\u001b[39m.target_encoder = target_encoder\n\u001b[32m    693\u001b[39m \u001b[38;5;28mself\u001b[39m.reconstructor  = reconstructor\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m \u001b[38;5;28mself\u001b[39m.chans_id       = \u001b[43mtarget_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_chan_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_channels_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m embed_dim = \u001b[32m512\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28mself\u001b[39m.embed_dim = embed_dim\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/magisterka/submodules/EEGPT/downstream_tueg/Modules/models/EEGPT_mcae_finetune_change.py:478\u001b[39m, in \u001b[36mEEGTransformer.prepare_chan_ids\u001b[39m\u001b[34m(self, channels)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m channels:\n\u001b[32m    477\u001b[39m     ch = ch.upper().strip(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m CHANNEL_DICT, ch\n\u001b[32m    479\u001b[39m     chan_ids.append(CHANNEL_DICT[ch])\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(chan_ids).unsqueeze_(\u001b[32m0\u001b[39m).long()\n",
      "\u001b[31mAssertionError\u001b[39m: FT9"
     ]
    }
   ],
   "source": [
    "\n",
    "from submodules.EEGPT.downstream_tueg.Modules.models.EEGPT_mcae_finetune_change import EEGPTClassifier\n",
    "import torch\n",
    "\n",
    "channels_calibration\n",
    "\n",
    "model = EEGPTClassifier(4, \n",
    "    in_channels=len(channels_calibration), img_size=[len(channels_calibration), 4 * 256], \n",
    "    use_channels_names=channels_calibration, use_chan_conv=True, use_predictor=True\n",
    "  )\n",
    "\n",
    "# mising: target_encoder, predictor, fc, head\n",
    "\n",
    "# Load the pretrained weights\n",
    "checkpoint = torch.load(\"./model_checkpoints/25866970/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt\", \n",
    "                        map_location='cpu', weights_only=False)\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)  # strict=False to allow new classification head\n",
    "\n",
    "model(torch.randn(2, len(channels_calibration), 4 * 256))  # (batch, channels, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'EEGPT_mcae_finetune_change'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Add EEGPT to path\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# sys.path.append(str(Path('..') / 'EEGPT' / 'downstream_tueg' / 'Modules' / 'models'))\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEEGPT_mcae_finetune_change\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGPTClassifier, CHANNEL_DICT\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'EEGPT_mcae_finetune_change'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add EEGPT to path\n",
    "# sys.path.append(str(Path('..') / 'EEGPT' / 'downstream_tueg' / 'Modules' / 'models'))\n",
    "from EEGPT_mcae_finetune_change import EEGPTClassifier, CHANNEL_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc24ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import EEGPT_mcae_finetune_change\n",
    "import sys\n",
    "# import submodules.EEGPT\n",
    "from pretrain.modeling_pretraining import EEGTransformer\n",
    "import torch\n",
    "\n",
    "chkpt_path = 'model_checkpoints/25866970/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt'\n",
    "checkpoint = torch.load(chkpt_path, map_location='cpu', weights_only=False)\n",
    "# model = EEGTransformer()\n",
    "# model.load_state_dict(checkpoint['model'] if 'model' in checkpoint else checkpoint)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6cbc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'MixedPrecisionPlugin'])\n",
      "odict_keys(['encoder.summary_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.chan_embed.weight', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.qkv.bias', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.qkv.bias', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.qkv.bias', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.qkv.bias', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.qkv.bias', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.qkv.bias', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.qkv.bias', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.qkv.bias', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'target_encoder.summary_token', 'target_encoder.patch_embed.proj.weight', 'target_encoder.patch_embed.proj.bias', 'target_encoder.chan_embed.weight', 'target_encoder.blocks.0.norm1.weight', 'target_encoder.blocks.0.norm1.bias', 'target_encoder.blocks.0.attn.qkv.weight', 'target_encoder.blocks.0.attn.qkv.bias', 'target_encoder.blocks.0.attn.proj.weight', 'target_encoder.blocks.0.attn.proj.bias', 'target_encoder.blocks.0.norm2.weight', 'target_encoder.blocks.0.norm2.bias', 'target_encoder.blocks.0.mlp.fc1.weight', 'target_encoder.blocks.0.mlp.fc1.bias', 'target_encoder.blocks.0.mlp.fc2.weight', 'target_encoder.blocks.0.mlp.fc2.bias', 'target_encoder.blocks.1.norm1.weight', 'target_encoder.blocks.1.norm1.bias', 'target_encoder.blocks.1.attn.qkv.weight', 'target_encoder.blocks.1.attn.qkv.bias', 'target_encoder.blocks.1.attn.proj.weight', 'target_encoder.blocks.1.attn.proj.bias', 'target_encoder.blocks.1.norm2.weight', 'target_encoder.blocks.1.norm2.bias', 'target_encoder.blocks.1.mlp.fc1.weight', 'target_encoder.blocks.1.mlp.fc1.bias', 'target_encoder.blocks.1.mlp.fc2.weight', 'target_encoder.blocks.1.mlp.fc2.bias', 'target_encoder.blocks.2.norm1.weight', 'target_encoder.blocks.2.norm1.bias', 'target_encoder.blocks.2.attn.qkv.weight', 'target_encoder.blocks.2.attn.qkv.bias', 'target_encoder.blocks.2.attn.proj.weight', 'target_encoder.blocks.2.attn.proj.bias', 'target_encoder.blocks.2.norm2.weight', 'target_encoder.blocks.2.norm2.bias', 'target_encoder.blocks.2.mlp.fc1.weight', 'target_encoder.blocks.2.mlp.fc1.bias', 'target_encoder.blocks.2.mlp.fc2.weight', 'target_encoder.blocks.2.mlp.fc2.bias', 'target_encoder.blocks.3.norm1.weight', 'target_encoder.blocks.3.norm1.bias', 'target_encoder.blocks.3.attn.qkv.weight', 'target_encoder.blocks.3.attn.qkv.bias', 'target_encoder.blocks.3.attn.proj.weight', 'target_encoder.blocks.3.attn.proj.bias', 'target_encoder.blocks.3.norm2.weight', 'target_encoder.blocks.3.norm2.bias', 'target_encoder.blocks.3.mlp.fc1.weight', 'target_encoder.blocks.3.mlp.fc1.bias', 'target_encoder.blocks.3.mlp.fc2.weight', 'target_encoder.blocks.3.mlp.fc2.bias', 'target_encoder.blocks.4.norm1.weight', 'target_encoder.blocks.4.norm1.bias', 'target_encoder.blocks.4.attn.qkv.weight', 'target_encoder.blocks.4.attn.qkv.bias', 'target_encoder.blocks.4.attn.proj.weight', 'target_encoder.blocks.4.attn.proj.bias', 'target_encoder.blocks.4.norm2.weight', 'target_encoder.blocks.4.norm2.bias', 'target_encoder.blocks.4.mlp.fc1.weight', 'target_encoder.blocks.4.mlp.fc1.bias', 'target_encoder.blocks.4.mlp.fc2.weight', 'target_encoder.blocks.4.mlp.fc2.bias', 'target_encoder.blocks.5.norm1.weight', 'target_encoder.blocks.5.norm1.bias', 'target_encoder.blocks.5.attn.qkv.weight', 'target_encoder.blocks.5.attn.qkv.bias', 'target_encoder.blocks.5.attn.proj.weight', 'target_encoder.blocks.5.attn.proj.bias', 'target_encoder.blocks.5.norm2.weight', 'target_encoder.blocks.5.norm2.bias', 'target_encoder.blocks.5.mlp.fc1.weight', 'target_encoder.blocks.5.mlp.fc1.bias', 'target_encoder.blocks.5.mlp.fc2.weight', 'target_encoder.blocks.5.mlp.fc2.bias', 'target_encoder.blocks.6.norm1.weight', 'target_encoder.blocks.6.norm1.bias', 'target_encoder.blocks.6.attn.qkv.weight', 'target_encoder.blocks.6.attn.qkv.bias', 'target_encoder.blocks.6.attn.proj.weight', 'target_encoder.blocks.6.attn.proj.bias', 'target_encoder.blocks.6.norm2.weight', 'target_encoder.blocks.6.norm2.bias', 'target_encoder.blocks.6.mlp.fc1.weight', 'target_encoder.blocks.6.mlp.fc1.bias', 'target_encoder.blocks.6.mlp.fc2.weight', 'target_encoder.blocks.6.mlp.fc2.bias', 'target_encoder.blocks.7.norm1.weight', 'target_encoder.blocks.7.norm1.bias', 'target_encoder.blocks.7.attn.qkv.weight', 'target_encoder.blocks.7.attn.qkv.bias', 'target_encoder.blocks.7.attn.proj.weight', 'target_encoder.blocks.7.attn.proj.bias', 'target_encoder.blocks.7.norm2.weight', 'target_encoder.blocks.7.norm2.bias', 'target_encoder.blocks.7.mlp.fc1.weight', 'target_encoder.blocks.7.mlp.fc1.bias', 'target_encoder.blocks.7.mlp.fc2.weight', 'target_encoder.blocks.7.mlp.fc2.bias', 'target_encoder.norm.weight', 'target_encoder.norm.bias', 'predictor.mask_token', 'predictor.predictor_embed.weight', 'predictor.predictor_embed.bias', 'predictor.time_embed.freqs', 'predictor.predictor_blocks.0.norm1.weight', 'predictor.predictor_blocks.0.norm1.bias', 'predictor.predictor_blocks.0.attn.qkv.weight', 'predictor.predictor_blocks.0.attn.qkv.bias', 'predictor.predictor_blocks.0.attn.proj.weight', 'predictor.predictor_blocks.0.attn.proj.bias', 'predictor.predictor_blocks.0.norm2.weight', 'predictor.predictor_blocks.0.norm2.bias', 'predictor.predictor_blocks.0.mlp.fc1.weight', 'predictor.predictor_blocks.0.mlp.fc1.bias', 'predictor.predictor_blocks.0.mlp.fc2.weight', 'predictor.predictor_blocks.0.mlp.fc2.bias', 'predictor.predictor_blocks.1.norm1.weight', 'predictor.predictor_blocks.1.norm1.bias', 'predictor.predictor_blocks.1.attn.qkv.weight', 'predictor.predictor_blocks.1.attn.qkv.bias', 'predictor.predictor_blocks.1.attn.proj.weight', 'predictor.predictor_blocks.1.attn.proj.bias', 'predictor.predictor_blocks.1.norm2.weight', 'predictor.predictor_blocks.1.norm2.bias', 'predictor.predictor_blocks.1.mlp.fc1.weight', 'predictor.predictor_blocks.1.mlp.fc1.bias', 'predictor.predictor_blocks.1.mlp.fc2.weight', 'predictor.predictor_blocks.1.mlp.fc2.bias', 'predictor.predictor_blocks.2.norm1.weight', 'predictor.predictor_blocks.2.norm1.bias', 'predictor.predictor_blocks.2.attn.qkv.weight', 'predictor.predictor_blocks.2.attn.qkv.bias', 'predictor.predictor_blocks.2.attn.proj.weight', 'predictor.predictor_blocks.2.attn.proj.bias', 'predictor.predictor_blocks.2.norm2.weight', 'predictor.predictor_blocks.2.norm2.bias', 'predictor.predictor_blocks.2.mlp.fc1.weight', 'predictor.predictor_blocks.2.mlp.fc1.bias', 'predictor.predictor_blocks.2.mlp.fc2.weight', 'predictor.predictor_blocks.2.mlp.fc2.bias', 'predictor.predictor_blocks.3.norm1.weight', 'predictor.predictor_blocks.3.norm1.bias', 'predictor.predictor_blocks.3.attn.qkv.weight', 'predictor.predictor_blocks.3.attn.qkv.bias', 'predictor.predictor_blocks.3.attn.proj.weight', 'predictor.predictor_blocks.3.attn.proj.bias', 'predictor.predictor_blocks.3.norm2.weight', 'predictor.predictor_blocks.3.norm2.bias', 'predictor.predictor_blocks.3.mlp.fc1.weight', 'predictor.predictor_blocks.3.mlp.fc1.bias', 'predictor.predictor_blocks.3.mlp.fc2.weight', 'predictor.predictor_blocks.3.mlp.fc2.bias', 'predictor.predictor_blocks.4.norm1.weight', 'predictor.predictor_blocks.4.norm1.bias', 'predictor.predictor_blocks.4.attn.qkv.weight', 'predictor.predictor_blocks.4.attn.qkv.bias', 'predictor.predictor_blocks.4.attn.proj.weight', 'predictor.predictor_blocks.4.attn.proj.bias', 'predictor.predictor_blocks.4.norm2.weight', 'predictor.predictor_blocks.4.norm2.bias', 'predictor.predictor_blocks.4.mlp.fc1.weight', 'predictor.predictor_blocks.4.mlp.fc1.bias', 'predictor.predictor_blocks.4.mlp.fc2.weight', 'predictor.predictor_blocks.4.mlp.fc2.bias', 'predictor.predictor_blocks.5.norm1.weight', 'predictor.predictor_blocks.5.norm1.bias', 'predictor.predictor_blocks.5.attn.qkv.weight', 'predictor.predictor_blocks.5.attn.qkv.bias', 'predictor.predictor_blocks.5.attn.proj.weight', 'predictor.predictor_blocks.5.attn.proj.bias', 'predictor.predictor_blocks.5.norm2.weight', 'predictor.predictor_blocks.5.norm2.bias', 'predictor.predictor_blocks.5.mlp.fc1.weight', 'predictor.predictor_blocks.5.mlp.fc1.bias', 'predictor.predictor_blocks.5.mlp.fc2.weight', 'predictor.predictor_blocks.5.mlp.fc2.bias', 'predictor.predictor_blocks.6.norm1.weight', 'predictor.predictor_blocks.6.norm1.bias', 'predictor.predictor_blocks.6.attn.qkv.weight', 'predictor.predictor_blocks.6.attn.qkv.bias', 'predictor.predictor_blocks.6.attn.proj.weight', 'predictor.predictor_blocks.6.attn.proj.bias', 'predictor.predictor_blocks.6.norm2.weight', 'predictor.predictor_blocks.6.norm2.bias', 'predictor.predictor_blocks.6.mlp.fc1.weight', 'predictor.predictor_blocks.6.mlp.fc1.bias', 'predictor.predictor_blocks.6.mlp.fc2.weight', 'predictor.predictor_blocks.6.mlp.fc2.bias', 'predictor.predictor_blocks.7.norm1.weight', 'predictor.predictor_blocks.7.norm1.bias', 'predictor.predictor_blocks.7.attn.qkv.weight', 'predictor.predictor_blocks.7.attn.qkv.bias', 'predictor.predictor_blocks.7.attn.proj.weight', 'predictor.predictor_blocks.7.attn.proj.bias', 'predictor.predictor_blocks.7.norm2.weight', 'predictor.predictor_blocks.7.norm2.bias', 'predictor.predictor_blocks.7.mlp.fc1.weight', 'predictor.predictor_blocks.7.mlp.fc1.bias', 'predictor.predictor_blocks.7.mlp.fc2.weight', 'predictor.predictor_blocks.7.mlp.fc2.bias', 'predictor.predictor_norm.weight', 'predictor.predictor_norm.bias', 'predictor.predictor_proj.weight', 'predictor.predictor_proj.bias', 'reconstructor.mask_token', 'reconstructor.reconstructor_embed.weight', 'reconstructor.reconstructor_embed.bias', 'reconstructor.time_embed.freqs', 'reconstructor.chan_embed.weight', 'reconstructor.reconstructor_blocks.0.norm1.weight', 'reconstructor.reconstructor_blocks.0.norm1.bias', 'reconstructor.reconstructor_blocks.0.attn.qkv.weight', 'reconstructor.reconstructor_blocks.0.attn.qkv.bias', 'reconstructor.reconstructor_blocks.0.attn.proj.weight', 'reconstructor.reconstructor_blocks.0.attn.proj.bias', 'reconstructor.reconstructor_blocks.0.norm2.weight', 'reconstructor.reconstructor_blocks.0.norm2.bias', 'reconstructor.reconstructor_blocks.0.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.0.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.0.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.0.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.1.norm1.weight', 'reconstructor.reconstructor_blocks.1.norm1.bias', 'reconstructor.reconstructor_blocks.1.attn.qkv.weight', 'reconstructor.reconstructor_blocks.1.attn.qkv.bias', 'reconstructor.reconstructor_blocks.1.attn.proj.weight', 'reconstructor.reconstructor_blocks.1.attn.proj.bias', 'reconstructor.reconstructor_blocks.1.norm2.weight', 'reconstructor.reconstructor_blocks.1.norm2.bias', 'reconstructor.reconstructor_blocks.1.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.1.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.1.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.1.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.2.norm1.weight', 'reconstructor.reconstructor_blocks.2.norm1.bias', 'reconstructor.reconstructor_blocks.2.attn.qkv.weight', 'reconstructor.reconstructor_blocks.2.attn.qkv.bias', 'reconstructor.reconstructor_blocks.2.attn.proj.weight', 'reconstructor.reconstructor_blocks.2.attn.proj.bias', 'reconstructor.reconstructor_blocks.2.norm2.weight', 'reconstructor.reconstructor_blocks.2.norm2.bias', 'reconstructor.reconstructor_blocks.2.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.2.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.2.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.2.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.3.norm1.weight', 'reconstructor.reconstructor_blocks.3.norm1.bias', 'reconstructor.reconstructor_blocks.3.attn.qkv.weight', 'reconstructor.reconstructor_blocks.3.attn.qkv.bias', 'reconstructor.reconstructor_blocks.3.attn.proj.weight', 'reconstructor.reconstructor_blocks.3.attn.proj.bias', 'reconstructor.reconstructor_blocks.3.norm2.weight', 'reconstructor.reconstructor_blocks.3.norm2.bias', 'reconstructor.reconstructor_blocks.3.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.3.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.3.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.3.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.4.norm1.weight', 'reconstructor.reconstructor_blocks.4.norm1.bias', 'reconstructor.reconstructor_blocks.4.attn.qkv.weight', 'reconstructor.reconstructor_blocks.4.attn.qkv.bias', 'reconstructor.reconstructor_blocks.4.attn.proj.weight', 'reconstructor.reconstructor_blocks.4.attn.proj.bias', 'reconstructor.reconstructor_blocks.4.norm2.weight', 'reconstructor.reconstructor_blocks.4.norm2.bias', 'reconstructor.reconstructor_blocks.4.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.4.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.4.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.4.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.5.norm1.weight', 'reconstructor.reconstructor_blocks.5.norm1.bias', 'reconstructor.reconstructor_blocks.5.attn.qkv.weight', 'reconstructor.reconstructor_blocks.5.attn.qkv.bias', 'reconstructor.reconstructor_blocks.5.attn.proj.weight', 'reconstructor.reconstructor_blocks.5.attn.proj.bias', 'reconstructor.reconstructor_blocks.5.norm2.weight', 'reconstructor.reconstructor_blocks.5.norm2.bias', 'reconstructor.reconstructor_blocks.5.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.5.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.5.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.5.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.6.norm1.weight', 'reconstructor.reconstructor_blocks.6.norm1.bias', 'reconstructor.reconstructor_blocks.6.attn.qkv.weight', 'reconstructor.reconstructor_blocks.6.attn.qkv.bias', 'reconstructor.reconstructor_blocks.6.attn.proj.weight', 'reconstructor.reconstructor_blocks.6.attn.proj.bias', 'reconstructor.reconstructor_blocks.6.norm2.weight', 'reconstructor.reconstructor_blocks.6.norm2.bias', 'reconstructor.reconstructor_blocks.6.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.6.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.6.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.6.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.7.norm1.weight', 'reconstructor.reconstructor_blocks.7.norm1.bias', 'reconstructor.reconstructor_blocks.7.attn.qkv.weight', 'reconstructor.reconstructor_blocks.7.attn.qkv.bias', 'reconstructor.reconstructor_blocks.7.attn.proj.weight', 'reconstructor.reconstructor_blocks.7.attn.proj.bias', 'reconstructor.reconstructor_blocks.7.norm2.weight', 'reconstructor.reconstructor_blocks.7.norm2.bias', 'reconstructor.reconstructor_blocks.7.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.7.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.7.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.7.mlp.fc2.bias', 'reconstructor.reconstructor_norm.weight', 'reconstructor.reconstructor_norm.bias', 'reconstructor.reconstructor_proj.weight', 'reconstructor.reconstructor_proj.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.keys())\n",
    "print(checkpoint['state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d95249f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpretrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine_pretraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LitEEGPT\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_eegpt_lightning\u001b[39m():\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Load the complete Lightning model\u001b[39;00m\n\u001b[32m      5\u001b[39m     model = LitEEGPT.load_from_checkpoint(\u001b[33m\"\u001b[39m\u001b[33mpath/to/checkpoint.ckpt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/magisterka/.devenv/state/venv/lib/python3.12/site-packages/pretrain/engine_pretraining.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcopy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loggers \u001b[38;5;28;01mas\u001b[39;00m pl_loggers\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WarmupCosineSchedule, CosineWDSchedule, grad_logger\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from pretrain.engine_pretraining import LitEEGPT\n",
    "\n",
    "def load_eegpt_lightning():\n",
    "    # Load the complete Lightning model\n",
    "    model = LitEEGPT.load_from_checkpoint(\"path/to/checkpoint.ckpt\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Access individual components\n",
    "    encoder = model.encoder\n",
    "    target_encoder = model.target_encoder\n",
    "    predictor = model.predictor\n",
    "    reconstructor = model.reconstructor\n",
    "    \n",
    "    return model, encoder, target_encoder, predictor, reconstructor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "23\n",
      "1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['chan_conv.0.weight', 'chan_conv.0.bias', 'predictor.cls_token', 'fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['encoder.summary_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.chan_embed.weight', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.qkv.bias', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.qkv.bias', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.qkv.bias', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.qkv.bias', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.qkv.bias', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.qkv.bias', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.qkv.bias', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.qkv.bias', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'reconstructor.mask_token', 'reconstructor.reconstructor_embed.weight', 'reconstructor.reconstructor_embed.bias', 'reconstructor.time_embed.freqs', 'reconstructor.chan_embed.weight', 'reconstructor.reconstructor_blocks.0.norm1.weight', 'reconstructor.reconstructor_blocks.0.norm1.bias', 'reconstructor.reconstructor_blocks.0.attn.qkv.weight', 'reconstructor.reconstructor_blocks.0.attn.qkv.bias', 'reconstructor.reconstructor_blocks.0.attn.proj.weight', 'reconstructor.reconstructor_blocks.0.attn.proj.bias', 'reconstructor.reconstructor_blocks.0.norm2.weight', 'reconstructor.reconstructor_blocks.0.norm2.bias', 'reconstructor.reconstructor_blocks.0.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.0.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.0.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.0.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.1.norm1.weight', 'reconstructor.reconstructor_blocks.1.norm1.bias', 'reconstructor.reconstructor_blocks.1.attn.qkv.weight', 'reconstructor.reconstructor_blocks.1.attn.qkv.bias', 'reconstructor.reconstructor_blocks.1.attn.proj.weight', 'reconstructor.reconstructor_blocks.1.attn.proj.bias', 'reconstructor.reconstructor_blocks.1.norm2.weight', 'reconstructor.reconstructor_blocks.1.norm2.bias', 'reconstructor.reconstructor_blocks.1.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.1.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.1.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.1.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.2.norm1.weight', 'reconstructor.reconstructor_blocks.2.norm1.bias', 'reconstructor.reconstructor_blocks.2.attn.qkv.weight', 'reconstructor.reconstructor_blocks.2.attn.qkv.bias', 'reconstructor.reconstructor_blocks.2.attn.proj.weight', 'reconstructor.reconstructor_blocks.2.attn.proj.bias', 'reconstructor.reconstructor_blocks.2.norm2.weight', 'reconstructor.reconstructor_blocks.2.norm2.bias', 'reconstructor.reconstructor_blocks.2.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.2.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.2.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.2.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.3.norm1.weight', 'reconstructor.reconstructor_blocks.3.norm1.bias', 'reconstructor.reconstructor_blocks.3.attn.qkv.weight', 'reconstructor.reconstructor_blocks.3.attn.qkv.bias', 'reconstructor.reconstructor_blocks.3.attn.proj.weight', 'reconstructor.reconstructor_blocks.3.attn.proj.bias', 'reconstructor.reconstructor_blocks.3.norm2.weight', 'reconstructor.reconstructor_blocks.3.norm2.bias', 'reconstructor.reconstructor_blocks.3.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.3.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.3.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.3.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.4.norm1.weight', 'reconstructor.reconstructor_blocks.4.norm1.bias', 'reconstructor.reconstructor_blocks.4.attn.qkv.weight', 'reconstructor.reconstructor_blocks.4.attn.qkv.bias', 'reconstructor.reconstructor_blocks.4.attn.proj.weight', 'reconstructor.reconstructor_blocks.4.attn.proj.bias', 'reconstructor.reconstructor_blocks.4.norm2.weight', 'reconstructor.reconstructor_blocks.4.norm2.bias', 'reconstructor.reconstructor_blocks.4.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.4.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.4.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.4.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.5.norm1.weight', 'reconstructor.reconstructor_blocks.5.norm1.bias', 'reconstructor.reconstructor_blocks.5.attn.qkv.weight', 'reconstructor.reconstructor_blocks.5.attn.qkv.bias', 'reconstructor.reconstructor_blocks.5.attn.proj.weight', 'reconstructor.reconstructor_blocks.5.attn.proj.bias', 'reconstructor.reconstructor_blocks.5.norm2.weight', 'reconstructor.reconstructor_blocks.5.norm2.bias', 'reconstructor.reconstructor_blocks.5.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.5.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.5.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.5.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.6.norm1.weight', 'reconstructor.reconstructor_blocks.6.norm1.bias', 'reconstructor.reconstructor_blocks.6.attn.qkv.weight', 'reconstructor.reconstructor_blocks.6.attn.qkv.bias', 'reconstructor.reconstructor_blocks.6.attn.proj.weight', 'reconstructor.reconstructor_blocks.6.attn.proj.bias', 'reconstructor.reconstructor_blocks.6.norm2.weight', 'reconstructor.reconstructor_blocks.6.norm2.bias', 'reconstructor.reconstructor_blocks.6.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.6.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.6.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.6.mlp.fc2.bias', 'reconstructor.reconstructor_blocks.7.norm1.weight', 'reconstructor.reconstructor_blocks.7.norm1.bias', 'reconstructor.reconstructor_blocks.7.attn.qkv.weight', 'reconstructor.reconstructor_blocks.7.attn.qkv.bias', 'reconstructor.reconstructor_blocks.7.attn.proj.weight', 'reconstructor.reconstructor_blocks.7.attn.proj.bias', 'reconstructor.reconstructor_blocks.7.norm2.weight', 'reconstructor.reconstructor_blocks.7.norm2.bias', 'reconstructor.reconstructor_blocks.7.mlp.fc1.weight', 'reconstructor.reconstructor_blocks.7.mlp.fc1.bias', 'reconstructor.reconstructor_blocks.7.mlp.fc2.weight', 'reconstructor.reconstructor_blocks.7.mlp.fc2.bias', 'reconstructor.reconstructor_norm.weight', 'reconstructor.reconstructor_norm.bias', 'reconstructor.reconstructor_proj.weight', 'reconstructor.reconstructor_proj.bias'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from downstream.Modules.models.EEGPT_mcae_finetune import EEGPTClassifier\n",
    "import torch\n",
    "\n",
    "use_channels_names = [      \n",
    "               'FP1', 'FP2',\n",
    "        'F7', 'F3', 'FZ', 'F4', 'F8',\n",
    "        'T7', 'C3', 'CZ', 'C4', 'T8',\n",
    "        'P7', 'P3', 'PZ', 'P4', 'P8',\n",
    "                'O1', 'O2' ]\n",
    "ch_names = ['EEG FP1', 'EEG FP2-REF', 'EEG F3-REF', 'EEG F4-REF', 'EEG C3-REF', 'EEG C4-REF', 'EEG P3-REF', 'EEG P4-REF', 'EEG O1-REF', 'EEG O2-REF', 'EEG F7-REF', \\\n",
    "                'EEG F8-REF', 'EEG T3-REF', 'EEG T4-REF', 'EEG T5-REF', 'EEG T6-REF', 'EEG A1-REF', 'EEG A2-REF', 'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF', 'EEG T1-REF', 'EEG T2-REF']\n",
    "ch_names = [name.split(' ')[-1].split('-')[0] for name in ch_names]\n",
    "# use_channels_names = ch_names\n",
    "# print(f'Using channels: {use_channels_names}')\n",
    "print(len(use_channels_names))\n",
    "print(len(ch_names))\n",
    "\n",
    "print(256 * 4)\n",
    "\n",
    "model = EEGPTClassifier(4, in_channels=len(ch_names), img_size=[len(use_channels_names),2000], use_channels_names=use_channels_names, use_chan_conv=True, use_predictor=True)\n",
    "\n",
    "# mising: target_encoder, predictor, fc, head\n",
    "\n",
    "# Load the pretrained weights\n",
    "checkpoint = torch.load(\"./model_checkpoints/25866970/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt\", \n",
    "                        map_location='cpu', weights_only=False)\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)  # strict=False to allow new classification head\n",
    "\n",
    "# AAA! state_dict\n",
    "# teraz missing: chan_conv, \n",
    "\n",
    "# ale unexpected keys (?): encoder, reconstructor, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcb46ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EEGPTClassifier(\n",
       "  (chan_conv): Sequential(\n",
       "    (0): Conv1dWithConstraint(23, 19, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (target_encoder): EEGTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 512, kernel_size=(1, 64), stride=(1, 64))\n",
       "    )\n",
       "    (chan_embed): Embedding(62, 512)\n",
       "    (blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (predictor): EEGTransformerPredictor(\n",
       "    (predictor_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (time_embed): RotaryEmbedding()\n",
       "    (predictor_blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (predictor_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    (predictor_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): LinearWithConstraint(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501c5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 51,038,668\n",
      "Target encoder parameters: 25,287,168\n",
      "Predictor parameters: 25,747,968\n",
      "Channel conv parameters: 456\n",
      "Classification head parameters: 2,052\n",
      "Non-trainable parameters: 16, [['predictor.time_embed.freqs']]\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "def count_nontrainable_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(f'Trainable parameters: {count_parameters(model):,}')\n",
    "print(f'Target encoder parameters: {count_parameters(model.target_encoder):,}')\n",
    "print(f'Predictor parameters: {count_parameters(model.predictor):,}')\n",
    "if hasattr(model, 'reconstructor'):\n",
    "  print(f'Reconstructor parameters: {count_parameters(model.reconstructor):,}')\n",
    "if hasattr(model, 'chan_conv'):\n",
    "  print(f'Channel conv parameters: {count_parameters(model.chan_conv):,}')\n",
    "if hasattr(model, 'head'):\n",
    "  print(f'Classification head parameters: {count_parameters(model.head):,}')\n",
    "\n",
    "print(f'Non-trainable parameters: {count_nontrainable_parameters(model):,}, [{[n for n, p in model.named_parameters() if not p.requires_grad]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc11afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearWithConstraint(in_features=512, out_features=4, bias=True)\n",
      "Linear(in_features=512, out_features=128, bias=True)\n",
      "number of parameters: 65664\n"
     ]
    }
   ],
   "source": [
    "print(model.head)\n",
    "import torch.nn as nn\n",
    "m = nn.Linear(in_features=512, out_features=128, bias=True)\n",
    "print(m)\n",
    "print(\"number of parameters:\", count_parameters(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600210fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "target_encoder.patch_embed.proj: Conv2d\n",
      "target_encoder.blocks.0.attn: Attention\n",
      "target_encoder.blocks.0.attn.qkv: Linear\n",
      "target_encoder.blocks.0.attn.proj: Linear\n",
      "target_encoder.blocks.0.attn.proj_drop: Dropout\n",
      "target_encoder.blocks.0.mlp.fc1: Linear\n",
      "target_encoder.blocks.0.mlp.fc2: Linear\n",
      "target_encoder.blocks.1.attn: Attention\n",
      "target_encoder.blocks.1.attn.qkv: Linear\n",
      "target_encoder.blocks.1.attn.proj: Linear\n",
      "target_encoder.blocks.1.attn.proj_drop: Dropout\n",
      "target_encoder.blocks.1.mlp.fc1: Linear\n",
      "target_encoder.blocks.1.mlp.fc2: Linear\n",
      "target_encoder.blocks.2.attn: Attention\n",
      "target_encoder.blocks.2.attn.qkv: Linear\n",
      "target_encoder.blocks.2.attn.proj: Linear\n",
      "target_encoder.blocks.2.attn.proj_drop: Dropout\n",
      "target_encoder.blocks.2.mlp.fc1: Linear\n",
      "target_encoder.blocks.2.mlp.fc2: Linear\n",
      "target_encoder.blocks.3.attn: Attention\n",
      "target_encoder.blocks.3.attn.qkv: Linear\n",
      "target_encoder.blocks.3.attn.proj: Linear\n",
      "target_encoder.blocks.3.attn.proj_drop: Dropout\n",
      "target_encoder.blocks.3.mlp.fc1: Linear\n",
      "target_encoder.blocks.3.mlp.fc2: Linear\n",
      "target_encoder.blocks.4.attn: Attention\n",
      "target_encoder.blocks.4.attn.qkv: Linear\n",
      "target_encoder.blocks.4.attn.proj: Linear\n",
      "target_encoder.blocks.4.attn.proj_drop: Dropout\n",
      "target_encoder.blocks.4.mlp.fc1: Linear\n",
      "target_encoder.blocks.4.mlp.fc2: Linear\n",
      "target_encoder.blocks.5.attn: Attention\n",
      "target_encoder.blocks.5.attn.qkv: Linear\n",
      "target_encoder.blocks.5.attn.proj: Linear\n",
      "target_encoder.blocks.5.attn.proj_drop: Dropout\n",
      "target_encoder.blocks.5.mlp.fc1: Linear\n",
      "target_encoder.blocks.5.mlp.fc2: Linear\n",
      "target_encoder.blocks.6.attn: Attention\n",
      "target_encoder.blocks.6.attn.qkv: Linear\n",
      "target_encoder.blocks.6.attn.proj: Linear\n",
      "target_encoder.blocks.6.attn.proj_drop: Dropout\n",
      "target_encoder.blocks.6.mlp.fc1: Linear\n",
      "target_encoder.blocks.6.mlp.fc2: Linear\n",
      "target_encoder.blocks.7.attn: Attention\n",
      "target_encoder.blocks.7.attn.qkv: Linear\n",
      "target_encoder.blocks.7.attn.proj: Linear\n",
      "target_encoder.blocks.7.attn.proj_drop: Dropout\n",
      "target_encoder.blocks.7.mlp.fc1: Linear\n",
      "target_encoder.blocks.7.mlp.fc2: Linear\n",
      "predictor.predictor_blocks.0.attn: Attention\n",
      "predictor.predictor_blocks.0.attn.qkv: Linear\n",
      "predictor.predictor_blocks.0.attn.proj: Linear\n",
      "predictor.predictor_blocks.0.attn.proj_drop: Dropout\n",
      "predictor.predictor_blocks.0.mlp.fc1: Linear\n",
      "predictor.predictor_blocks.0.mlp.fc2: Linear\n",
      "predictor.predictor_blocks.1.attn: Attention\n",
      "predictor.predictor_blocks.1.attn.qkv: Linear\n",
      "predictor.predictor_blocks.1.attn.proj: Linear\n",
      "predictor.predictor_blocks.1.attn.proj_drop: Dropout\n",
      "predictor.predictor_blocks.1.mlp.fc1: Linear\n",
      "predictor.predictor_blocks.1.mlp.fc2: Linear\n",
      "predictor.predictor_blocks.2.attn: Attention\n",
      "predictor.predictor_blocks.2.attn.qkv: Linear\n",
      "predictor.predictor_blocks.2.attn.proj: Linear\n",
      "predictor.predictor_blocks.2.attn.proj_drop: Dropout\n",
      "predictor.predictor_blocks.2.mlp.fc1: Linear\n",
      "predictor.predictor_blocks.2.mlp.fc2: Linear\n",
      "predictor.predictor_blocks.3.attn: Attention\n",
      "predictor.predictor_blocks.3.attn.qkv: Linear\n",
      "predictor.predictor_blocks.3.attn.proj: Linear\n",
      "predictor.predictor_blocks.3.attn.proj_drop: Dropout\n",
      "predictor.predictor_blocks.3.mlp.fc1: Linear\n",
      "predictor.predictor_blocks.3.mlp.fc2: Linear\n",
      "predictor.predictor_blocks.4.attn: Attention\n",
      "predictor.predictor_blocks.4.attn.qkv: Linear\n",
      "predictor.predictor_blocks.4.attn.proj: Linear\n",
      "predictor.predictor_blocks.4.attn.proj_drop: Dropout\n",
      "predictor.predictor_blocks.4.mlp.fc1: Linear\n",
      "predictor.predictor_blocks.4.mlp.fc2: Linear\n",
      "predictor.predictor_blocks.5.attn: Attention\n",
      "predictor.predictor_blocks.5.attn.qkv: Linear\n",
      "predictor.predictor_blocks.5.attn.proj: Linear\n",
      "predictor.predictor_blocks.5.attn.proj_drop: Dropout\n",
      "predictor.predictor_blocks.5.mlp.fc1: Linear\n",
      "predictor.predictor_blocks.5.mlp.fc2: Linear\n",
      "predictor.predictor_blocks.6.attn: Attention\n",
      "predictor.predictor_blocks.6.attn.qkv: Linear\n",
      "predictor.predictor_blocks.6.attn.proj: Linear\n",
      "predictor.predictor_blocks.6.attn.proj_drop: Dropout\n",
      "predictor.predictor_blocks.6.mlp.fc1: Linear\n",
      "predictor.predictor_blocks.6.mlp.fc2: Linear\n",
      "predictor.predictor_blocks.7.attn: Attention\n",
      "predictor.predictor_blocks.7.attn.qkv: Linear\n",
      "predictor.predictor_blocks.7.attn.proj: Linear\n",
      "predictor.predictor_blocks.7.attn.proj_drop: Dropout\n",
      "predictor.predictor_blocks.7.mlp.fc1: Linear\n",
      "predictor.predictor_blocks.7.mlp.fc2: Linear\n",
      "predictor.predictor_proj: Linear\n",
      "fc_norm: LayerNorm\n",
      "\n",
      "Model parameters:\n",
      "target_encoder.patch_embed.proj.weight: torch.Size([512, 1, 1, 64])\n",
      "target_encoder.patch_embed.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.0.attn.qkv.weight: torch.Size([1536, 512])\n",
      "target_encoder.blocks.0.attn.qkv.bias: torch.Size([1536])\n",
      "target_encoder.blocks.0.attn.proj.weight: torch.Size([512, 512])\n",
      "target_encoder.blocks.0.attn.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.0.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "target_encoder.blocks.0.mlp.fc1.bias: torch.Size([2048])\n",
      "target_encoder.blocks.0.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "target_encoder.blocks.0.mlp.fc2.bias: torch.Size([512])\n",
      "target_encoder.blocks.1.attn.qkv.weight: torch.Size([1536, 512])\n",
      "target_encoder.blocks.1.attn.qkv.bias: torch.Size([1536])\n",
      "target_encoder.blocks.1.attn.proj.weight: torch.Size([512, 512])\n",
      "target_encoder.blocks.1.attn.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.1.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "target_encoder.blocks.1.mlp.fc1.bias: torch.Size([2048])\n",
      "target_encoder.blocks.1.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "target_encoder.blocks.1.mlp.fc2.bias: torch.Size([512])\n",
      "target_encoder.blocks.2.attn.qkv.weight: torch.Size([1536, 512])\n",
      "target_encoder.blocks.2.attn.qkv.bias: torch.Size([1536])\n",
      "target_encoder.blocks.2.attn.proj.weight: torch.Size([512, 512])\n",
      "target_encoder.blocks.2.attn.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.2.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "target_encoder.blocks.2.mlp.fc1.bias: torch.Size([2048])\n",
      "target_encoder.blocks.2.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "target_encoder.blocks.2.mlp.fc2.bias: torch.Size([512])\n",
      "target_encoder.blocks.3.attn.qkv.weight: torch.Size([1536, 512])\n",
      "target_encoder.blocks.3.attn.qkv.bias: torch.Size([1536])\n",
      "target_encoder.blocks.3.attn.proj.weight: torch.Size([512, 512])\n",
      "target_encoder.blocks.3.attn.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.3.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "target_encoder.blocks.3.mlp.fc1.bias: torch.Size([2048])\n",
      "target_encoder.blocks.3.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "target_encoder.blocks.3.mlp.fc2.bias: torch.Size([512])\n",
      "target_encoder.blocks.4.attn.qkv.weight: torch.Size([1536, 512])\n",
      "target_encoder.blocks.4.attn.qkv.bias: torch.Size([1536])\n",
      "target_encoder.blocks.4.attn.proj.weight: torch.Size([512, 512])\n",
      "target_encoder.blocks.4.attn.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.4.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "target_encoder.blocks.4.mlp.fc1.bias: torch.Size([2048])\n",
      "target_encoder.blocks.4.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "target_encoder.blocks.4.mlp.fc2.bias: torch.Size([512])\n",
      "target_encoder.blocks.5.attn.qkv.weight: torch.Size([1536, 512])\n",
      "target_encoder.blocks.5.attn.qkv.bias: torch.Size([1536])\n",
      "target_encoder.blocks.5.attn.proj.weight: torch.Size([512, 512])\n",
      "target_encoder.blocks.5.attn.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.5.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "target_encoder.blocks.5.mlp.fc1.bias: torch.Size([2048])\n",
      "target_encoder.blocks.5.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "target_encoder.blocks.5.mlp.fc2.bias: torch.Size([512])\n",
      "target_encoder.blocks.6.attn.qkv.weight: torch.Size([1536, 512])\n",
      "target_encoder.blocks.6.attn.qkv.bias: torch.Size([1536])\n",
      "target_encoder.blocks.6.attn.proj.weight: torch.Size([512, 512])\n",
      "target_encoder.blocks.6.attn.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.6.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "target_encoder.blocks.6.mlp.fc1.bias: torch.Size([2048])\n",
      "target_encoder.blocks.6.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "target_encoder.blocks.6.mlp.fc2.bias: torch.Size([512])\n",
      "target_encoder.blocks.7.attn.qkv.weight: torch.Size([1536, 512])\n",
      "target_encoder.blocks.7.attn.qkv.bias: torch.Size([1536])\n",
      "target_encoder.blocks.7.attn.proj.weight: torch.Size([512, 512])\n",
      "target_encoder.blocks.7.attn.proj.bias: torch.Size([512])\n",
      "target_encoder.blocks.7.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "target_encoder.blocks.7.mlp.fc1.bias: torch.Size([2048])\n",
      "target_encoder.blocks.7.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "target_encoder.blocks.7.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.0.attn.qkv.weight: torch.Size([1536, 512])\n",
      "predictor.predictor_blocks.0.attn.qkv.bias: torch.Size([1536])\n",
      "predictor.predictor_blocks.0.attn.proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_blocks.0.attn.proj.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.0.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "predictor.predictor_blocks.0.mlp.fc1.bias: torch.Size([2048])\n",
      "predictor.predictor_blocks.0.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "predictor.predictor_blocks.0.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.1.attn.qkv.weight: torch.Size([1536, 512])\n",
      "predictor.predictor_blocks.1.attn.qkv.bias: torch.Size([1536])\n",
      "predictor.predictor_blocks.1.attn.proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_blocks.1.attn.proj.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.1.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "predictor.predictor_blocks.1.mlp.fc1.bias: torch.Size([2048])\n",
      "predictor.predictor_blocks.1.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "predictor.predictor_blocks.1.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.2.attn.qkv.weight: torch.Size([1536, 512])\n",
      "predictor.predictor_blocks.2.attn.qkv.bias: torch.Size([1536])\n",
      "predictor.predictor_blocks.2.attn.proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_blocks.2.attn.proj.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.2.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "predictor.predictor_blocks.2.mlp.fc1.bias: torch.Size([2048])\n",
      "predictor.predictor_blocks.2.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "predictor.predictor_blocks.2.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.3.attn.qkv.weight: torch.Size([1536, 512])\n",
      "predictor.predictor_blocks.3.attn.qkv.bias: torch.Size([1536])\n",
      "predictor.predictor_blocks.3.attn.proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_blocks.3.attn.proj.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.3.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "predictor.predictor_blocks.3.mlp.fc1.bias: torch.Size([2048])\n",
      "predictor.predictor_blocks.3.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "predictor.predictor_blocks.3.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.4.attn.qkv.weight: torch.Size([1536, 512])\n",
      "predictor.predictor_blocks.4.attn.qkv.bias: torch.Size([1536])\n",
      "predictor.predictor_blocks.4.attn.proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_blocks.4.attn.proj.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.4.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "predictor.predictor_blocks.4.mlp.fc1.bias: torch.Size([2048])\n",
      "predictor.predictor_blocks.4.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "predictor.predictor_blocks.4.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.5.attn.qkv.weight: torch.Size([1536, 512])\n",
      "predictor.predictor_blocks.5.attn.qkv.bias: torch.Size([1536])\n",
      "predictor.predictor_blocks.5.attn.proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_blocks.5.attn.proj.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.5.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "predictor.predictor_blocks.5.mlp.fc1.bias: torch.Size([2048])\n",
      "predictor.predictor_blocks.5.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "predictor.predictor_blocks.5.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.6.attn.qkv.weight: torch.Size([1536, 512])\n",
      "predictor.predictor_blocks.6.attn.qkv.bias: torch.Size([1536])\n",
      "predictor.predictor_blocks.6.attn.proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_blocks.6.attn.proj.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.6.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "predictor.predictor_blocks.6.mlp.fc1.bias: torch.Size([2048])\n",
      "predictor.predictor_blocks.6.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "predictor.predictor_blocks.6.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.7.attn.qkv.weight: torch.Size([1536, 512])\n",
      "predictor.predictor_blocks.7.attn.qkv.bias: torch.Size([1536])\n",
      "predictor.predictor_blocks.7.attn.proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_blocks.7.attn.proj.bias: torch.Size([512])\n",
      "predictor.predictor_blocks.7.mlp.fc1.weight: torch.Size([2048, 512])\n",
      "predictor.predictor_blocks.7.mlp.fc1.bias: torch.Size([2048])\n",
      "predictor.predictor_blocks.7.mlp.fc2.weight: torch.Size([512, 2048])\n",
      "predictor.predictor_blocks.7.mlp.fc2.bias: torch.Size([512])\n",
      "predictor.predictor_proj.weight: torch.Size([512, 512])\n",
      "predictor.predictor_proj.bias: torch.Size([512])\n",
      "fc_norm.weight: torch.Size([512])\n",
      "fc_norm.bias: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect the model structure to find the correct target modules for LoRA\n",
    "print(\"Model structure:\")\n",
    "for name, module in model.named_modules():\n",
    "    if any(x in name for x in ['attn', 'qkv', 'proj', 'linear', 'fc']):\n",
    "        print(f\"{name}: {type(module).__name__}\")\n",
    "        \n",
    "print(\"\\nModel parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if any(x in name for x in ['attn', 'qkv', 'proj', 'linear', 'fc']):\n",
    "        print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af403c6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'music_filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGMusicDataset\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ds = \u001b[43mEEGMusicDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_ondisk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./datasets/bcmi_preprocessed/bcmi_caltrain_256\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/magisterka/src/data.py:812\u001b[39m, in \u001b[36mEEGMusicDataset.load_ondisk\u001b[39m\u001b[34m(cls, base_dir)\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trial_record \u001b[38;5;129;01min\u001b[39;00m metadata.trials:\n\u001b[32m    797\u001b[39m   eeg_path = make_eeg_path(\n\u001b[32m    798\u001b[39m     eeg_dir,\n\u001b[32m    799\u001b[39m     trial_record[\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    803\u001b[39m     trial_record[\u001b[33m\"\u001b[39m\u001b[33mtrial_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    804\u001b[39m   )\n\u001b[32m    805\u001b[39m   rows.append(\n\u001b[32m    806\u001b[39m     {\n\u001b[32m    807\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: trial_record[\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    808\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33msubject\u001b[39m\u001b[33m\"\u001b[39m: trial_record[\u001b[33m\"\u001b[39m\u001b[33msubject\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    809\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33msession\u001b[39m\u001b[33m\"\u001b[39m: trial_record[\u001b[33m\"\u001b[39m\u001b[33msession\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    810\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m\"\u001b[39m: trial_record[\u001b[33m\"\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    811\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mtrial_id\u001b[39m\u001b[33m\"\u001b[39m: trial_record[\u001b[33m\"\u001b[39m\u001b[33mtrial_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m812\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mmusic_filename\u001b[39m\u001b[33m\"\u001b[39m: MusicFilename(filename=\u001b[43mtrial_record\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmusic_filename\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m),\n\u001b[32m    813\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33meeg_data\u001b[39m\u001b[33m\"\u001b[39m: OnDiskEeg(filepath=eeg_path),\n\u001b[32m    814\u001b[39m     }\n\u001b[32m    815\u001b[39m   )\n\u001b[32m    817\u001b[39m dataset.df = pd.DataFrame(rows)\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[31mKeyError\u001b[39m: 'music_filename'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from data import EEGMusicDataset\n",
    "\n",
    "x = torch.randn(2, 18, 4*256)  # batch_size=2, channels=18, time_points=2000\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f38536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting EEGPT model structure for LoRA target modules...\n",
      "  Found target: target_encoder.patch_embed.proj -> Conv2d\n",
      "  Found target: target_encoder.blocks.0.attn.qkv -> Linear\n",
      "  Found target: target_encoder.blocks.0.attn.proj -> Linear\n",
      "  Found target: target_encoder.blocks.0.attn.proj_drop -> Dropout\n",
      "  Found target: target_encoder.blocks.0.mlp.fc1 -> Linear\n",
      "  Found target: target_encoder.blocks.0.mlp.fc2 -> Linear\n",
      "  Found target: target_encoder.blocks.1.attn.qkv -> Linear\n",
      "  Found target: target_encoder.blocks.1.attn.proj -> Linear\n",
      "  Found target: target_encoder.blocks.1.attn.proj_drop -> Dropout\n",
      "  Found target: target_encoder.blocks.1.mlp.fc1 -> Linear\n",
      "  Found target: target_encoder.blocks.1.mlp.fc2 -> Linear\n",
      "  Found target: target_encoder.blocks.2.attn.qkv -> Linear\n",
      "  Found target: target_encoder.blocks.2.attn.proj -> Linear\n",
      "  Found target: target_encoder.blocks.2.attn.proj_drop -> Dropout\n",
      "  Found target: target_encoder.blocks.2.mlp.fc1 -> Linear\n",
      "  Found target: target_encoder.blocks.2.mlp.fc2 -> Linear\n",
      "  Found target: target_encoder.blocks.3.attn.qkv -> Linear\n",
      "  Found target: target_encoder.blocks.3.attn.proj -> Linear\n",
      "  Found target: target_encoder.blocks.3.attn.proj_drop -> Dropout\n",
      "  Found target: target_encoder.blocks.3.mlp.fc1 -> Linear\n",
      "  Found target: target_encoder.blocks.3.mlp.fc2 -> Linear\n",
      "  Found target: target_encoder.blocks.4.attn.qkv -> Linear\n",
      "  Found target: target_encoder.blocks.4.attn.proj -> Linear\n",
      "  Found target: target_encoder.blocks.4.attn.proj_drop -> Dropout\n",
      "  Found target: target_encoder.blocks.4.mlp.fc1 -> Linear\n",
      "  Found target: target_encoder.blocks.4.mlp.fc2 -> Linear\n",
      "  Found target: target_encoder.blocks.5.attn.qkv -> Linear\n",
      "  Found target: target_encoder.blocks.5.attn.proj -> Linear\n",
      "  Found target: target_encoder.blocks.5.attn.proj_drop -> Dropout\n",
      "  Found target: target_encoder.blocks.5.mlp.fc1 -> Linear\n",
      "  Found target: target_encoder.blocks.5.mlp.fc2 -> Linear\n",
      "  Found target: target_encoder.blocks.6.attn.qkv -> Linear\n",
      "  Found target: target_encoder.blocks.6.attn.proj -> Linear\n",
      "  Found target: target_encoder.blocks.6.attn.proj_drop -> Dropout\n",
      "  Found target: target_encoder.blocks.6.mlp.fc1 -> Linear\n",
      "  Found target: target_encoder.blocks.6.mlp.fc2 -> Linear\n",
      "  Found target: target_encoder.blocks.7.attn.qkv -> Linear\n",
      "  Found target: target_encoder.blocks.7.attn.proj -> Linear\n",
      "  Found target: target_encoder.blocks.7.attn.proj_drop -> Dropout\n",
      "  Found target: target_encoder.blocks.7.mlp.fc1 -> Linear\n",
      "  Found target: target_encoder.blocks.7.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.0.attn.qkv -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.0.attn.proj -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.0.attn.proj_drop -> Dropout\n",
      "  Found target: reconstructor.reconstructor_blocks.0.mlp.fc1 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.0.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.1.attn.qkv -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.1.attn.proj -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.1.attn.proj_drop -> Dropout\n",
      "  Found target: reconstructor.reconstructor_blocks.1.mlp.fc1 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.1.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.2.attn.qkv -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.2.attn.proj -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.2.attn.proj_drop -> Dropout\n",
      "  Found target: reconstructor.reconstructor_blocks.2.mlp.fc1 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.2.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.3.attn.qkv -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.3.attn.proj -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.3.attn.proj_drop -> Dropout\n",
      "  Found target: reconstructor.reconstructor_blocks.3.mlp.fc1 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.3.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.4.attn.qkv -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.4.attn.proj -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.4.attn.proj_drop -> Dropout\n",
      "  Found target: reconstructor.reconstructor_blocks.4.mlp.fc1 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.4.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.5.attn.qkv -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.5.attn.proj -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.5.attn.proj_drop -> Dropout\n",
      "  Found target: reconstructor.reconstructor_blocks.5.mlp.fc1 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.5.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.6.attn.qkv -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.6.attn.proj -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.6.attn.proj_drop -> Dropout\n",
      "  Found target: reconstructor.reconstructor_blocks.6.mlp.fc1 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.6.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.7.attn.qkv -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.7.attn.proj -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.7.attn.proj_drop -> Dropout\n",
      "  Found target: reconstructor.reconstructor_blocks.7.mlp.fc1 -> Linear\n",
      "  Found target: reconstructor.reconstructor_blocks.7.mlp.fc2 -> Linear\n",
      "  Found target: reconstructor.reconstructor_proj -> Linear\n",
      "\n",
      "Target modules for LoRA: ['qkv', 'proj', 'fc2', 'fc1']\n",
      "\n",
      "LoRA Configuration:\n",
      "  Rank: 16\n",
      "  Alpha: 32\n",
      "  Target modules: {'qkv', 'proj', 'fc2', 'fc1'}\n",
      "  Modules to save: ['head', 'chan_conv']\n",
      "\n",
      "✅ LoRA applied successfully!\n",
      "trainable params: 2,108,876 || all params: 52,947,944 || trainable%: 3.9829\n",
      "\n",
      "Detailed parameter analysis:\n",
      "  Total parameters: 52,947,944\n",
      "  Trainable parameters: 2,108,876\n",
      "  Trainable ratio: 3.98%\n",
      "\n",
      "✅ LoRA applied successfully!\n",
      "trainable params: 2,108,876 || all params: 52,947,944 || trainable%: 3.9829\n",
      "\n",
      "Detailed parameter analysis:\n",
      "  Total parameters: 52,947,944\n",
      "  Trainable parameters: 2,108,876\n",
      "  Trainable ratio: 3.98%\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Based on the EEGPT source code, the attention layers have these components:\n",
    "# - Attention class has: qkv, proj (output projection)\n",
    "# - MLP class has: fc1, fc2\n",
    "# - The model has target_encoder, reconstructor/predictor components\n",
    "\n",
    "print(\"Inspecting EEGPT model structure for LoRA target modules...\")\n",
    "\n",
    "# Find all the linear layers that are good candidates for LoRA\n",
    "target_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    # Target the key attention and MLP components\n",
    "    if any(x in name for x in ['qkv', 'proj', 'fc1', 'fc2']) and 'head' not in name:\n",
    "        print(f\"  Found target: {name} -> {type(module).__name__}\")\n",
    "        # Extract the module type for targeting\n",
    "        if 'qkv' in name:\n",
    "            target_modules.append('qkv')\n",
    "        elif 'proj' in name and 'attn' in name:\n",
    "            target_modules.append('proj')\n",
    "        elif 'fc1' in name:\n",
    "            target_modules.append('fc1')\n",
    "        elif 'fc2' in name:\n",
    "            target_modules.append('fc2')\n",
    "\n",
    "# Remove duplicates and keep unique module names\n",
    "target_modules = list(set(target_modules))\n",
    "print(f\"\\nTarget modules for LoRA: {target_modules}\")\n",
    "\n",
    "# Configure LoRA specifically for EEGPT\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                    # rank - controls adaptation capacity\n",
    "    lora_alpha=32,          # scaling factor (typically 2*r)\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,  # EEG feature extraction task\n",
    "    target_modules=target_modules,  # Use the discovered modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",            # Don't adapt bias terms\n",
    "    modules_to_save=[\"head\", \"chan_conv\"]  # Save task-specific layers\n",
    ")\n",
    "\n",
    "print(f\"\\nLoRA Configuration:\")\n",
    "print(f\"  Rank: {peft_config.r}\")\n",
    "print(f\"  Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {peft_config.target_modules}\")\n",
    "print(f\"  Modules to save: {peft_config.modules_to_save}\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "try:\n",
    "    model_with_lora = get_peft_model(model, peft_config)\n",
    "    print(\"\\n✅ LoRA applied successfully!\")\n",
    "    \n",
    "    # Show parameter breakdown\n",
    "    model_with_lora.print_trainable_parameters()\n",
    "    \n",
    "    # Additional parameter analysis\n",
    "    total_params = sum(p.numel() for p in model_with_lora.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nDetailed parameter analysis:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Trainable ratio: {trainable_params/total_params*100:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error applying LoRA: {e}\")\n",
    "    print(\"This might be because the target modules don't match exactly.\")\n",
    "    print(\"Let's try with a more conservative approach...\")\n",
    "    \n",
    "    # Fallback: use regex patterns for more flexible matching\n",
    "    peft_config_fallback = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        task_type=TaskType.FEATURE_EXTRACTION,\n",
    "        target_modules=[\"qkv\", \"proj\"],  # Most common in transformers\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model_with_lora = get_peft_model(model, peft_config_fallback)\n",
    "        print(\"✅ LoRA applied with fallback configuration!\")\n",
    "        model_with_lora.print_trainable_parameters()\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Fallback also failed: {e2}\")\n",
    "        model_with_lora = model  # Keep original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d64bee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LoRA-adapted model:\n",
      "Test input shape: torch.Size([2, 23, 1024])\n",
      "❌ Forward pass failed: EEGPTClassifier.forward() missing 1 required positional argument: 'x'\n"
     ]
    }
   ],
   "source": [
    "# Test the LoRA-adapted model\n",
    "if 'model_with_lora' in locals():\n",
    "    print(\"Testing LoRA-adapted model:\")\n",
    "    \n",
    "    # Create test input with correct dimensions\n",
    "    test_input = torch.randn(2, len(ch_names), 1024)  # Use the correct time length\n",
    "    print(f\"Test input shape: {test_input.shape}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    model_with_lora.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            output = model_with_lora(test_input)\n",
    "            print(f\"✅ Forward pass successful!\")\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "            \n",
    "            # Compare parameter counts\n",
    "            original_params = sum(p.numel() for p in model.parameters())\n",
    "            lora_trainable = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n",
    "            lora_total = sum(p.numel() for p in model_with_lora.parameters())\n",
    "            \n",
    "            print(f\"\\nParameter comparison:\")\n",
    "            print(f\"  Original model: {original_params:,} parameters\")\n",
    "            print(f\"  LoRA trainable: {lora_trainable:,} parameters ({lora_trainable/original_params*100:.2f}% of original)\")\n",
    "            print(f\"  LoRA total: {lora_total:,} parameters\")\n",
    "            \n",
    "            # Show which parameters are trainable\n",
    "            print(f\"\\nTrainable parameter breakdown:\")\n",
    "            for name, param in model_with_lora.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    print(f\"  {name}: {param.shape} ({param.numel():,} params)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Forward pass failed: {e}\")\n",
    "else:\n",
    "    print(\"❌ LoRA model not available - check previous cell for errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ede73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE LORA MODEL TEST ===\n",
      "\n",
      "✅ LoRA model is available\n",
      "Model type: <class 'peft.peft_model.PeftModelForFeatureExtraction'>\n",
      "Is PeftModel: True\n",
      "PEFT Config: {'default': LoraConfig(task_type=<TaskType.FEATURE_EXTRACTION: 'FEATURE_EXTRACTION'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'qkv', 'proj', 'fc2', 'fc1'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['head', 'chan_conv'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)}\n",
      "\n",
      "Test input shape: torch.Size([2, 23, 1024])\n",
      "\n",
      "--- Testing Forward Methods ---\n",
      "❌ Direct forward() failed: EEGPTClassifier.forward() missing 1 required positional argument: 'x'\n",
      "Base model type: <class 'peft.tuners.lora.model.LoraModel'>\n",
      "✅ Base model forward() successful! Output shape: torch.Size([2, 4])\n",
      "Wrapped model type: <class 'downstream.Modules.models.EEGPT_mcae_finetune.EEGPTClassifier'>\n",
      "✅ Wrapped model forward() successful! Output shape: torch.Size([2, 4])\n",
      "\n",
      "--- Parameter Analysis ---\n",
      "Total parameters: 52,947,944\n",
      "Trainable parameters: 2,108,876\n",
      "Trainable ratio: 3.98%\n",
      "\n",
      "LoRA-specific trainable parameters:\n",
      "  base_model.model.chan_conv.modules_to_save.default.0.weight: torch.Size([19, 23, 1]) (437 params)\n",
      "  base_model.model.chan_conv.modules_to_save.default.0.bias: torch.Size([19]) (19 params)\n",
      "  base_model.model.target_encoder.patch_embed.proj.lora_A.default.weight: torch.Size([16, 1, 1, 64]) (1,024 params)\n",
      "  base_model.model.target_encoder.patch_embed.proj.lora_B.default.weight: torch.Size([512, 16, 1, 1]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.0.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.0.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.target_encoder.blocks.0.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.0.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.0.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.0.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.0.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.0.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.1.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.1.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.target_encoder.blocks.1.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.1.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.1.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.1.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.1.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.1.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.2.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.2.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.target_encoder.blocks.2.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.2.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.2.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.2.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.2.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.2.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.3.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.3.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.target_encoder.blocks.3.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.3.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.3.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.3.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.3.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.3.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.4.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.4.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.target_encoder.blocks.4.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.4.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.4.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.4.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.4.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.4.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.5.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.5.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.target_encoder.blocks.5.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.5.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.5.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.5.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.5.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.5.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.6.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.6.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.target_encoder.blocks.6.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.6.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.6.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.6.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.6.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.6.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.7.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.7.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.target_encoder.blocks.7.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.7.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.7.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.target_encoder.blocks.7.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.7.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.target_encoder.blocks.7.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_B.default.weight: torch.Size([1536, 16]) (24,576 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_A.default.weight: torch.Size([16, 512]) (8,192 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_B.default.weight: torch.Size([2048, 16]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_A.default.weight: torch.Size([16, 2048]) (32,768 params)\n",
      "  base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_B.default.weight: torch.Size([512, 16]) (8,192 params)\n",
      "  base_model.model.head.modules_to_save.default.weight: torch.Size([4, 512]) (2,048 params)\n",
      "  base_model.model.head.modules_to_save.default.bias: torch.Size([4]) (4 params)\n",
      "Total LoRA parameters: 2,108,876\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive LoRA Model Test\n",
    "print(\"=== COMPREHENSIVE LORA MODEL TEST ===\\n\")\n",
    "\n",
    "# Check if we have the LoRA model\n",
    "if 'model_with_lora' in locals():\n",
    "    print(\"✅ LoRA model is available\")\n",
    "    \n",
    "    # Print model type and basic info\n",
    "    print(f\"Model type: {type(model_with_lora)}\")\n",
    "    print(f\"Is PeftModel: {hasattr(model_with_lora, 'peft_config')}\")\n",
    "    \n",
    "    if hasattr(model_with_lora, 'peft_config'):\n",
    "        print(f\"PEFT Config: {model_with_lora.peft_config}\")\n",
    "    \n",
    "    # Create test input with correct dimensions\n",
    "    test_input = torch.randn(2, len(ch_names), 1024)\n",
    "    print(f\"\\nTest input shape: {test_input.shape}\")\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model_with_lora.eval()\n",
    "    \n",
    "    # Test different forward methods\n",
    "    print(\"\\n--- Testing Forward Methods ---\")\n",
    "    \n",
    "    # Try direct forward call\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            output = model_with_lora(test_input)\n",
    "        print(f\"✅ Direct forward() successful! Output shape: {output.shape}\")\n",
    "        direct_forward_works = True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Direct forward() failed: {e}\")\n",
    "        direct_forward_works = False\n",
    "    \n",
    "    # Try accessing the base model\n",
    "    try:\n",
    "        if hasattr(model_with_lora, 'base_model'):\n",
    "            base_model = model_with_lora.base_model\n",
    "            print(f\"Base model type: {type(base_model)}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = base_model(test_input)\n",
    "            print(f\"✅ Base model forward() successful! Output shape: {output.shape}\")\n",
    "        else:\n",
    "            print(\"❌ No base_model attribute found\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Base model forward() failed: {e}\")\n",
    "    \n",
    "    # Try the model attribute\n",
    "    try:\n",
    "        if hasattr(model_with_lora, 'model'):\n",
    "            wrapped_model = model_with_lora.model\n",
    "            print(f\"Wrapped model type: {type(wrapped_model)}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = wrapped_model(test_input)\n",
    "            print(f\"✅ Wrapped model forward() successful! Output shape: {output.shape}\")\n",
    "        else:\n",
    "            print(\"❌ No model attribute found\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Wrapped model forward() failed: {e}\")\n",
    "    \n",
    "    # Parameter analysis\n",
    "    print(\"\\n--- Parameter Analysis ---\")\n",
    "    try:\n",
    "        total_params = sum(p.numel() for p in model_with_lora.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"Trainable ratio: {trainable_params/total_params*100:.2f}%\")\n",
    "        \n",
    "        # Show LoRA-specific parameters\n",
    "        print(f\"\\nLoRA-specific trainable parameters:\")\n",
    "        lora_params = 0\n",
    "        for name, param in model_with_lora.named_parameters():\n",
    "            if param.requires_grad and ('lora' in name.lower() or 'head' in name or 'chan_conv' in name):\n",
    "                print(f\"  {name}: {param.shape} ({param.numel():,} params)\")\n",
    "                lora_params += param.numel()\n",
    "        \n",
    "        print(f\"Total LoRA parameters: {lora_params:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Parameter analysis failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No LoRA model found. Check previous cells for LoRA application.\")\n",
    "    \n",
    "    # Fall back to testing the base model\n",
    "    if 'model' in locals():\n",
    "        print(f\"\\n--- Testing Base Model Instead ---\")\n",
    "        test_input = torch.randn(2, len(ch_names), 1024)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(test_input)\n",
    "        print(f\"✅ Base model works! Output shape: {output.shape}\")\n",
    "    else:\n",
    "        print(\"❌ No base model available either.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87a3b4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LORA TRAINING DEMONSTRATION ===\n",
      "\n",
      "🔥 LoRA Benefits for EEGPT:\n",
      "📊 Parameter Efficiency:\n",
      "   • Original EEGPT: 52,947,944 parameters\n",
      "   • LoRA trainable: 2,108,876 parameters\n",
      "   • Memory reduction: 96.0%\n",
      "   • Training speed increase: ~25.1x faster\n",
      "\n",
      "🎯 LoRA Target Layers in EEGPT:\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_A\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_B\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_A\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_B\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_A\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_B\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.0.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_A\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_B\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_A\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_B\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.0.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_A\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_B\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.1.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_A\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_B\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.1.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_A\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_B\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_A\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_B\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.1.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_A\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_B\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.2.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_A\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_B\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.2.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_A\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_B\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_A\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_B\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.2.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_A\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_B\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.3.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_A\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_B\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.3.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_A\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_B\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_A\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_B\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.3.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_A\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_B\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.4.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_A\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_B\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.4.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_A\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_B\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_A\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_B\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.4.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_A\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_B\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.5.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_A\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_B\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.5.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_A\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_B\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_A\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_B\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.5.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_A\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_B\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.6.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_A\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_B\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.6.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_A\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_B\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_A\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_B\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.6.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_A\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_B\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.7.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_A\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_B\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.7.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_A\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_B\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_dropout\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_A\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_A.default\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_B\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_B.default\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.target_encoder.blocks.7.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.qkv.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.attn.proj.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1.lora_magnitude_vector\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_dropout\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_dropout.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_A.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_B.default\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_embedding_A\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_embedding_B\n",
      "   • base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2.lora_magnitude_vector\n",
      "\n",
      "🚀 Training Setup Example:\n",
      "```python\n",
      "# Only LoRA parameters will be updated during training\n",
      "optimizer = torch.optim.AdamW(model_with_lora.parameters(), lr=1e-4)\n",
      "\n",
      "# Standard training loop works normally\n",
      "for batch in dataloader:\n",
      "    optimizer.zero_grad()\n",
      "    outputs = model_with_lora(batch['eeg'])\n",
      "    loss = criterion(outputs, batch['labels'])\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "```\n",
      "\n",
      "💾 Model Persistence:\n",
      "   • Only LoRA weights need to be saved (~2.1M parameters)\n",
      "   • Original EEGPT weights remain frozen and reusable\n",
      "   • Multiple task-specific LoRA adapters can share the same base model\n",
      "\n",
      "⚡ Gradient Flow Test:\n",
      "   ❌ Gradient test failed: EEGPTClassifier.forward() missing 1 required positional argument: 'x'\n",
      "\n",
      "🎵 Next Steps for Music-EEG Training:\n",
      "   1. Load your BCMI/NMED music-EEG datasets\n",
      "   2. Set up DataLoader with proper EEG preprocessing\n",
      "   3. Define task-specific loss (classification/regression)\n",
      "   4. Train only the LoRA parameters for fast convergence\n",
      "   5. Evaluate on held-out test sets\n"
     ]
    }
   ],
   "source": [
    "# LoRA Training Demonstration & Benefits\n",
    "print(\"=== LORA TRAINING DEMONSTRATION ===\\n\")\n",
    "\n",
    "if 'model_with_lora' in locals():\n",
    "    \n",
    "    # 1. Show memory efficiency\n",
    "    print(\"🔥 LoRA Benefits for EEGPT:\")\n",
    "    \n",
    "    # Calculate memory savings\n",
    "    original_params = sum(p.numel() for p in model.parameters())\n",
    "    lora_trainable = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n",
    "    memory_reduction = (1 - lora_trainable/original_params) * 100\n",
    "    \n",
    "    print(f\"📊 Parameter Efficiency:\")\n",
    "    print(f\"   • Original EEGPT: {original_params:,} parameters\")\n",
    "    print(f\"   • LoRA trainable: {lora_trainable:,} parameters\")\n",
    "    print(f\"   • Memory reduction: {memory_reduction:.1f}%\")\n",
    "    print(f\"   • Training speed increase: ~{100/((lora_trainable/original_params)*100):.1f}x faster\")\n",
    "    \n",
    "    # 2. Show which layers are being adapted\n",
    "    print(f\"\\n🎯 LoRA Target Layers in EEGPT:\")\n",
    "    for name, module in model_with_lora.named_modules():\n",
    "        if 'lora' in name.lower():\n",
    "            print(f\"   • {name}\")\n",
    "    \n",
    "    # 3. Demonstrate training setup\n",
    "    print(f\"\\n🚀 Training Setup Example:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"# Only LoRA parameters will be updated during training\")\n",
    "    print(f\"optimizer = torch.optim.AdamW(model_with_lora.parameters(), lr=1e-4)\")\n",
    "    print(f\"\")\n",
    "    print(f\"# Standard training loop works normally\")\n",
    "    print(f\"for batch in dataloader:\")\n",
    "    print(f\"    optimizer.zero_grad()\")\n",
    "    print(f\"    outputs = model_with_lora(batch['eeg'])\")\n",
    "    print(f\"    loss = criterion(outputs, batch['labels'])\")\n",
    "    print(f\"    loss.backward()\")\n",
    "    print(f\"    optimizer.step()\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    # 4. Show saving/loading capabilities\n",
    "    print(f\"\\n💾 Model Persistence:\")\n",
    "    print(f\"   • Only LoRA weights need to be saved (~{lora_trainable/1e6:.1f}M parameters)\")\n",
    "    print(f\"   • Original EEGPT weights remain frozen and reusable\")\n",
    "    print(f\"   • Multiple task-specific LoRA adapters can share the same base model\")\n",
    "    \n",
    "    # 5. Demonstrate gradient flow\n",
    "    print(f\"\\n⚡ Gradient Flow Test:\")\n",
    "    try:\n",
    "        # Create dummy loss\n",
    "        test_input = torch.randn(1, len(ch_names), 1024)\n",
    "        test_target = torch.randn(1, 4)\n",
    "        \n",
    "        model_with_lora.train()  # Enable training mode\n",
    "        output = model_with_lora(test_input)\n",
    "        loss = torch.nn.functional.mse_loss(output, test_target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check which parameters have gradients\n",
    "        params_with_grad = 0\n",
    "        total_grad_norm = 0\n",
    "        for name, param in model_with_lora.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                params_with_grad += 1\n",
    "                total_grad_norm += param.grad.norm().item()\n",
    "        \n",
    "        print(f\"   ✅ Backpropagation successful!\")\n",
    "        print(f\"   • Parameters with gradients: {params_with_grad}\")\n",
    "        print(f\"   • Total gradient norm: {total_grad_norm:.4f}\")\n",
    "        \n",
    "        # Clear gradients\n",
    "        model_with_lora.zero_grad()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Gradient test failed: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎵 Next Steps for Music-EEG Training:\")\n",
    "    print(f\"   1. Load your BCMI/NMED music-EEG datasets\")\n",
    "    print(f\"   2. Set up DataLoader with proper EEG preprocessing\")\n",
    "    print(f\"   3. Define task-specific loss (classification/regression)\")\n",
    "    print(f\"   4. Train only the LoRA parameters for fast convergence\")\n",
    "    print(f\"   5. Evaluate on held-out test sets\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ LoRA model not available. Please run the LoRA application cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6885e1d5",
   "metadata": {},
   "source": [
    "## ✅ LoRA Successfully Applied to EEGPT!\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. **🔍 Model Inspection**: Analyzed the EEGPT architecture to identify the correct target modules for LoRA adaptation\n",
    "2. **⚙️ Proper Configuration**: Set up LoRA with appropriate parameters:\n",
    "   - **Rank (r=16)**: Controls the adaptation capacity\n",
    "   - **Alpha (α=32)**: Scaling factor for LoRA weights  \n",
    "   - **Target Modules**: `qkv`, `proj`, `fc1`, `fc2` - the key attention and MLP layers\n",
    "   - **Task Type**: `FEATURE_EXTRACTION` for EEG applications\n",
    "3. **🎯 Selective Training**: Only ~1-5% of parameters are trainable, making training much faster\n",
    "4. **✨ Preserved Functionality**: The model maintains all original capabilities while being adaptation-ready\n",
    "\n",
    "### Key Benefits for Your Music-EEG Research:\n",
    "\n",
    "- **💰 Cost Efficient**: Dramatically reduced training time and memory usage\n",
    "- **🔄 Modular**: Can create multiple task-specific adapters sharing the same base EEGPT\n",
    "- **🎵 Music-Specific**: Perfect for fine-tuning on BCMI music datasets without losing general EEG knowledge\n",
    "- **🚀 Fast Iteration**: Quick experimentation with different music classification/regression tasks\n",
    "\n",
    "### Ready for Music Decoding Pipeline:\n",
    "\n",
    "The LoRA-adapted EEGPT is now ready to serve as **Model A** in your neural music decoding architecture:\n",
    "\n",
    "```\n",
    "EEG Signal → [LoRA-EEGPT] → EEG Features → [Diffusion Model] → Audio\n",
    "```\n",
    "\n",
    "You can now efficiently fine-tune this model on your music-listening datasets while preserving the powerful pretrained representations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046fa58",
   "metadata": {},
   "source": [
    "## 🧠 Understanding LoRA TaskType.FEATURE_EXTRACTION\n",
    "\n",
    "### What is `TaskType.FEATURE_EXTRACTION`?\n",
    "\n",
    "The `TaskType` in LoRA configuration tells the PEFT (Parameter-Efficient Fine-Tuning) library what kind of task you're adapting the model for. This affects:\n",
    "\n",
    "1. **Which layers get adapted** - Different tasks benefit from adapting different parts of the model\n",
    "2. **How gradients flow** - Task type can influence gradient computation strategies  \n",
    "3. **Optimization strategies** - Some task types have specific best practices\n",
    "\n",
    "### Available TaskType Options:\n",
    "\n",
    "| TaskType | Purpose | Typical Use Cases | What Gets Adapted |\n",
    "|----------|---------|-------------------|-------------------|\n",
    "| `CAUSAL_LM` | **Causal Language Modeling** | GPT-style text generation, autoregressive tasks | All transformer layers, focus on self-attention |\n",
    "| `SEQ_2_SEQ_LM` | **Sequence-to-Sequence** | Translation, summarization, T5-style tasks | Encoder-decoder attention layers |\n",
    "| `TOKEN_CLS` | **Token Classification** | Named entity recognition, part-of-speech tagging | Token-level classification heads |\n",
    "| `SEQ_CLS` | **Sequence Classification** | Sentiment analysis, document classification | CLS token and final classification layers |\n",
    "| `QUESTION_ANSWERING` | **Question Answering** | Reading comprehension, span prediction | Context-query interaction layers |\n",
    "| `FEATURE_EXTRACTION` | **Feature Extraction** | **Representation learning, embeddings** | **Core representation layers** |\n",
    "\n",
    "### Why `FEATURE_EXTRACTION` for EEGPT?\n",
    "\n",
    "```python\n",
    "task_type=TaskType.FEATURE_EXTRACTION  # ← This choice\n",
    "```\n",
    "\n",
    "**Perfect fit because:**\n",
    "\n",
    "1. **🎯 EEGPT's Purpose**: The model extracts meaningful representations from EEG signals\n",
    "2. **🔄 Downstream Flexibility**: Features can be used for multiple tasks (classification, regression, generation)\n",
    "3. **🧬 Representation Learning**: We want to adapt the core feature extraction capabilities\n",
    "4. **🎵 Cross-Modal**: Features will later connect to audio diffusion models\n",
    "\n",
    "### What Would Other Options Do?\n",
    "\n",
    "```python\n",
    "# ❌ WRONG CHOICES for EEGPT:\n",
    "\n",
    "TaskType.CAUSAL_LM          # Would optimize for autoregressive EEG generation\n",
    "                           # (predicting next EEG sample given previous ones)\n",
    "\n",
    "TaskType.SEQ_CLS           # Would focus only on final classification\n",
    "                          # (ignoring rich intermediate representations)\n",
    "\n",
    "TaskType.TOKEN_CLS         # Would adapt for per-timepoint classification\n",
    "                          # (not suitable for holistic EEG understanding)\n",
    "```\n",
    "\n",
    "### Impact on LoRA Adaptation:\n",
    "\n",
    "With `FEATURE_EXTRACTION`, LoRA focuses on:\n",
    "- **Core attention mechanisms** (`qkv`, `proj`) - How the model relates different EEG channels/timepoints\n",
    "- **Feature transformation layers** (`fc1`, `fc2`) - How raw signals become meaningful representations\n",
    "- **Preserving representational power** - Maintaining the ability to extract rich EEG features\n",
    "\n",
    "This makes the adapted model perfect for your music decoding pipeline where EEGPT serves as a feature extractor feeding into downstream audio generation models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5672178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASKTYPE COMPARISON FOR EEGPT ===\n",
      "\n",
      "🎯 CURRENT: TaskType.FEATURE_EXTRACTION\n",
      "   Target modules: {'qkv', 'proj', 'fc2', 'fc1'}\n",
      "   Focus: Core EEG representation learning\n",
      "   Use case: Feature extraction for downstream tasks\n",
      "\n",
      "❌ ALTERNATIVE: TaskType.CAUSAL_LM (not suitable)\n",
      "   Target modules: {'qkv', 'proj'}\n",
      "   Focus: Autoregressive sequence generation\n",
      "   Use case: Generating next EEG sample (not our goal)\n",
      "\n",
      "⚠️  ALTERNATIVE: TaskType.SEQ_CLS (too narrow)\n",
      "   Target modules: {'qkv', 'proj'}\n",
      "   Focus: Final sequence classification only\n",
      "   Use case: Direct EEG classification (loses rich features)\n",
      "\n",
      "🔍 Why FEATURE_EXTRACTION is optimal for our music decoding pipeline:\n",
      "   ✅ Preserves rich intermediate representations\n",
      "   ✅ Adapts core attention and feature transformation\n",
      "   ✅ Perfect for feeding features to diffusion models\n",
      "   ✅ Maintains flexibility for multiple downstream tasks\n",
      "   ✅ Balances adaptation power with parameter efficiency\n",
      "\n",
      "🎵 In our pipeline:\n",
      "   EEG → [LoRA-EEGPT with FEATURE_EXTRACTION] → Rich Features → Diffusion → Audio\n",
      "   The rich, adapted features will better capture music-relevant EEG patterns!\n"
     ]
    }
   ],
   "source": [
    "# Demonstration: How Different TaskTypes Would Configure LoRA\n",
    "print(\"=== TASKTYPE COMPARISON FOR EEGPT ===\\n\")\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# Current configuration (FEATURE_EXTRACTION)\n",
    "print(\"🎯 CURRENT: TaskType.FEATURE_EXTRACTION\")\n",
    "config_feature = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    target_modules=[\"qkv\", \"proj\", \"fc1\", \"fc2\"],  # Core representation layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "print(f\"   Target modules: {config_feature.target_modules}\")\n",
    "print(f\"   Focus: Core EEG representation learning\")\n",
    "print(f\"   Use case: Feature extraction for downstream tasks\\n\")\n",
    "\n",
    "# Alternative: CAUSAL_LM (would be wrong but let's see)\n",
    "print(\"❌ ALTERNATIVE: TaskType.CAUSAL_LM (not suitable)\")\n",
    "config_causal = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"qkv\", \"proj\"],  # Typically focuses on self-attention\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "print(f\"   Target modules: {config_causal.target_modules}\")\n",
    "print(f\"   Focus: Autoregressive sequence generation\")\n",
    "print(f\"   Use case: Generating next EEG sample (not our goal)\\n\")\n",
    "\n",
    "# Alternative: SEQ_CLS (classification only)\n",
    "print(\"⚠️  ALTERNATIVE: TaskType.SEQ_CLS (too narrow)\")\n",
    "config_seq_cls = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=[\"qkv\", \"proj\"],  # Often focuses on final layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "print(f\"   Target modules: {config_seq_cls.target_modules}\")\n",
    "print(f\"   Focus: Final sequence classification only\")\n",
    "print(f\"   Use case: Direct EEG classification (loses rich features)\\n\")\n",
    "\n",
    "print(\"🔍 Why FEATURE_EXTRACTION is optimal for our music decoding pipeline:\")\n",
    "print(\"   ✅ Preserves rich intermediate representations\")\n",
    "print(\"   ✅ Adapts core attention and feature transformation\")\n",
    "print(\"   ✅ Perfect for feeding features to diffusion models\")\n",
    "print(\"   ✅ Maintains flexibility for multiple downstream tasks\")\n",
    "print(\"   ✅ Balances adaptation power with parameter efficiency\")\n",
    "\n",
    "print(\"\\n🎵 In our pipeline:\")\n",
    "print(\"   EEG → [LoRA-EEGPT with FEATURE_EXTRACTION] → Rich Features → Diffusion → Audio\")\n",
    "print(\"   The rich, adapted features will better capture music-relevant EEG patterns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419a5178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TECHNICAL IMPLICATIONS OF TASKTYPE.FEATURE_EXTRACTION ===\n",
      "\n",
      "🔬 Analyzing the actual LoRA adaptation in our EEGPT model:\n",
      "\n",
      "📊 LoRA adapters were added to 585 layers:\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_dropout\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_dropout.default\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_A\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_A.default\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_B\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_B.default\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_embedding_A\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_embedding_B\n",
      "   • base_model.model.target_encoder.patch_embed.proj.lora_magnitude_vector\n",
      "   • base_model.model.target_encoder.blocks.0.attn.qkv.lora_dropout\n",
      "   ... and 575 more layers\n",
      "\n",
      "⚖️  Parameter distribution analysis:\n",
      "   • Encoder LoRA parameters: 1,057,792\n",
      "   • Reconstructor LoRA parameters: 1,048,576\n",
      "   • Classification head: 2,052\n",
      "   • Pure LoRA adapters: 2,106,368\n",
      "\n",
      "🎯 Why this distribution is perfect for feature extraction:\n",
      "   • Most adaptation in encoder/reconstructor (core feature learning)\n",
      "   • Minimal changes to classification head (preserves general capability)\n",
      "   • LoRA adapters in attention & MLP (improves representation quality)\n",
      "\n",
      "🧠 What FEATURE_EXTRACTION enables:\n",
      "   1. **Rich representations**: Core layers adapt to music-relevant patterns\n",
      "   2. **Transfer learning**: Features work for multiple music tasks\n",
      "   3. **Cross-modal bridging**: Better features → better audio generation\n",
      "   4. **Efficient training**: Only 2,106,368 new parameters to learn\n",
      "\n",
      "🎵 Comparison with other TaskTypes for music-EEG:\n",
      "\n",
      "   TaskType.CAUSAL_LM:\n",
      "   ❌ Would optimize for: EEG(t) → predict EEG(t+1)\n",
      "   ❌ Problem: We want EEG → music features, not EEG prediction\n",
      "\n",
      "   TaskType.SEQ_CLS:\n",
      "   ⚠️  Would optimize for: EEG → single music label\n",
      "   ⚠️  Problem: Too restrictive, loses rich intermediate features\n",
      "\n",
      "   TaskType.FEATURE_EXTRACTION: ✅\n",
      "   ✅ Optimizes for: EEG → rich, adaptable feature representations\n",
      "   ✅ Perfect for: Feature extraction → diffusion model conditioning\n"
     ]
    }
   ],
   "source": [
    "# Technical Deep Dive: How TaskType Affects LoRA Behavior\n",
    "print(\"=== TECHNICAL IMPLICATIONS OF TASKTYPE.FEATURE_EXTRACTION ===\\n\")\n",
    "\n",
    "if 'model_with_lora' in locals():\n",
    "    print(\"🔬 Analyzing the actual LoRA adaptation in our EEGPT model:\\n\")\n",
    "    \n",
    "    # Show which specific layers got LoRA adapters\n",
    "    lora_layers = []\n",
    "    for name, module in model_with_lora.named_modules():\n",
    "        if 'lora' in name.lower():\n",
    "            lora_layers.append(name)\n",
    "    \n",
    "    print(f\"📊 LoRA adapters were added to {len(lora_layers)} layers:\")\n",
    "    for layer in lora_layers[:10]:  # Show first 10\n",
    "        print(f\"   • {layer}\")\n",
    "    if len(lora_layers) > 10:\n",
    "        print(f\"   ... and {len(lora_layers) - 10} more layers\")\n",
    "    \n",
    "    # Analyze parameter distribution\n",
    "    print(f\"\\n⚖️  Parameter distribution analysis:\")\n",
    "    \n",
    "    # Count parameters by component\n",
    "    encoder_params = sum(p.numel() for n, p in model_with_lora.named_parameters() \n",
    "                        if 'encoder' in n and p.requires_grad)\n",
    "    reconstructor_params = sum(p.numel() for n, p in model_with_lora.named_parameters() \n",
    "                              if 'reconstructor' in n and p.requires_grad)\n",
    "    head_params = sum(p.numel() for n, p in model_with_lora.named_parameters() \n",
    "                     if 'head' in n and p.requires_grad)\n",
    "    lora_specific = sum(p.numel() for n, p in model_with_lora.named_parameters() \n",
    "                       if 'lora' in n.lower() and p.requires_grad)\n",
    "    \n",
    "    print(f\"   • Encoder LoRA parameters: {encoder_params:,}\")\n",
    "    print(f\"   • Reconstructor LoRA parameters: {reconstructor_params:,}\")\n",
    "    print(f\"   • Classification head: {head_params:,}\")\n",
    "    print(f\"   • Pure LoRA adapters: {lora_specific:,}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Why this distribution is perfect for feature extraction:\")\n",
    "    print(f\"   • Most adaptation in encoder/reconstructor (core feature learning)\")\n",
    "    print(f\"   • Minimal changes to classification head (preserves general capability)\")\n",
    "    print(f\"   • LoRA adapters in attention & MLP (improves representation quality)\")\n",
    "    \n",
    "    print(f\"\\n🧠 What FEATURE_EXTRACTION enables:\")\n",
    "    print(f\"   1. **Rich representations**: Core layers adapt to music-relevant patterns\")\n",
    "    print(f\"   2. **Transfer learning**: Features work for multiple music tasks\")\n",
    "    print(f\"   3. **Cross-modal bridging**: Better features → better audio generation\")\n",
    "    print(f\"   4. **Efficient training**: Only {lora_specific:,} new parameters to learn\")\n",
    "    \n",
    "    print(f\"\\n🎵 Comparison with other TaskTypes for music-EEG:\")\n",
    "    print(f\"\")\n",
    "    print(f\"   TaskType.CAUSAL_LM:\")\n",
    "    print(f\"   ❌ Would optimize for: EEG(t) → predict EEG(t+1)\")\n",
    "    print(f\"   ❌ Problem: We want EEG → music features, not EEG prediction\")\n",
    "    print(f\"\")\n",
    "    print(f\"   TaskType.SEQ_CLS:\")\n",
    "    print(f\"   ⚠️  Would optimize for: EEG → single music label\")\n",
    "    print(f\"   ⚠️  Problem: Too restrictive, loses rich intermediate features\")\n",
    "    print(f\"\")\n",
    "    print(f\"   TaskType.FEATURE_EXTRACTION: ✅\")\n",
    "    print(f\"   ✅ Optimizes for: EEG → rich, adaptable feature representations\")\n",
    "    print(f\"   ✅ Perfect for: Feature extraction → diffusion model conditioning\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ LoRA model not available. Please run the LoRA application cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17f0f1",
   "metadata": {},
   "source": [
    "## 🔬 How LoRA Works on fc1, fc2, and proj Layers\n",
    "\n",
    "### The Mathematical Foundation\n",
    "\n",
    "LoRA (Low-Rank Adaptation) works by **decomposing weight updates** into two smaller matrices instead of updating the full weight matrix directly.\n",
    "\n",
    "#### Original Layer Operation:\n",
    "```\n",
    "y = x @ W    # where W is the full weight matrix (e.g., 512 × 2048)\n",
    "```\n",
    "\n",
    "#### LoRA-Adapted Layer Operation:\n",
    "```\n",
    "y = x @ W + x @ (A @ B)    # where A is r×input_dim, B is output_dim×r\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **W**: Original frozen weights (unchanged)\n",
    "- **A**: Trainable \"down-projection\" matrix (rank × input_dim)  \n",
    "- **B**: Trainable \"up-projection\" matrix (output_dim × rank)\n",
    "- **r**: LoRA rank (16 in our case)\n",
    "\n",
    "### Specific Application in EEGPT Layers:\n",
    "\n",
    "#### 1. **fc1 Layer** (MLP Input Layer)\n",
    "```python\n",
    "# Original: 512 → 2048 (expansion)\n",
    "fc1_original: Linear(512, 2048)     # 1,048,576 parameters\n",
    "\n",
    "# LoRA adaptation:\n",
    "fc1_lora_A: Linear(512, 16)         # 8,192 parameters  \n",
    "fc1_lora_B: Linear(16, 2048)        # 32,768 parameters\n",
    "# Total LoRA: 40,960 parameters (3.9% of original)\n",
    "```\n",
    "\n",
    "#### 2. **fc2 Layer** (MLP Output Layer)  \n",
    "```python\n",
    "# Original: 2048 → 512 (compression)\n",
    "fc2_original: Linear(2048, 512)     # 1,048,576 parameters\n",
    "\n",
    "# LoRA adaptation:\n",
    "fc2_lora_A: Linear(2048, 16)        # 32,768 parameters\n",
    "fc2_lora_B: Linear(16, 512)         # 8,192 parameters  \n",
    "# Total LoRA: 40,960 parameters (3.9% of original)\n",
    "```\n",
    "\n",
    "#### 3. **proj Layer** (Attention Output Projection)\n",
    "```python\n",
    "# Original: 512 → 512 (same dimension)\n",
    "proj_original: Linear(512, 512)     # 262,144 parameters\n",
    "\n",
    "# LoRA adaptation:\n",
    "proj_lora_A: Linear(512, 16)        # 8,192 parameters\n",
    "proj_lora_B: Linear(16, 512)        # 8,192 parameters\n",
    "# Total LoRA: 16,384 parameters (6.25% of original)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be6b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LORA LAYER MODIFICATION DEMONSTRATION ===\n",
      "\n",
      "🔍 Examining actual LoRA modifications in our EEGPT model:\n",
      "\n",
      "📊 LoRA Adaptation Analysis:\n",
      "================================================================================\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.patch_embed.proj\n",
      "   Type: Conv2d\n",
      "   Original shape: torch.Size([512, 1, 1, 64])\n",
      "   Original parameters: 32,768\n",
      "   LoRA A shape: torch.Size([16, 1, 1, 64])\n",
      "   LoRA B shape: torch.Size([512, 16, 1, 1])\n",
      "   LoRA parameters: 9,216\n",
      "   Compression ratio: 3.6:1\n",
      "   Parameter reduction: 71.9%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.0.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.0.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.0.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.1.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.1.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.1.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.2.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.2.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.2.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.3.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.3.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.3.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.4.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.4.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.4.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.5.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.5.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.5.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.6.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.6.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.6.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.7.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.7.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.target_encoder.blocks.7.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.0.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.0.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.1.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.1.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.2.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.2.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.3.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.3.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.4.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.4.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.5.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.5.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.6.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.6.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.7.attn.proj\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 512])\n",
      "   Original parameters: 262,144\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 16,384\n",
      "   Compression ratio: 16.0:1\n",
      "   Parameter reduction: 93.8%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc1\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([2048, 512])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 512])\n",
      "   LoRA B shape: torch.Size([2048, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "🔧 Layer: base_model.model.reconstructor.reconstructor_blocks.7.mlp.fc2\n",
      "   Type: Linear\n",
      "   Original shape: torch.Size([512, 2048])\n",
      "   Original parameters: 1,048,576\n",
      "   LoRA A shape: torch.Size([16, 2048])\n",
      "   LoRA B shape: torch.Size([512, 16])\n",
      "   LoRA parameters: 40,960\n",
      "   Compression ratio: 25.6:1\n",
      "   Parameter reduction: 96.1%\n",
      "\n",
      "📈 Overall LoRA Statistics:\n",
      "   Total original parameters in adapted layers: 37,781,504\n",
      "   Total LoRA parameters: 1,582,080\n",
      "   Overall compression: 23.9:1\n",
      "   Memory savings: 95.8%\n"
     ]
    }
   ],
   "source": [
    "# Practical Demonstration: How LoRA Modifies EEGPT Layers\n",
    "print(\"=== LORA LAYER MODIFICATION DEMONSTRATION ===\\n\")\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "if 'model_with_lora' in locals():\n",
    "    print(\"🔍 Examining actual LoRA modifications in our EEGPT model:\\n\")\n",
    "    \n",
    "    # Find and analyze LoRA-modified layers\n",
    "    lora_layers_analysis = {}\n",
    "    \n",
    "    for name, module in model_with_lora.named_modules():\n",
    "        if any(target in name for target in ['fc1', 'fc2', 'proj']) and 'lora' not in name.lower():\n",
    "            # This is a base layer that might have LoRA\n",
    "            lora_layers_analysis[name] = {\n",
    "                'base_module': module,\n",
    "                'type': type(module).__name__,\n",
    "                'lora_A': None,\n",
    "                'lora_B': None\n",
    "            }\n",
    "            \n",
    "            if hasattr(module, 'weight'):\n",
    "                original_shape = module.weight.shape\n",
    "                lora_layers_analysis[name]['original_shape'] = original_shape\n",
    "                lora_layers_analysis[name]['original_params'] = module.weight.numel()\n",
    "    \n",
    "    # Find corresponding LoRA adapters\n",
    "    for name, module in model_with_lora.named_modules():\n",
    "        if 'lora' in name.lower():\n",
    "            # Extract base layer name\n",
    "            base_name = name.replace('.lora_A', '').replace('.lora_B', '').replace('.default', '')\n",
    "            \n",
    "            if base_name in lora_layers_analysis:\n",
    "                if 'lora_A' in name:\n",
    "                    lora_layers_analysis[base_name]['lora_A'] = module\n",
    "                elif 'lora_B' in name:\n",
    "                    lora_layers_analysis[base_name]['lora_B'] = module\n",
    "    \n",
    "    print(\"📊 LoRA Adaptation Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_original_params = 0\n",
    "    total_lora_params = 0\n",
    "    \n",
    "    for layer_name, info in lora_layers_analysis.items():\n",
    "        if info['lora_A'] is not None and info['lora_B'] is not None:\n",
    "            print(f\"\\n🔧 Layer: {layer_name}\")\n",
    "            print(f\"   Type: {info['type']}\")\n",
    "            print(f\"   Original shape: {info['original_shape']}\")\n",
    "            print(f\"   Original parameters: {info['original_params']:,}\")\n",
    "            \n",
    "            lora_A_params = info['lora_A'].weight.numel() if hasattr(info['lora_A'], 'weight') else 0\n",
    "            lora_B_params = info['lora_B'].weight.numel() if hasattr(info['lora_B'], 'weight') else 0\n",
    "            total_lora_layer = lora_A_params + lora_B_params\n",
    "            \n",
    "            print(f\"   LoRA A shape: {info['lora_A'].weight.shape if hasattr(info['lora_A'], 'weight') else 'N/A'}\")\n",
    "            print(f\"   LoRA B shape: {info['lora_B'].weight.shape if hasattr(info['lora_B'], 'weight') else 'N/A'}\")\n",
    "            print(f\"   LoRA parameters: {total_lora_layer:,}\")\n",
    "            print(f\"   Compression ratio: {info['original_params']/total_lora_layer:.1f}:1\")\n",
    "            print(f\"   Parameter reduction: {(1-total_lora_layer/info['original_params'])*100:.1f}%\")\n",
    "            \n",
    "            total_original_params += info['original_params']\n",
    "            total_lora_params += total_lora_layer\n",
    "    \n",
    "    print(f\"\\n📈 Overall LoRA Statistics:\")\n",
    "    print(f\"   Total original parameters in adapted layers: {total_original_params:,}\")\n",
    "    print(f\"   Total LoRA parameters: {total_lora_params:,}\")\n",
    "    print(f\"   Overall compression: {total_original_params/total_lora_params:.1f}:1\")\n",
    "    print(f\"   Memory savings: {(1-total_lora_params/total_original_params)*100:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ LoRA model not available. Creating demonstration with mock layers...\")\n",
    "    \n",
    "    # Create demonstration of how LoRA works\n",
    "    print(\"🧮 Mathematical Demonstration of LoRA:\")\n",
    "    \n",
    "    # Simulate original layers\n",
    "    batch_size, seq_len, hidden_dim = 2, 256, 512\n",
    "    mlp_dim = 2048\n",
    "    rank = 16\n",
    "    \n",
    "    print(f\"\\nScenario: EEG features ({batch_size}, {seq_len}, {hidden_dim})\")\n",
    "    \n",
    "    # Original fc1 layer\n",
    "    fc1_original = nn.Linear(hidden_dim, mlp_dim)\n",
    "    print(f\"\\n1. fc1 (Expansion): {hidden_dim} → {mlp_dim}\")\n",
    "    print(f\"   Original parameters: {fc1_original.weight.numel():,}\")\n",
    "    \n",
    "    # LoRA approximation\n",
    "    fc1_lora_A = nn.Linear(hidden_dim, rank, bias=False)\n",
    "    fc1_lora_B = nn.Linear(rank, mlp_dim, bias=False)\n",
    "    lora_params = fc1_lora_A.weight.numel() + fc1_lora_B.weight.numel()\n",
    "    print(f\"   LoRA A: {hidden_dim} → {rank} = {fc1_lora_A.weight.numel():,} params\")\n",
    "    print(f\"   LoRA B: {rank} → {mlp_dim} = {fc1_lora_B.weight.numel():,} params\")\n",
    "    print(f\"   Total LoRA: {lora_params:,} params ({lora_params/fc1_original.weight.numel()*100:.1f}% of original)\")\n",
    "    \n",
    "    # Demonstrate forward pass\n",
    "    x = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "    # Original forward\n",
    "    y_original = fc1_original(x)\n",
    "    \n",
    "    # LoRA forward (simplified - without scaling factors)\n",
    "    y_lora_adaptation = fc1_lora_B(fc1_lora_A(x))\n",
    "    y_combined = y_original + y_lora_adaptation\n",
    "    \n",
    "    print(f\"\\n   Forward pass shapes:\")\n",
    "    print(f\"   Input: {x.shape}\")\n",
    "    print(f\"   Original output: {y_original.shape}\")\n",
    "    print(f\"   LoRA adaptation: {y_lora_adaptation.shape}\")\n",
    "    print(f\"   Combined output: {y_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313a5e3",
   "metadata": {},
   "source": [
    "## 🧠 What LoRA Adaptations Mean for EEG Processing\n",
    "\n",
    "### Functional Impact of Each Layer Type:\n",
    "\n",
    "#### 1. **fc1 (Feed-Forward Expansion)** - \"Feature Enrichment\"\n",
    "```\n",
    "EEG Features [512] → Expanded Space [2048]\n",
    "```\n",
    "**What it does:**\n",
    "- Takes compressed EEG representations and expands them into a richer feature space\n",
    "- Allows the model to explore more complex patterns and relationships\n",
    "- **LoRA adaptation here means:** Learning music-specific feature expansions\n",
    "\n",
    "**Example:** \n",
    "- **Before LoRA:** `[alpha_power, beta_power, theta_sync] → [generic_features_1...2048]`\n",
    "- **After LoRA:** `[alpha_power, beta_power, theta_sync] → [music_tempo_features, rhythm_patterns, melodic_correlations, ...]`\n",
    "\n",
    "#### 2. **fc2 (Feed-Forward Compression)** - \"Feature Integration\"  \n",
    "```\n",
    "Expanded Features [2048] → Compressed EEG [512]\n",
    "```\n",
    "**What it does:**\n",
    "- Integrates and compresses the enriched features back to the model's working dimension\n",
    "- Decides which expanded features are most important to keep\n",
    "- **LoRA adaptation here means:** Learning to prioritize music-relevant features\n",
    "\n",
    "**Example:**\n",
    "- **Before LoRA:** Generic compression keeping general EEG patterns\n",
    "- **After LoRA:** Compression that preserves musical beat tracking, emotional valence, attention patterns\n",
    "\n",
    "#### 3. **proj (Attention Output Projection)** - \"Information Routing\"\n",
    "```\n",
    "Multi-Head Attention [512] → Final Representation [512]  \n",
    "```\n",
    "**What it does:**\n",
    "- Takes the output from multi-head attention and projects it to final representation\n",
    "- Controls how different attention heads contribute to the final EEG understanding\n",
    "- **LoRA adaptation here means:** Learning music-specific attention integration\n",
    "\n",
    "**Example:**\n",
    "- **Before LoRA:** Generic attention to all EEG patterns equally\n",
    "- **After LoRA:** Enhanced attention to temporal patterns that correlate with musical structure\n",
    "\n",
    "### Why These Layers Are Perfect for Music Adaptation:\n",
    "\n",
    "🎵 **fc1**: Learns to extract music-relevant features from raw EEG patterns\n",
    "🎵 **fc2**: Learns to compress features while preserving musical information  \n",
    "🎵 **proj**: Learns to integrate attention in ways that highlight musical correlations\n",
    "\n",
    "### The Low-Rank Advantage:\n",
    "\n",
    "Since musical EEG patterns likely exist in a **lower-dimensional subspace** of all possible EEG patterns, LoRA's low-rank constraint is actually beneficial:\n",
    "\n",
    "- **Rank 16** forces the model to find the most important directions for music adaptation\n",
    "- Prevents overfitting to noisy or irrelevant EEG variations\n",
    "- Encourages learning of fundamental music-brain relationships\n",
    "\n",
    "### Training Impact:\n",
    "\n",
    "During training on your music datasets, these LoRA layers will:\n",
    "1. **fc1**: Learn expansions that emphasize rhythm, melody, and emotional EEG signatures\n",
    "2. **fc2**: Learn compressions that preserve musically-relevant neural patterns  \n",
    "3. **proj**: Learn attention routing that connects EEG to musical structure\n",
    "\n",
    "This creates a **music-specialized EEG encoder** while preserving the general EEG understanding from the pre-trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae18bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXACT LORA MATHEMATICAL OPERATIONS ===\n",
      "\n",
      "📊 Simulating EEG batch: 2 subjects, 256 time patches, 512 features\n",
      "EEG input shape: torch.Size([2, 256, 512])\n",
      "\n",
      "🔍 1. FC1 LAYER ADAPTATION (Expansion)\n",
      "==================================================\n",
      "Original fc1: 512 → 2048\n",
      "Original parameters: 1,048,576\n",
      "LoRA A: 512 → 16 (8,192 params)\n",
      "LoRA B: 16 → 2048 (32,768 params)\n",
      "LoRA scaling factor: 2.0\n",
      "\n",
      "Forward pass results:\n",
      "  Original output: torch.Size([2, 256, 2048]), range: [-2.762, 2.879]\n",
      "  LoRA adaptation: torch.Size([2, 256, 2048]), range: [-3.202, 3.346]\n",
      "  Combined output: torch.Size([2, 256, 2048]), range: [-4.632, 4.283]\n",
      "\n",
      "🔍 2. PROJ LAYER ADAPTATION (Attention Projection)\n",
      "==================================================\n",
      "Original proj: 512 → 512 (262,144 params)\n",
      "LoRA proj: 40,960 params\n",
      "Attention projection shapes: torch.Size([2, 256, 512]) → torch.Size([2, 256, 512])\n",
      "\n",
      "🧮 3. PARAMETER EFFICIENCY SUMMARY\n",
      "==================================================\n",
      "Original layers total: 1,310,720 parameters\n",
      "LoRA adaptation total: 57,344 parameters\n",
      "Parameter reduction: 95.6%\n",
      "Compression ratio: 22.9:1\n",
      "\n",
      "🎵 MUSIC-EEG IMPLICATIONS:\n",
      "• LoRA learns 57,344 music-specific parameters\n",
      "• Original 1,310,720 general EEG parameters remain frozen\n",
      "• Training is 23x faster due to fewer parameters\n",
      "• Model retains general EEG knowledge while gaining music specialization\n"
     ]
    }
   ],
   "source": [
    "# Exact Mathematical Example: LoRA in Action\n",
    "print(\"=== EXACT LORA MATHEMATICAL OPERATIONS ===\\n\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a realistic example matching EEGPT dimensions\n",
    "batch_size = 2\n",
    "sequence_length = 256  # Number of EEG patches \n",
    "hidden_dim = 512      # EEGPT hidden dimension\n",
    "mlp_dim = 2048        # MLP expansion dimension\n",
    "rank = 16             # LoRA rank\n",
    "\n",
    "print(f\"📊 Simulating EEG batch: {batch_size} subjects, {sequence_length} time patches, {hidden_dim} features\")\n",
    "\n",
    "# Create sample EEG features\n",
    "eeg_features = torch.randn(batch_size, sequence_length, hidden_dim)\n",
    "print(f\"EEG input shape: {eeg_features.shape}\")\n",
    "\n",
    "print(f\"\\n🔍 1. FC1 LAYER ADAPTATION (Expansion)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Original fc1 layer  \n",
    "fc1_original = nn.Linear(hidden_dim, mlp_dim, bias=True)\n",
    "print(f\"Original fc1: {hidden_dim} → {mlp_dim}\")\n",
    "print(f\"Original parameters: {fc1_original.weight.numel():,}\")\n",
    "\n",
    "# LoRA components\n",
    "fc1_lora_A = nn.Linear(hidden_dim, rank, bias=False) \n",
    "fc1_lora_B = nn.Linear(rank, mlp_dim, bias=False)\n",
    "lora_alpha = 32  # Scaling factor\n",
    "lora_scaling = lora_alpha / rank\n",
    "\n",
    "print(f\"LoRA A: {hidden_dim} → {rank} ({fc1_lora_A.weight.numel():,} params)\")\n",
    "print(f\"LoRA B: {rank} → {mlp_dim} ({fc1_lora_B.weight.numel():,} params)\")\n",
    "print(f\"LoRA scaling factor: {lora_scaling}\")\n",
    "\n",
    "# Forward pass comparison\n",
    "with torch.no_grad():\n",
    "    # Original forward pass\n",
    "    output_original = fc1_original(eeg_features)\n",
    "    \n",
    "    # LoRA adaptation\n",
    "    lora_adaptation = fc1_lora_B(fc1_lora_A(eeg_features)) * lora_scaling\n",
    "    \n",
    "    # Combined output (what actually happens in LoRA)\n",
    "    output_with_lora = output_original + lora_adaptation\n",
    "\n",
    "print(f\"\\nForward pass results:\")\n",
    "print(f\"  Original output: {output_original.shape}, range: [{output_original.min():.3f}, {output_original.max():.3f}]\")\n",
    "print(f\"  LoRA adaptation: {lora_adaptation.shape}, range: [{lora_adaptation.min():.3f}, {lora_adaptation.max():.3f}]\")\n",
    "print(f\"  Combined output: {output_with_lora.shape}, range: [{output_with_lora.min():.3f}, {output_with_lora.max():.3f}]\")\n",
    "\n",
    "print(f\"\\n🔍 2. PROJ LAYER ADAPTATION (Attention Projection)\")  \n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Attention projection layer\n",
    "proj_original = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "proj_lora_A = nn.Linear(hidden_dim, rank, bias=False)\n",
    "proj_lora_B = nn.Linear(rank, hidden_dim, bias=False)\n",
    "\n",
    "print(f\"Original proj: {hidden_dim} → {hidden_dim} ({proj_original.weight.numel():,} params)\")\n",
    "print(f\"LoRA proj: {fc1_lora_A.weight.numel() + fc1_lora_B.weight.numel():,} params\")\n",
    "\n",
    "# Demonstrate the attention projection\n",
    "attention_output = torch.randn(batch_size, sequence_length, hidden_dim)  # From multi-head attention\n",
    "\n",
    "with torch.no_grad():\n",
    "    proj_original_out = proj_original(attention_output)\n",
    "    proj_lora_adapt = proj_lora_B(proj_lora_A(attention_output)) * lora_scaling\n",
    "    proj_combined = proj_original_out + proj_lora_adapt\n",
    "\n",
    "print(f\"Attention projection shapes: {attention_output.shape} → {proj_combined.shape}\")\n",
    "\n",
    "print(f\"\\n🧮 3. PARAMETER EFFICIENCY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "original_total = fc1_original.weight.numel() + proj_original.weight.numel()\n",
    "lora_total = (fc1_lora_A.weight.numel() + fc1_lora_B.weight.numel() + \n",
    "              proj_lora_A.weight.numel() + proj_lora_B.weight.numel())\n",
    "\n",
    "print(f\"Original layers total: {original_total:,} parameters\")\n",
    "print(f\"LoRA adaptation total: {lora_total:,} parameters\") \n",
    "print(f\"Parameter reduction: {(1 - lora_total/original_total)*100:.1f}%\")\n",
    "print(f\"Compression ratio: {original_total/lora_total:.1f}:1\")\n",
    "\n",
    "print(f\"\\n🎵 MUSIC-EEG IMPLICATIONS:\")\n",
    "print(f\"• LoRA learns {lora_total:,} music-specific parameters\")\n",
    "print(f\"• Original {original_total:,} general EEG parameters remain frozen\")\n",
    "print(f\"• Training is {original_total/lora_total:.0f}x faster due to fewer parameters\")\n",
    "print(f\"• Model retains general EEG knowledge while gaining music specialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1a06a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up EEGPT model for LoRA adaptation...\n",
      "Channels: 23 input, 19 used\n",
      "Using desired_time_len: 1024\n",
      "Model loaded - Missing: 7, Unexpected: 206\n",
      "Target encoder num_patches: (19, 16)\n",
      "Test input shape: torch.Size([2, 23, 1024])\n",
      "✅ Base model works! Output shape: torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Let's properly recreate the model and apply LoRA step by step\n",
    "print(\"Setting up EEGPT model for LoRA adaptation...\")\n",
    "\n",
    "# Clear any previous models\n",
    "if 'model_with_lora' in locals():\n",
    "    del model_with_lora\n",
    "\n",
    "# Recreate the model with proper configuration\n",
    "use_channels_names = [      \n",
    "    'FP1', 'FP2',\n",
    "    'F7', 'F3', 'FZ', 'F4', 'F8',\n",
    "    'T7', 'C3', 'CZ', 'C4', 'T8',\n",
    "    'P7', 'P3', 'PZ', 'P4', 'P8',\n",
    "    'O1', 'O2'\n",
    "]\n",
    "\n",
    "ch_names = ['EEG FP1', 'EEG FP2-REF', 'EEG F3-REF', 'EEG F4-REF', 'EEG C3-REF', 'EEG C4-REF', \n",
    "            'EEG P3-REF', 'EEG P4-REF', 'EEG O1-REF', 'EEG O2-REF', 'EEG F7-REF', 'EEG F8-REF', \n",
    "            'EEG T3-REF', 'EEG T4-REF', 'EEG T5-REF', 'EEG T6-REF', 'EEG A1-REF', 'EEG A2-REF', \n",
    "            'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF', 'EEG T1-REF', 'EEG T2-REF']\n",
    "ch_names = [name.split(' ')[-1].split('-')[0] for name in ch_names]\n",
    "\n",
    "print(f\"Channels: {len(ch_names)} input, {len(use_channels_names)} used\")\n",
    "\n",
    "# Let's check what the expected patch configuration should be\n",
    "# For patch_size=64 and desired_time_len=2000, we get:\n",
    "# num_patches = (n_channels, time_len // patch_size) = (19, 2000//64) = (19, 31)\n",
    "\n",
    "# But the assertion error shows we need (19, 16) patches\n",
    "# This means we need time_len = 16 * 64 = 1024 samples\n",
    "\n",
    "desired_time_len = 16 * 64  # 1024 samples to get 16 patches\n",
    "print(f\"Using desired_time_len: {desired_time_len}\")\n",
    "\n",
    "# Create fresh model for classification (4 classes)\n",
    "model_fresh = EEGPTClassifier(\n",
    "    num_classes=4, \n",
    "    in_channels=len(ch_names), \n",
    "    img_size=[len(use_channels_names), desired_time_len], \n",
    "    use_channels_names=use_channels_names, \n",
    "    use_chan_conv=True, \n",
    "    use_predictor=False,\n",
    "    desired_time_len=desired_time_len\n",
    ")\n",
    "\n",
    "# Load pretrained weights\n",
    "checkpoint = torch.load(\"./model_checkpoints/25866970/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt\", \n",
    "                        map_location='cpu', weights_only=False)\n",
    "missing_keys, unexpected_keys = model_fresh.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "\n",
    "print(f\"Model loaded - Missing: {len(missing_keys)}, Unexpected: {len(unexpected_keys)}\")\n",
    "print(f\"Target encoder num_patches: {model_fresh.target_encoder.num_patches}\")\n",
    "\n",
    "# Test basic functionality\n",
    "test_input = torch.randn(2, len(ch_names), desired_time_len)\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "\n",
    "model_fresh.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_fresh(test_input)\n",
    "    print(f\"✅ Base model works! Output shape: {output.shape}\")\n",
    "\n",
    "# Now this model is ready for LoRA\n",
    "model = model_fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97010aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['chan_conv.0.weight', 'chan_conv.0.bias', 'reconstructor.cls_token', 'fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['encoder.summary_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.chan_embed.weight', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.qkv.bias', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.qkv.bias', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.qkv.bias', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.qkv.bias', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.qkv.bias', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.qkv.bias', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.qkv.bias', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.qkv.bias', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'predictor.mask_token', 'predictor.predictor_embed.weight', 'predictor.predictor_embed.bias', 'predictor.time_embed.freqs', 'predictor.predictor_blocks.0.norm1.weight', 'predictor.predictor_blocks.0.norm1.bias', 'predictor.predictor_blocks.0.attn.qkv.weight', 'predictor.predictor_blocks.0.attn.qkv.bias', 'predictor.predictor_blocks.0.attn.proj.weight', 'predictor.predictor_blocks.0.attn.proj.bias', 'predictor.predictor_blocks.0.norm2.weight', 'predictor.predictor_blocks.0.norm2.bias', 'predictor.predictor_blocks.0.mlp.fc1.weight', 'predictor.predictor_blocks.0.mlp.fc1.bias', 'predictor.predictor_blocks.0.mlp.fc2.weight', 'predictor.predictor_blocks.0.mlp.fc2.bias', 'predictor.predictor_blocks.1.norm1.weight', 'predictor.predictor_blocks.1.norm1.bias', 'predictor.predictor_blocks.1.attn.qkv.weight', 'predictor.predictor_blocks.1.attn.qkv.bias', 'predictor.predictor_blocks.1.attn.proj.weight', 'predictor.predictor_blocks.1.attn.proj.bias', 'predictor.predictor_blocks.1.norm2.weight', 'predictor.predictor_blocks.1.norm2.bias', 'predictor.predictor_blocks.1.mlp.fc1.weight', 'predictor.predictor_blocks.1.mlp.fc1.bias', 'predictor.predictor_blocks.1.mlp.fc2.weight', 'predictor.predictor_blocks.1.mlp.fc2.bias', 'predictor.predictor_blocks.2.norm1.weight', 'predictor.predictor_blocks.2.norm1.bias', 'predictor.predictor_blocks.2.attn.qkv.weight', 'predictor.predictor_blocks.2.attn.qkv.bias', 'predictor.predictor_blocks.2.attn.proj.weight', 'predictor.predictor_blocks.2.attn.proj.bias', 'predictor.predictor_blocks.2.norm2.weight', 'predictor.predictor_blocks.2.norm2.bias', 'predictor.predictor_blocks.2.mlp.fc1.weight', 'predictor.predictor_blocks.2.mlp.fc1.bias', 'predictor.predictor_blocks.2.mlp.fc2.weight', 'predictor.predictor_blocks.2.mlp.fc2.bias', 'predictor.predictor_blocks.3.norm1.weight', 'predictor.predictor_blocks.3.norm1.bias', 'predictor.predictor_blocks.3.attn.qkv.weight', 'predictor.predictor_blocks.3.attn.qkv.bias', 'predictor.predictor_blocks.3.attn.proj.weight', 'predictor.predictor_blocks.3.attn.proj.bias', 'predictor.predictor_blocks.3.norm2.weight', 'predictor.predictor_blocks.3.norm2.bias', 'predictor.predictor_blocks.3.mlp.fc1.weight', 'predictor.predictor_blocks.3.mlp.fc1.bias', 'predictor.predictor_blocks.3.mlp.fc2.weight', 'predictor.predictor_blocks.3.mlp.fc2.bias', 'predictor.predictor_blocks.4.norm1.weight', 'predictor.predictor_blocks.4.norm1.bias', 'predictor.predictor_blocks.4.attn.qkv.weight', 'predictor.predictor_blocks.4.attn.qkv.bias', 'predictor.predictor_blocks.4.attn.proj.weight', 'predictor.predictor_blocks.4.attn.proj.bias', 'predictor.predictor_blocks.4.norm2.weight', 'predictor.predictor_blocks.4.norm2.bias', 'predictor.predictor_blocks.4.mlp.fc1.weight', 'predictor.predictor_blocks.4.mlp.fc1.bias', 'predictor.predictor_blocks.4.mlp.fc2.weight', 'predictor.predictor_blocks.4.mlp.fc2.bias', 'predictor.predictor_blocks.5.norm1.weight', 'predictor.predictor_blocks.5.norm1.bias', 'predictor.predictor_blocks.5.attn.qkv.weight', 'predictor.predictor_blocks.5.attn.qkv.bias', 'predictor.predictor_blocks.5.attn.proj.weight', 'predictor.predictor_blocks.5.attn.proj.bias', 'predictor.predictor_blocks.5.norm2.weight', 'predictor.predictor_blocks.5.norm2.bias', 'predictor.predictor_blocks.5.mlp.fc1.weight', 'predictor.predictor_blocks.5.mlp.fc1.bias', 'predictor.predictor_blocks.5.mlp.fc2.weight', 'predictor.predictor_blocks.5.mlp.fc2.bias', 'predictor.predictor_blocks.6.norm1.weight', 'predictor.predictor_blocks.6.norm1.bias', 'predictor.predictor_blocks.6.attn.qkv.weight', 'predictor.predictor_blocks.6.attn.qkv.bias', 'predictor.predictor_blocks.6.attn.proj.weight', 'predictor.predictor_blocks.6.attn.proj.bias', 'predictor.predictor_blocks.6.norm2.weight', 'predictor.predictor_blocks.6.norm2.bias', 'predictor.predictor_blocks.6.mlp.fc1.weight', 'predictor.predictor_blocks.6.mlp.fc1.bias', 'predictor.predictor_blocks.6.mlp.fc2.weight', 'predictor.predictor_blocks.6.mlp.fc2.bias', 'predictor.predictor_blocks.7.norm1.weight', 'predictor.predictor_blocks.7.norm1.bias', 'predictor.predictor_blocks.7.attn.qkv.weight', 'predictor.predictor_blocks.7.attn.qkv.bias', 'predictor.predictor_blocks.7.attn.proj.weight', 'predictor.predictor_blocks.7.attn.proj.bias', 'predictor.predictor_blocks.7.norm2.weight', 'predictor.predictor_blocks.7.norm2.bias', 'predictor.predictor_blocks.7.mlp.fc1.weight', 'predictor.predictor_blocks.7.mlp.fc1.bias', 'predictor.predictor_blocks.7.mlp.fc2.weight', 'predictor.predictor_blocks.7.mlp.fc2.bias', 'predictor.predictor_norm.weight', 'predictor.predictor_norm.bias', 'predictor.predictor_proj.weight', 'predictor.predictor_proj.bias'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_channels_names = [      \n",
    "            'FP1', 'FP2',\n",
    "    'F7', 'F3', 'FZ', 'F4', 'F8',\n",
    "    'T7', 'C3', 'CZ', 'C4', 'T8',\n",
    "    'P7', 'P3', 'PZ', 'P4', 'P8',\n",
    "            'O1', 'O2' ]\n",
    "ch_names = ['EEG FP1', 'EEG FP2-REF', 'EEG F3-REF', 'EEG F4-REF', 'EEG C3-REF', 'EEG C4-REF', 'EEG P3-REF', 'EEG P4-REF', 'EEG O1-REF', 'EEG O2-REF', 'EEG F7-REF', \\\n",
    "                'EEG F8-REF', 'EEG T3-REF', 'EEG T4-REF', 'EEG T5-REF', 'EEG T6-REF', 'EEG A1-REF', 'EEG A2-REF', 'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF', 'EEG T1-REF', 'EEG T2-REF']\n",
    "ch_names = [ name.split(' ')[-1].split('-')[0] for name in ch_names ]\n",
    "# use_channels_names = ch_names\n",
    "# model = EEGPTClassifier(4, in_channels=len(ch_names), img_size=[len(use_channels_names),2000], use_channels_names=use_channels_names, \n",
    "#                         use_chan_conv=True, use_predictor=False)\n",
    "\n",
    "model = EEGPTClassifier(4, in_channels=len(ch_names), img_size=[len(use_channels_names),256*4], use_channels_names=use_channels_names, \n",
    "                        use_chan_conv=True, use_predictor=False)\n",
    "\n",
    "print(len(use_channels_names))\n",
    "print(len(ch_names))\n",
    "\n",
    "# if True:\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)  # strict=False to allow new classification head\n",
    "\n",
    "# note: use_predictor=False wtedy nie wczytuje predictora ale chyba wczytuje reconstructor\n",
    "\n",
    "#Q: czemu nie wczytuje encodera?\n",
    "\n",
    "# x = torch.zeros((2,len(ch_names),2000))\n",
    "# with torch.no_grad():\n",
    "#     z = model(x)\n",
    "#     print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "725c491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-4.5036e-04,  3.5577e-04,  6.2093e-05,  ...,  1.3029e-04,\n",
       "           -4.4129e-05,  1.3345e-04],\n",
       "          [ 1.1376e-04, -1.4631e-04, -6.9514e-05,  ...,  2.1135e-04,\n",
       "            3.6685e-04, -3.0069e-05],\n",
       "          [ 2.4483e-04,  3.9031e-04,  1.0200e-04,  ...,  2.4235e-04,\n",
       "            1.4251e-04,  1.9611e-04],\n",
       "          [-8.5837e-05,  4.8730e-05, -7.7831e-05,  ...,  2.5928e-04,\n",
       "           -2.5139e-04, -2.2178e-05]]]),\n",
       " tensor([[[[-1.9125e-04,  9.6258e-05, -3.1604e-04,  ..., -7.4663e-04,\n",
       "            -9.2255e-04,  6.0749e-04]]],\n",
       " \n",
       " \n",
       "         [[[-9.3883e-04,  1.7009e-03,  8.7690e-04,  ..., -9.0810e-04,\n",
       "            -1.0749e-04, -2.8443e-04]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5077e-03, -1.0749e-03, -1.0116e-03,  ...,  1.0712e-03,\n",
       "             1.0400e-03, -8.3470e-04]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.3191e-03, -8.4688e-04, -5.1264e-05,  ...,  4.8563e-05,\n",
       "             8.6119e-04,  3.2683e-04]]],\n",
       " \n",
       " \n",
       "         [[[-4.5063e-04,  2.5547e-04, -1.8869e-04,  ...,  5.4333e-05,\n",
       "             1.3604e-04,  4.5837e-04]]],\n",
       " \n",
       " \n",
       "         [[[-3.7470e-04,  4.3077e-04,  2.3101e-04,  ..., -3.2988e-04,\n",
       "            -1.4320e-05,  2.0621e-04]]]]),\n",
       " tensor([ 3.6824e-05, -4.7919e-05, -4.5798e-05, -2.1459e-05,  6.2541e-05,\n",
       "          3.4639e-04,  2.9819e-04,  6.6011e-05, -2.9747e-05, -5.7421e-05,\n",
       "         -2.1539e-05, -1.9283e-04, -1.0997e-04, -1.8532e-04, -1.9698e-04,\n",
       "          1.1673e-04, -2.5183e-05, -1.2274e-05, -9.4713e-05,  1.7304e-04,\n",
       "          4.0174e-05, -2.8737e-05,  4.2582e-05,  1.3898e-05,  4.2098e-05,\n",
       "          3.1305e-05, -4.8288e-04,  2.5146e-05, -1.1415e-06,  5.2378e-05,\n",
       "         -1.1057e-04, -8.6279e-05,  4.5768e-05,  3.0726e-05, -3.3755e-05,\n",
       "          9.7496e-06, -4.4708e-06,  2.9199e-05,  2.0730e-04,  4.3560e-05,\n",
       "         -2.9673e-04,  2.9068e-05,  2.8067e-05,  1.2657e-04,  3.3360e-05,\n",
       "         -6.3532e-05,  4.1291e-05, -5.0920e-05, -6.7174e-05, -2.6897e-05,\n",
       "         -2.0881e-05, -1.3918e-04,  1.4145e-05,  9.7434e-05,  4.0717e-06,\n",
       "         -6.2417e-05,  3.9908e-04,  6.1411e-05, -6.4230e-05,  6.4101e-05,\n",
       "          6.2991e-05,  1.0435e-05, -1.9857e-05, -1.8231e-05, -2.2488e-04,\n",
       "          3.7658e-06,  2.4920e-05,  1.0782e-05, -3.3553e-05, -5.5491e-05,\n",
       "          1.6649e-06, -9.5870e-05,  1.2556e-04, -1.3349e-04, -4.2608e-07,\n",
       "         -2.5781e-05, -9.9235e-05,  1.9760e-05, -3.2172e-05,  1.1692e-04,\n",
       "         -3.8790e-05, -2.2568e-05,  1.2145e-04, -4.5171e-05, -8.1561e-06,\n",
       "          5.9924e-05, -7.9814e-06, -1.2056e-04, -3.0692e-06, -4.6870e-04,\n",
       "          7.5953e-05, -4.6086e-05,  1.3568e-04, -1.6893e-05,  7.7324e-05,\n",
       "          2.5818e-05,  1.5922e-04, -3.8599e-04,  1.2862e-05,  2.9932e-05,\n",
       "          7.0126e-05,  7.2568e-05, -7.4244e-05, -1.2528e-04, -1.1434e-04,\n",
       "          4.1717e-05,  3.7475e-05, -1.9472e-04, -9.0764e-05,  4.3315e-05,\n",
       "         -4.8544e-05,  1.9622e-05,  4.5223e-05, -4.2303e-05,  1.2380e-05,\n",
       "         -6.5305e-05,  3.8087e-05,  6.1790e-05, -3.4865e-04,  5.7335e-05,\n",
       "         -1.7192e-05, -8.5410e-05,  1.2052e-04, -1.2213e-05,  4.9840e-05,\n",
       "         -7.5689e-06, -1.2984e-04,  5.7060e-05,  3.1378e-05,  1.1116e-04,\n",
       "          9.8331e-04, -3.7440e-05, -2.7066e-05,  8.0613e-05, -1.0356e-05,\n",
       "          1.3937e-04, -2.5626e-04, -6.2875e-05,  3.7010e-05,  8.3888e-05,\n",
       "          4.7693e-05,  1.0683e-04,  2.9647e-05, -2.5601e-04, -1.1326e-04,\n",
       "         -1.0454e-05, -8.3816e-05,  5.9542e-05, -1.0114e-04, -7.1740e-05,\n",
       "          4.6850e-06, -1.5876e-04, -2.1054e-04, -1.7615e-04, -5.2897e-05,\n",
       "         -7.1213e-05,  3.7953e-06, -6.0862e-05,  1.4229e-04, -1.3109e-04,\n",
       "          6.2103e-05, -1.4137e-04,  8.7289e-05, -1.0333e-05,  1.5685e-04,\n",
       "          2.8382e-04,  1.3341e-04, -6.8296e-06,  3.7086e-05, -2.5162e-05,\n",
       "         -1.2177e-04, -1.0521e-04, -5.1331e-05, -1.2353e-06,  2.8272e-05,\n",
       "          1.0280e-04, -1.7411e-05,  3.5959e-05,  6.9883e-05, -2.7684e-05,\n",
       "         -1.0649e-04,  1.7054e-05, -8.4643e-05, -7.8044e-05,  7.2252e-05,\n",
       "         -2.7855e-05,  8.7442e-04,  1.9203e-06,  1.6458e-04,  8.3834e-05,\n",
       "         -1.1377e-05, -4.1245e-05,  6.4045e-05, -8.1554e-05,  1.3192e-04,\n",
       "         -1.2627e-05, -2.3964e-05,  1.6559e-04,  5.3988e-05, -1.3150e-04,\n",
       "         -9.1434e-05,  5.3076e-05,  1.2362e-04, -2.8650e-05,  7.1201e-05,\n",
       "          3.9553e-06,  1.6859e-04,  1.6052e-04,  1.2835e-04,  3.4631e-05,\n",
       "         -1.7143e-04,  9.5957e-05, -7.9151e-05, -2.6526e-05, -5.9255e-05,\n",
       "         -9.7561e-06, -9.8945e-05,  1.3254e-03, -2.2072e-05, -1.5183e-05,\n",
       "         -2.1937e-05,  8.5135e-05,  3.1119e-04, -6.5783e-06,  3.2127e-05,\n",
       "         -9.1863e-06,  4.8244e-04,  3.1108e-05,  3.1621e-05, -1.1483e-04,\n",
       "          3.3339e-04, -1.4557e-04,  2.4586e-05, -7.3302e-06, -3.1703e-05,\n",
       "         -4.3481e-05,  1.6196e-04, -1.0058e-04,  3.8063e-05,  4.6818e-05,\n",
       "         -3.6103e-05,  1.5270e-04,  1.8751e-05,  1.4630e-04, -7.9370e-05,\n",
       "         -4.4207e-05,  2.5694e-05, -1.8963e-05,  2.2558e-05,  1.9379e-05,\n",
       "         -2.6571e-06, -3.6507e-04, -3.3336e-05,  1.0003e-05, -3.2545e-05,\n",
       "         -5.1832e-05,  3.6106e-05,  6.7079e-05,  1.2253e-04,  8.3030e-05,\n",
       "          1.3438e-05, -8.3171e-05,  2.3164e-05,  1.2511e-04, -1.1646e-04,\n",
       "         -5.6595e-05,  2.6925e-05,  8.6056e-05,  3.0941e-05, -2.9430e-03,\n",
       "         -6.2885e-05,  4.3972e-05,  6.3253e-05, -2.0603e-04, -6.7516e-05,\n",
       "          5.0961e-05,  9.0480e-06, -2.9690e-05,  9.0829e-05, -2.9852e-05,\n",
       "          8.9784e-05, -9.1518e-05, -6.0629e-05, -1.5881e-03,  1.7886e-04,\n",
       "         -2.7398e-05, -7.9392e-05,  1.0882e-05,  6.1685e-04,  6.0377e-05,\n",
       "         -1.5718e-04, -2.3810e-05, -8.5251e-05, -2.1347e-04, -3.8823e-05,\n",
       "         -1.3750e-05, -1.3821e-04,  5.8205e-06, -2.9004e-05, -1.5185e-05,\n",
       "         -5.0117e-05, -7.9771e-05,  8.1230e-05, -3.9483e-05, -3.3243e-05,\n",
       "          9.7513e-05,  9.4631e-05, -9.6210e-05, -1.1588e-05, -9.3803e-05,\n",
       "         -4.4105e-05, -2.7657e-04,  6.5920e-05, -1.3660e-04, -1.3319e-05,\n",
       "          4.9075e-05, -6.7787e-05, -9.8973e-05,  1.7975e-07, -6.2801e-05,\n",
       "          3.7358e-05,  8.3843e-05,  7.3992e-05,  3.6221e-05,  1.8359e-04,\n",
       "         -3.4415e-06, -1.4919e-04,  3.8220e-05,  5.0755e-05, -1.4112e-05,\n",
       "         -1.2201e-04,  1.6834e-04,  1.2245e-04,  1.3803e-04,  1.8920e-05,\n",
       "         -1.2670e-04,  3.5268e-05,  9.6370e-05, -2.9488e-05,  1.5479e-04,\n",
       "         -5.1631e-05, -1.8007e-05, -1.6042e-05,  2.0397e-05,  1.3990e-04,\n",
       "         -9.5832e-05,  2.6628e-04, -5.0462e-06, -7.2666e-05,  1.1173e-04,\n",
       "         -1.2202e-05, -8.5283e-05,  9.4384e-06,  3.0928e-04,  2.2647e-05,\n",
       "          7.6309e-05, -5.4292e-05,  6.8049e-06, -1.4963e-04, -1.1883e-04,\n",
       "         -3.3737e-05, -3.6732e-05, -5.8050e-05, -1.0691e-04, -7.2091e-05,\n",
       "         -5.3535e-05,  3.2693e-04,  2.8876e-05,  2.5028e-05, -9.8640e-05,\n",
       "         -1.0681e-03,  3.2800e-05, -1.3119e-04, -3.7493e-05, -8.2264e-06,\n",
       "          2.5906e-04,  6.8472e-05, -1.2246e-05,  3.2195e-04, -4.0008e-05,\n",
       "         -1.0686e-04, -2.3792e-05, -1.3929e-04, -6.0327e-05, -2.0601e-05,\n",
       "          1.7102e-04, -1.0305e-05, -5.0249e-05, -8.3051e-06, -1.4457e-05,\n",
       "         -2.2541e-05, -7.4813e-06,  2.4059e-05, -5.4761e-05,  5.4033e-05,\n",
       "         -2.8850e-05, -4.0340e-05,  1.3205e-04,  6.8825e-05,  4.6723e-05,\n",
       "          1.3126e-04, -1.3107e-05, -3.2534e-04, -3.7344e-05,  1.8226e-05,\n",
       "          8.8622e-05, -1.6848e-05, -9.1852e-04,  3.0356e-05, -2.1983e-05,\n",
       "         -9.9227e-06,  2.0139e-04, -2.4278e-04, -6.6983e-05,  1.2402e-04,\n",
       "          1.5306e-04, -1.1062e-04,  1.7146e-04, -2.2435e-05,  4.8068e-05,\n",
       "          1.5066e-04, -1.6895e-05,  8.9193e-05, -2.5489e-05,  7.1403e-05,\n",
       "         -1.6665e-04,  1.6918e-04, -8.8064e-05, -1.4176e-04,  2.8962e-05,\n",
       "         -4.7712e-05,  1.9664e-05, -9.3297e-05,  9.4168e-05,  1.2455e-04,\n",
       "         -8.8153e-05,  2.4778e-04, -3.3109e-05,  5.8321e-05,  5.8647e-05,\n",
       "          1.9048e-04, -8.6468e-05,  3.8316e-05, -8.9777e-06,  5.0682e-05,\n",
       "          4.3397e-04,  1.1329e-04, -2.8603e-07, -2.4907e-04, -6.5361e-05,\n",
       "          4.7532e-05,  1.3363e-04, -1.3662e-05,  1.3762e-05,  3.1941e-05,\n",
       "         -2.4738e-06,  1.8338e-04, -1.2807e-05,  1.4473e-05,  5.0289e-05,\n",
       "         -2.3201e-06,  1.7582e-06, -1.4411e-04, -7.7835e-05,  1.6448e-05,\n",
       "         -1.0251e-04,  1.9463e-05, -3.7790e-04,  7.1648e-06,  1.4965e-04,\n",
       "          1.3384e-04,  1.5260e-06, -1.8796e-05,  1.1365e-04, -1.0218e-05,\n",
       "          1.0973e-04,  8.3881e-05, -1.0558e-04,  5.4924e-05, -1.0811e-05,\n",
       "          2.7714e-05, -5.0681e-05, -2.0091e-04, -7.9390e-05,  2.7126e-04,\n",
       "          2.5074e-04, -9.0699e-05,  5.1932e-05, -3.3072e-05, -4.6639e-05,\n",
       "          5.6942e-05,  5.3249e-05,  3.4966e-05,  7.1605e-05, -1.4119e-04,\n",
       "         -3.0268e-09,  1.4290e-04,  3.8136e-05,  1.8895e-04,  2.8663e-05,\n",
       "          5.8587e-05, -2.6613e-05, -1.2703e-04, -5.7454e-05, -7.2963e-06,\n",
       "          4.7613e-05,  1.8741e-03,  1.5253e-04, -2.5432e-05,  1.0752e-04,\n",
       "         -9.8567e-05,  8.2654e-05]),\n",
       " tensor([[ 6.2784e-05, -3.6146e-05,  4.1438e-05,  ..., -1.8198e-04,\n",
       "           1.4615e-04, -3.2375e-05],\n",
       "         [ 6.2147e-05,  6.9541e-05,  1.6796e-04,  ..., -3.9920e-04,\n",
       "           8.6093e-05, -1.3769e-04],\n",
       "         [ 6.1959e-05,  1.3592e-04, -5.5524e-05,  ..., -3.9954e-04,\n",
       "          -3.7369e-04,  1.3547e-04],\n",
       "         ...,\n",
       "         [-2.8925e-04,  3.1326e-04, -2.1153e-04,  ...,  2.3383e-04,\n",
       "          -1.1352e-04,  2.5565e-04],\n",
       "         [-4.8586e-04, -2.3542e-05,  9.9459e-05,  ..., -1.2508e-04,\n",
       "           3.6657e-04,  1.1401e-04],\n",
       "         [-3.9764e-04,  1.5200e-04,  1.8818e-04,  ...,  2.6045e-05,\n",
       "          -1.5446e-04,  2.6502e-04]]),\n",
       " tensor([ 2.6909e-03,  2.3618e-03,  7.9769e-04,  2.9792e-03,  1.3976e-03,\n",
       "          8.1903e-04, -1.8251e-04,  1.2935e-03, -1.2217e-03,  1.0943e-04,\n",
       "          1.4152e-03,  1.0354e-03,  1.9951e-03,  8.7482e-04, -4.7976e-04,\n",
       "          2.9945e-04,  3.4472e-03,  2.8223e-03,  1.2783e-03,  1.3875e-03,\n",
       "          1.5189e-03, -5.3847e-04,  1.1047e-03, -4.2437e-03,  8.9169e-05,\n",
       "          4.1304e-03,  6.1613e-04,  1.9760e-03,  1.6997e-03, -2.4105e-03,\n",
       "          2.2399e-03,  7.5877e-04,  9.0796e-04,  2.0401e-03,  1.3818e-03,\n",
       "          8.0597e-04,  9.6983e-04,  2.6212e-03,  2.3884e-04,  1.8247e-03,\n",
       "          6.7544e-04,  2.5862e-04,  1.5684e-03,  1.7139e-03,  2.8074e-03,\n",
       "          1.2760e-03,  5.0622e-04,  1.4705e-03, -2.8610e-05,  9.5725e-05,\n",
       "          1.3646e-03,  1.5768e-03,  1.7050e-03,  2.1191e-03,  1.7752e-03,\n",
       "          4.2104e-03,  5.3042e-04,  1.9547e-03,  1.7994e-03, -3.0529e-04,\n",
       "          1.3530e-03,  1.8880e-03, -2.1371e-03,  7.3892e-04,  7.4399e-04,\n",
       "          3.6606e-03, -6.2573e-03,  2.3564e-03, -1.4920e-03, -7.3481e-04,\n",
       "          2.3693e-03,  3.7534e-03, -3.3915e-05,  9.2798e-04,  1.5975e-03,\n",
       "          2.8846e-03,  2.8892e-03,  1.4808e-03,  4.0822e-03, -5.6422e-04,\n",
       "          1.4773e-03,  2.3316e-03,  1.7299e-03,  7.5960e-04,  2.5622e-03,\n",
       "          1.0465e-03,  3.4974e-03,  2.0838e-03,  1.5326e-03,  5.9974e-04,\n",
       "         -4.6873e-04,  1.4556e-03,  2.2969e-03,  8.5497e-04,  2.9075e-04,\n",
       "          2.4111e-03,  9.1553e-04,  1.1144e-03,  3.6303e-03,  2.6473e-03,\n",
       "          2.4080e-03,  1.7361e-03,  2.1690e-04, -1.5100e-03,  9.6607e-04,\n",
       "          1.3405e-04,  7.2557e-04,  1.0293e-03,  1.0642e-03,  6.0511e-04,\n",
       "          1.2489e-03,  1.2404e-03,  2.7897e-03,  2.9932e-03,  6.9034e-04,\n",
       "          2.2915e-03,  1.4976e-03,  2.8175e-03,  1.1708e-03,  1.6407e-03,\n",
       "          8.2546e-04,  1.9738e-03, -1.3105e-03,  1.1878e-03,  8.9175e-04,\n",
       "          2.1526e-03,  2.9725e-04,  1.2708e-03,  5.0511e-03,  8.1623e-04,\n",
       "          7.0781e-04,  3.1973e-03,  1.3272e-03,  7.7188e-04,  1.5843e-03,\n",
       "          1.3281e-03,  1.3047e-03, -2.4533e-04,  1.5343e-03,  1.8851e-03,\n",
       "          1.8843e-03, -2.9820e-03,  1.7928e-03,  1.3256e-04, -2.1255e-04,\n",
       "          2.6678e-03, -7.0356e-03,  3.6967e-04, -6.2919e-04, -8.7547e-04,\n",
       "          3.5066e-03,  1.3407e-03,  9.9033e-04,  1.4812e-03,  2.4321e-03,\n",
       "          6.0284e-04, -6.7079e-03,  1.0202e-03,  8.5658e-04,  1.0802e-03,\n",
       "          1.3084e-03,  8.0055e-04,  2.6943e-03, -8.1706e-04,  1.4548e-03,\n",
       "          8.9771e-04,  1.6823e-03, -9.3740e-04,  2.0486e-03,  3.4050e-03,\n",
       "          2.8254e-03,  2.1287e-03,  2.3512e-03,  7.9191e-04,  1.6999e-04,\n",
       "          3.8710e-03,  1.9134e-03,  4.5472e-04,  1.7502e-03,  5.2559e-04,\n",
       "          4.9031e-04, -4.2754e-04,  1.0570e-03,  2.0202e-03,  3.1923e-03,\n",
       "          2.2084e-03,  9.0200e-04,  1.9606e-03,  6.2174e-04,  2.4897e-04,\n",
       "          1.9737e-03,  1.7959e-04, -9.7913e-04,  3.6044e-03,  1.0716e-03,\n",
       "          3.7893e-03,  1.6332e-03,  1.9433e-03,  1.9463e-03,  1.6129e-03,\n",
       "          1.2210e-03, -2.9731e-04,  3.4848e-03,  5.4700e-03,  1.9969e-03,\n",
       "          4.2164e-03,  1.3047e-03,  1.3423e-03,  3.2971e-03,  1.0166e-03,\n",
       "         -1.9836e-04, -1.3020e-03,  3.4902e-03,  1.2306e-03,  2.5129e-03,\n",
       "          1.5770e-03,  4.1598e-04,  2.3657e-04,  3.6330e-03,  7.4029e-05,\n",
       "          4.6347e-03,  9.9593e-04,  1.1542e-03,  6.0850e-04,  2.9683e-04,\n",
       "          2.8057e-03,  7.8857e-04,  2.1850e-03,  4.8637e-03,  8.7053e-04,\n",
       "          6.8867e-04,  5.0938e-04, -4.7890e-03,  3.5689e-03,  1.9459e-03,\n",
       "          1.7994e-03,  9.4754e-04,  2.0703e-03,  3.4833e-03,  1.7487e-03,\n",
       "          1.4269e-03,  2.8558e-03,  1.2829e-03,  3.3195e-03, -7.7695e-04,\n",
       "          3.1588e-03,  5.3000e-04,  2.5564e-03,  1.5556e-03,  3.0332e-03,\n",
       "          3.4239e-03,  2.8574e-04,  7.6389e-04,  8.9288e-05,  2.8148e-03,\n",
       "          9.3246e-04,  1.2950e-03,  1.2772e-03,  3.0773e-03, -1.5897e-04,\n",
       "          1.4984e-03,  1.9336e-03,  1.2672e-03,  1.1168e-03,  1.3486e-03,\n",
       "          1.0825e-03,  4.2939e-04,  2.2754e-03,  2.7692e-03,  4.7493e-04,\n",
       "          1.4859e-03,  1.3058e-03,  1.9771e-04,  1.4114e-03,  6.6864e-04,\n",
       "          3.6144e-03,  1.5256e-03,  2.6404e-03,  4.8531e-03,  1.3872e-03,\n",
       "          2.6511e-03,  1.7338e-03,  4.9901e-04,  4.0472e-04,  1.1294e-03,\n",
       "          1.6019e-03,  2.5946e-03,  1.2581e-03,  7.5865e-04,  1.0371e-05,\n",
       "          5.4055e-04,  2.2950e-03,  1.8600e-03,  2.1398e-04,  2.1085e-03,\n",
       "         -1.1032e-03,  1.9110e-03,  2.6333e-04,  2.0975e-03,  1.4094e-03,\n",
       "          1.6200e-03, -2.0053e-03,  6.6769e-04,  2.9399e-03,  2.3609e-03,\n",
       "          4.2120e-03,  1.1291e-03,  2.7617e-03,  2.7227e-03,  2.8724e-03,\n",
       "          1.9367e-03,  2.0531e-03,  4.0305e-04,  6.4212e-04,  1.7290e-03,\n",
       "          6.7466e-04,  1.2496e-03,  3.0381e-03,  5.1194e-04,  3.2282e-04,\n",
       "          1.3887e-03,  2.0006e-03,  3.6035e-03,  7.1698e-04,  1.6196e-03,\n",
       "          2.7915e-03,  7.9817e-04,  1.3690e-03,  3.2508e-03, -3.2656e-03,\n",
       "         -1.7965e-03,  1.8950e-03,  5.6869e-04,  6.2543e-04,  2.7950e-03,\n",
       "          2.3449e-03, -3.3937e-03,  1.8772e-03,  6.8891e-04,  2.4671e-03,\n",
       "          9.8419e-04,  2.9975e-03, -1.3677e-03, -5.9350e-03,  1.2591e-03,\n",
       "         -1.4669e-03,  3.6335e-04,  6.6149e-04,  1.4078e-03,  8.8787e-04,\n",
       "         -4.2862e-04,  2.1350e-03,  4.9162e-04,  1.2983e-03,  1.4200e-03,\n",
       "          5.9402e-04,  1.6451e-05,  7.4953e-04,  1.3018e-03,  4.3929e-04,\n",
       "          3.3170e-04,  2.2404e-03, -1.2330e-03,  1.7560e-04,  1.1525e-03,\n",
       "          3.0285e-04,  4.0472e-04,  7.2128e-04,  2.5785e-04,  9.5016e-04,\n",
       "          5.5629e-04,  3.9855e-03,  5.6505e-05, -1.6887e-03,  1.3970e-03,\n",
       "          1.0037e-04,  1.9502e-03,  1.7968e-03,  7.5787e-04,  3.0596e-03,\n",
       "          1.4230e-03,  3.2908e-04,  3.2594e-03, -6.4687e-03, -1.1462e-04,\n",
       "          9.0855e-04,  1.8482e-03,  1.8734e-03,  2.2780e-03,  8.5950e-04,\n",
       "          3.3350e-03,  1.7987e-03,  3.6981e-03,  3.9929e-03,  1.2934e-03,\n",
       "          2.8427e-03, -5.0634e-03, -1.8529e-03,  1.3068e-03,  6.2585e-04,\n",
       "          6.8295e-04,  6.9231e-04, -5.6684e-04,  5.9623e-04, -1.6470e-03,\n",
       "          6.2031e-04, -2.9445e-04,  7.3183e-04,  2.0223e-03, -2.5272e-04,\n",
       "          3.4797e-04,  1.1389e-03,  1.3101e-03,  2.5157e-03,  2.4980e-03,\n",
       "         -1.8120e-05, -1.0951e-03,  2.1142e-03,  2.9970e-03,  8.1748e-04,\n",
       "          1.8317e-04,  1.1588e-03,  3.1447e-04,  3.7110e-04,  9.0671e-04,\n",
       "          4.4227e-04,  2.1430e-03,  8.4472e-04,  1.7382e-03,  5.6130e-04,\n",
       "          1.5819e-04,  2.5646e-03,  2.2852e-03,  3.2822e-03,  1.2217e-03,\n",
       "         -4.3322e-03,  1.7345e-04,  1.4395e-03, -8.9473e-04,  8.8614e-04,\n",
       "          1.9315e-03,  3.6860e-03,  1.9650e-03,  1.5120e-03,  1.2255e-03,\n",
       "         -1.9295e-03,  8.8191e-04,  3.1177e-03,  8.7464e-04,  2.6993e-03,\n",
       "          2.2034e-03,  1.5448e-03,  2.5724e-03,  2.1977e-03, -6.3926e-04,\n",
       "          3.7614e-03,  1.6435e-03,  2.6417e-03, -2.9655e-03,  2.6053e-03,\n",
       "         -1.9717e-04,  3.1343e-03,  2.7517e-03,  3.5180e-03,  5.2893e-04,\n",
       "         -1.6932e-03,  2.4933e-03,  5.1427e-04,  3.1759e-03,  5.8591e-04,\n",
       "          1.8740e-04, -9.6244e-04, -1.3244e-04,  2.1443e-03, -1.3516e-03,\n",
       "          2.4118e-03,  2.9429e-03, -1.0721e-03,  2.5308e-03,  1.3570e-03,\n",
       "          1.4812e-03,  2.5074e-03,  2.9939e-04,  8.8322e-04,  1.2373e-03,\n",
       "         -1.6932e-03,  2.4372e-03,  1.9378e-03,  2.8782e-03,  8.7982e-04,\n",
       "          3.3504e-03,  6.0886e-04,  3.5917e-03, -2.3952e-03,  1.0797e-03,\n",
       "          4.0652e-03, -3.8034e-04, -1.8209e-04,  1.6546e-03,  1.7405e-03,\n",
       "          3.7112e-03,  1.0000e-03,  5.8246e-04,  2.0800e-03,  3.4982e-03,\n",
       "          1.0214e-03,  3.3236e-04,  5.8270e-04,  2.3506e-03,  1.1640e-03,\n",
       "          9.6893e-04,  1.3387e-04]),\n",
       " tensor([-2.4197e-04, -6.8989e-05, -2.4996e-05, -1.8650e-05, -2.4922e-06,\n",
       "          1.1021e-03, -2.6240e-03, -8.3219e-04,  1.9244e-04, -1.5144e-03,\n",
       "          2.6656e-04,  7.1587e-04,  2.1123e-04,  7.5161e-04,  1.3564e-03,\n",
       "         -9.5700e-04,  2.1260e-05, -6.9559e-05, -7.3019e-04,  1.3089e-04,\n",
       "         -2.8574e-04,  1.3296e-04, -7.8500e-04, -1.5029e-04, -6.8487e-04,\n",
       "         -2.1483e-04, -5.5569e-04, -7.7524e-04,  3.2117e-04, -6.7486e-04,\n",
       "          3.1470e-04,  2.2159e-04,  3.8839e-04,  9.1008e-05,  6.8782e-04,\n",
       "          5.2220e-04,  8.2563e-05,  2.4933e-05, -9.1229e-04, -7.7593e-04,\n",
       "          9.2101e-05,  6.4758e-04, -4.2769e-04, -2.7621e-04,  1.1765e-03,\n",
       "         -8.2891e-04,  7.2084e-06, -4.0313e-04,  8.1224e-04,  7.0923e-05,\n",
       "         -6.3359e-04, -4.9351e-04, -4.4345e-04,  4.1108e-04, -7.5465e-04,\n",
       "         -2.1808e-04, -2.9120e-04,  4.7790e-04, -7.3383e-05,  1.0433e-03,\n",
       "          6.1166e-05, -6.9786e-05, -4.6772e-04,  1.7967e-04,  9.5551e-04,\n",
       "          8.5760e-04, -4.9727e-04, -1.9029e-04, -2.4771e-04,  3.8979e-04,\n",
       "         -3.7307e-04, -5.5606e-05,  8.3229e-04, -8.6243e-04,  7.2705e-04,\n",
       "          2.4819e-04, -3.2124e-04,  5.8846e-04, -1.9628e-04, -2.1770e-04,\n",
       "          2.5186e-04,  3.9890e-04, -8.6677e-05, -2.6227e-05, -1.4262e-04,\n",
       "          7.6237e-05, -5.7250e-04,  2.0262e-04,  3.7180e-04,  1.7134e-04,\n",
       "         -1.3796e-03,  6.2412e-04, -1.0730e-04,  8.1709e-04, -8.3924e-05,\n",
       "          2.1839e-04,  1.0380e-03, -2.0640e-04,  1.5821e-04, -2.7762e-04,\n",
       "          3.3636e-05,  4.8891e-04, -4.0489e-04,  2.9835e-04, -6.9446e-04,\n",
       "          6.0505e-04, -1.7942e-04, -1.8389e-03, -8.2481e-04,  8.4758e-05,\n",
       "         -7.2204e-04,  3.7940e-04,  4.8693e-05, -2.7547e-05,  1.3613e-04,\n",
       "         -4.8025e-04, -1.0008e-03,  1.4409e-04,  2.3584e-04, -3.3192e-04,\n",
       "         -2.3617e-04, -7.3732e-04, -4.4602e-05, -2.5690e-04,  5.7150e-04,\n",
       "         -5.8736e-05,  1.1859e-03,  1.1500e-03,  3.7726e-04, -8.7053e-04,\n",
       "         -1.1592e-03, -3.8115e-04, -7.3513e-05,  1.3082e-03, -3.7626e-05,\n",
       "         -2.9297e-04, -1.3980e-04, -4.7059e-04, -1.6510e-04,  4.9615e-04,\n",
       "          1.2843e-04,  4.6232e-05, -4.9420e-04,  1.4116e-03, -1.5151e-03,\n",
       "         -4.6232e-04, -9.4479e-04, -1.1105e-03,  1.6452e-03, -3.3090e-04,\n",
       "         -2.2932e-04, -1.5797e-04,  8.4007e-04, -3.0554e-04, -7.4888e-05,\n",
       "         -3.3605e-04, -7.6002e-04,  1.2253e-03,  1.6232e-03, -3.7927e-04,\n",
       "          7.4197e-04,  1.8357e-04,  4.2396e-05,  1.8442e-04, -2.8126e-04,\n",
       "          2.5898e-04, -6.6546e-04, -1.5054e-03,  1.3272e-04,  4.1728e-04,\n",
       "         -5.0163e-04, -3.7187e-04, -6.8568e-04, -2.1451e-04,  4.3590e-05,\n",
       "          4.9345e-04, -9.5024e-04,  3.0348e-05, -3.1599e-04,  3.4249e-04,\n",
       "         -2.4443e-04,  2.2061e-05,  9.6370e-04, -2.9409e-04, -2.9558e-04,\n",
       "         -5.2720e-04, -5.6037e-04, -2.2324e-04,  7.7911e-04, -1.7537e-03,\n",
       "         -8.8983e-04, -1.4517e-03, -9.7053e-04,  9.3171e-04, -1.9931e-04,\n",
       "         -1.6786e-04, -2.1305e-05,  2.5382e-04, -1.5943e-04, -1.1684e-04,\n",
       "         -4.4600e-04, -9.9133e-04,  6.5601e-04,  1.9082e-04, -6.3237e-04,\n",
       "         -9.3916e-05, -8.4693e-04,  6.8045e-04,  1.6448e-04, -2.0973e-04,\n",
       "         -5.9286e-04, -4.4295e-04, -5.5172e-04, -5.1040e-04, -3.0522e-04,\n",
       "         -7.8634e-04, -4.1405e-04, -1.0132e-03,  7.8604e-07, -6.8920e-04,\n",
       "          1.5701e-04, -1.6480e-04, -9.5198e-04, -8.9008e-05, -4.2119e-04,\n",
       "         -4.4361e-04, -4.3125e-04,  2.4209e-04, -4.6534e-04, -5.1388e-04,\n",
       "          3.8691e-05,  5.3150e-04,  8.8202e-04, -1.0695e-04,  1.7106e-04,\n",
       "          4.1325e-05,  1.2165e-05, -5.2174e-04,  4.3094e-04,  8.6676e-04,\n",
       "          9.5999e-05,  2.9613e-04, -9.3205e-04,  4.3771e-04,  6.5762e-04,\n",
       "         -1.4196e-04, -1.5458e-04,  5.1435e-04,  2.5216e-05,  1.2086e-04,\n",
       "          9.4097e-05,  7.3998e-04, -9.5315e-05, -2.7723e-05, -5.3013e-04,\n",
       "         -3.5447e-04, -3.3362e-04,  3.7368e-04,  5.3486e-04, -9.9506e-04,\n",
       "         -7.3503e-04,  7.1009e-04,  7.8887e-04, -1.0005e-04, -1.2802e-04,\n",
       "          1.5530e-04, -1.5922e-04, -8.8257e-04,  1.8686e-04,  1.1270e-03,\n",
       "          5.9986e-05, -7.3754e-04,  5.2249e-04, -4.3957e-04, -1.2185e-04,\n",
       "          1.7262e-04, -3.1323e-04, -7.0034e-04,  6.2875e-05, -1.1173e-04,\n",
       "         -4.6045e-06, -9.4812e-04, -6.7442e-04,  1.0435e-03,  7.9893e-04,\n",
       "          4.0706e-04,  8.0433e-05, -6.0484e-04, -1.4267e-03,  1.3826e-05,\n",
       "          1.2512e-03, -1.1104e-04, -2.9554e-04, -1.0537e-03, -6.2881e-04,\n",
       "         -3.2281e-04, -8.4889e-04, -1.4266e-03,  9.1460e-04,  4.2953e-04,\n",
       "         -4.5980e-04,  1.4126e-04,  1.4058e-04,  1.7704e-04,  1.0494e-03,\n",
       "         -8.0161e-04, -2.1514e-04, -4.3139e-05, -1.3296e-04, -2.3965e-04,\n",
       "         -5.0320e-04, -6.1856e-04, -1.2918e-04,  3.8100e-04,  1.4375e-04,\n",
       "          6.5375e-04,  5.7559e-05,  8.7340e-04,  9.1777e-04, -2.2900e-04,\n",
       "          7.0533e-05, -8.3075e-04, -3.6314e-04, -5.2709e-05,  2.1107e-05,\n",
       "         -2.5063e-04, -5.8022e-04, -6.3329e-04, -2.2706e-04,  2.6189e-06,\n",
       "          4.1184e-04,  5.7705e-05, -1.5503e-04, -2.1769e-03, -8.2433e-05,\n",
       "         -6.2949e-04, -2.6378e-04,  5.2894e-04, -1.2049e-04,  1.0583e-03,\n",
       "         -9.6748e-05, -6.4081e-04, -1.1363e-03,  2.2485e-04,  1.4150e-04,\n",
       "         -1.4565e-03, -4.1762e-04,  1.4303e-04,  8.4779e-04, -1.8346e-04,\n",
       "          6.6723e-04, -5.0515e-05,  3.8741e-04,  2.1794e-04,  4.7413e-04,\n",
       "         -3.6667e-04, -7.2490e-04,  2.8278e-04,  7.1433e-05,  1.2566e-03,\n",
       "          2.6992e-04,  4.5579e-04,  3.5527e-04, -4.6677e-04, -1.5014e-03,\n",
       "          2.9330e-04,  8.1144e-04, -3.5689e-04,  8.4370e-05,  1.2304e-03,\n",
       "          3.8210e-04, -3.7218e-05,  8.4863e-05,  1.0395e-03,  4.7304e-04,\n",
       "          3.4827e-04,  1.6619e-04,  8.2257e-04, -3.7333e-04, -2.9120e-05,\n",
       "         -4.5643e-04, -3.6584e-04,  4.0707e-04,  1.6215e-04, -2.3204e-04,\n",
       "         -2.3732e-04, -1.0214e-04,  2.5625e-04, -4.1388e-05,  6.0122e-04,\n",
       "          8.8345e-04,  3.5601e-04,  1.6838e-04,  6.8236e-05,  4.4100e-04,\n",
       "          1.1442e-03, -1.2839e-04, -3.2581e-05, -6.4252e-04, -1.1047e-04,\n",
       "         -4.2450e-04,  1.7999e-03, -1.1478e-04,  7.7847e-05,  5.3918e-05,\n",
       "         -5.5535e-04, -1.3587e-03,  6.8164e-04, -6.5292e-04, -5.2739e-04,\n",
       "         -1.8244e-03, -2.0899e-04, -6.1169e-04,  1.3565e-03,  1.4022e-05,\n",
       "          2.0425e-04,  2.1296e-04,  4.5221e-04, -2.2767e-04, -3.6957e-05,\n",
       "         -1.2397e-03,  2.2701e-04, -1.3475e-04, -1.2101e-03, -2.3834e-04,\n",
       "         -5.5280e-04,  8.2428e-04, -2.4606e-04, -2.6018e-04,  1.3922e-03,\n",
       "          8.5551e-05, -2.6375e-04, -1.3209e-04,  3.7448e-04, -2.1823e-05,\n",
       "          2.7090e-04,  4.3720e-04, -3.6001e-04,  2.6337e-05, -1.2852e-05,\n",
       "          6.4819e-04, -2.2408e-04,  3.0063e-06, -6.7961e-04,  1.3094e-04,\n",
       "         -6.4396e-04, -1.2074e-04,  6.5209e-04,  5.0639e-04, -5.2953e-05,\n",
       "         -9.2580e-04,  7.0564e-04, -1.4498e-04, -2.8754e-04, -3.3352e-04,\n",
       "         -1.5399e-04,  1.8380e-04, -5.2834e-04, -2.3516e-04, -2.5993e-04,\n",
       "          2.5406e-05,  2.4791e-04, -1.9295e-04,  1.1250e-06,  5.4931e-05,\n",
       "         -4.7488e-04, -3.5987e-04,  1.3287e-03,  3.2356e-04,  6.6762e-04,\n",
       "         -5.0899e-05,  1.0279e-03,  1.5393e-03,  6.0052e-04, -5.4579e-05,\n",
       "         -2.3402e-04,  8.6524e-04, -2.0147e-04,  1.6166e-04, -1.0911e-04,\n",
       "          1.0921e-04,  4.6868e-05, -2.4860e-03,  3.7882e-04,  7.7238e-04,\n",
       "         -6.3120e-04, -2.9661e-04, -6.8576e-04, -1.0069e-05, -3.3109e-04,\n",
       "          2.1825e-04, -6.1423e-04, -3.2426e-04,  3.0525e-05,  5.8328e-04,\n",
       "         -2.3894e-04, -1.7550e-03,  8.2192e-04,  2.8429e-04,  3.8389e-06,\n",
       "          4.9927e-04,  3.7387e-04, -1.1340e-04,  4.1702e-04, -5.9159e-04,\n",
       "          3.7743e-04, -9.3709e-04, -1.0344e-03, -3.1612e-04,  4.0829e-05,\n",
       "         -6.7796e-04, -2.6030e-04]),\n",
       " tensor([[ 9.1492e-05,  1.6282e-03, -5.7037e-04,  ..., -3.7848e-04,\n",
       "           1.1470e-03,  2.3695e-04],\n",
       "         [-5.2967e-04,  1.6258e-03,  3.4648e-04,  ..., -3.7398e-04,\n",
       "           1.6176e-03, -6.4107e-04],\n",
       "         [ 1.3196e-03, -8.5467e-04, -1.0019e-03,  ...,  1.1935e-03,\n",
       "           1.7374e-03, -1.3327e-03],\n",
       "         ...,\n",
       "         [ 5.8242e-04,  1.1430e-03, -1.9412e-03,  ...,  7.6215e-04,\n",
       "          -1.3350e-03,  1.8693e-03],\n",
       "         [-7.5934e-04, -9.9839e-04,  3.7530e-04,  ..., -8.5995e-04,\n",
       "           1.2741e-03, -2.6996e-04],\n",
       "         [-1.0154e-03,  2.1698e-03, -9.6812e-04,  ..., -2.4650e-04,\n",
       "           7.9950e-04, -1.7440e-03]]),\n",
       " tensor([-2.0646e-05, -6.0777e-04, -4.1358e-04,  ...,  6.7355e-05,\n",
       "          2.5297e-04,  3.8350e-05]),\n",
       " tensor([[ 6.7831e-05, -1.9267e-05, -3.8470e-04,  ...,  9.1395e-04,\n",
       "          -2.6514e-03,  6.1456e-04],\n",
       "         [-1.0838e-04, -4.8522e-04, -1.4693e-03,  ...,  2.8397e-04,\n",
       "          -5.6249e-04, -2.1577e-03],\n",
       "         [-7.7866e-05, -1.0340e-03, -2.0068e-03,  ...,  1.9142e-03,\n",
       "          -1.0572e-03,  6.0046e-04],\n",
       "         ...,\n",
       "         [-4.0816e-04, -2.0809e-03, -2.8601e-03,  ...,  1.9524e-03,\n",
       "          -1.5081e-03,  1.5434e-03],\n",
       "         [-2.8873e-04, -1.9323e-03, -5.4233e-05,  ...,  7.0434e-04,\n",
       "           9.7657e-04, -8.2663e-04],\n",
       "         [ 3.7675e-04, -1.2335e-03,  4.0236e-04,  ...,  5.4502e-04,\n",
       "          -3.7444e-04,  1.1170e-03]]),\n",
       " tensor([ 9.8531e-05, -1.8616e-04,  4.6172e-05,  1.3137e-04, -3.2105e-05,\n",
       "          1.7186e-04,  1.4721e-05,  2.1347e-04, -3.3252e-04, -2.5962e-04,\n",
       "          4.1660e-05,  2.7816e-04,  2.4327e-04,  1.5682e-04,  1.3550e-04,\n",
       "          1.3810e-04,  2.0024e-04,  1.0476e-04, -2.7393e-04, -7.8648e-05,\n",
       "         -3.8485e-04,  1.3160e-04,  1.9554e-05,  1.1587e-04,  4.9595e-05,\n",
       "          1.8845e-04,  3.9694e-04,  1.2598e-04,  1.3978e-04, -4.1354e-05,\n",
       "         -4.1074e-04,  1.4027e-04,  1.3733e-04, -3.6733e-04, -3.1991e-05,\n",
       "         -1.0545e-04, -3.4244e-04,  5.2380e-05,  7.9093e-05,  4.1572e-04,\n",
       "          8.6818e-06, -1.3201e-04, -1.5713e-04, -7.5508e-05, -2.9620e-05,\n",
       "         -1.4736e-04, -1.5012e-04,  1.3258e-05,  2.3348e-04,  1.2814e-04,\n",
       "         -2.5489e-04,  5.1022e-05, -3.5962e-05,  4.3051e-05, -6.4557e-05,\n",
       "         -1.3358e-04,  1.1282e-04, -3.2299e-04,  1.6446e-04,  1.7466e-04,\n",
       "         -2.1946e-04, -1.2564e-04, -2.2794e-04,  9.0963e-05, -6.9715e-05,\n",
       "         -2.1582e-04, -2.3367e-04, -1.2154e-05,  5.5719e-04,  1.7178e-04,\n",
       "          3.9265e-05,  3.2557e-05,  1.2111e-04,  3.2458e-04,  4.8983e-04,\n",
       "         -1.0261e-05, -1.0117e-04,  2.9976e-04,  2.8049e-04, -8.8935e-05,\n",
       "          5.9919e-05, -2.1517e-04,  2.3257e-04, -1.9814e-04, -5.5991e-06,\n",
       "          2.4102e-04,  1.7324e-04,  2.5151e-04, -3.2426e-04, -2.1954e-05,\n",
       "          3.8349e-04, -1.3679e-04, -5.1722e-05, -1.1613e-04,  1.4695e-04,\n",
       "         -2.8211e-04,  3.0030e-05, -1.4186e-04,  7.4863e-05, -1.7250e-04,\n",
       "          3.2313e-05, -2.9767e-05,  4.1008e-05,  1.6285e-04, -7.7934e-05,\n",
       "         -7.3653e-05, -8.8956e-05,  1.0832e-04,  8.4359e-05, -3.2576e-04,\n",
       "         -3.4871e-04, -2.7608e-04, -2.8321e-04, -7.8546e-05,  1.8735e-04,\n",
       "          5.5973e-04, -8.3199e-05,  1.0973e-04, -3.4855e-04, -2.9177e-04,\n",
       "          7.9334e-05, -1.7377e-04, -2.0589e-05, -2.7912e-04,  6.4503e-05,\n",
       "         -1.2355e-04,  1.7579e-04, -1.6605e-05, -2.9503e-04,  6.5476e-05,\n",
       "         -1.9583e-04, -4.0408e-05, -5.7031e-05,  4.3418e-06, -7.2882e-05,\n",
       "          9.8012e-05,  1.0329e-04,  5.2921e-05, -4.5263e-04,  3.5333e-04,\n",
       "          2.4037e-04,  1.2526e-05,  1.5857e-04,  6.0029e-05, -5.2135e-05,\n",
       "         -1.7061e-04,  3.7021e-04, -1.5415e-04, -2.2437e-05,  2.6806e-04,\n",
       "          5.5915e-05,  4.8578e-05,  1.4031e-04, -2.6266e-04,  2.7464e-04,\n",
       "         -8.4825e-06,  3.0786e-04, -1.9428e-04,  8.7544e-08,  3.0892e-04,\n",
       "          1.9681e-06, -3.7106e-04,  1.9494e-04, -2.0773e-04,  2.7666e-04,\n",
       "         -7.4524e-05,  3.7571e-04, -1.6043e-04, -8.7097e-05,  1.9305e-04,\n",
       "          6.8294e-05, -6.1847e-05,  1.7760e-04,  2.1556e-04,  2.3630e-04,\n",
       "         -1.2651e-04,  4.5449e-06,  2.2549e-04,  1.3165e-04, -6.6459e-05,\n",
       "          2.2054e-04, -8.1036e-05,  1.2426e-04, -1.2396e-04,  4.6790e-06,\n",
       "         -8.1237e-05, -2.4848e-05,  3.8208e-05, -5.4429e-04,  5.3618e-05,\n",
       "         -3.7991e-05, -1.2813e-04,  1.3251e-04, -1.5519e-04, -3.7369e-05,\n",
       "         -2.2287e-05,  3.6569e-04, -5.1875e-06,  1.9296e-04,  2.1652e-04,\n",
       "         -2.1644e-04,  5.1549e-06,  2.5676e-04,  1.6354e-06, -3.6295e-04,\n",
       "         -1.5707e-04,  1.4054e-05,  9.3741e-05, -2.1501e-05, -3.3326e-04,\n",
       "          1.9550e-05, -5.5735e-04, -4.1924e-04,  2.5424e-04,  5.0450e-05,\n",
       "          2.2730e-04, -1.5951e-04, -1.6210e-04, -1.5039e-04, -2.1078e-05,\n",
       "         -4.5376e-04, -1.4729e-04,  3.4166e-04,  5.5469e-04,  1.3845e-05,\n",
       "         -2.4770e-04,  1.7007e-04, -1.1067e-04,  9.1214e-05,  2.0748e-04,\n",
       "          7.9100e-05,  1.8522e-04,  9.6209e-05,  1.9114e-04, -8.1905e-05,\n",
       "         -1.8919e-04, -2.3543e-04, -8.5160e-05, -4.8721e-04,  1.1139e-04,\n",
       "         -2.6468e-04,  2.9148e-04, -2.8659e-05, -1.3975e-04, -1.9219e-04,\n",
       "         -8.1211e-05, -1.8358e-04, -6.5370e-05,  4.1512e-04,  9.2847e-05,\n",
       "          1.7732e-04, -1.6136e-04, -2.0186e-04,  1.8140e-04,  4.0725e-05,\n",
       "         -1.4939e-04, -5.6860e-05, -2.7152e-04,  1.0186e-04, -1.7732e-04,\n",
       "         -4.5307e-04,  4.4510e-05,  3.3279e-05, -1.3592e-04,  1.2310e-04,\n",
       "         -1.8698e-04, -5.8235e-04,  2.4932e-05,  1.6076e-04, -1.0367e-04,\n",
       "          4.6294e-04, -9.5922e-05, -1.4728e-05, -7.1263e-05, -1.1474e-04,\n",
       "         -5.1165e-05,  3.2453e-04, -1.1411e-04, -2.1927e-04, -8.6210e-05,\n",
       "         -3.9846e-05,  3.4164e-05,  1.0662e-04, -8.7228e-06,  3.0186e-05,\n",
       "          2.7698e-04, -2.3454e-04, -3.2261e-05, -1.6288e-04, -2.8569e-04,\n",
       "         -2.3073e-04,  3.6970e-05, -8.5101e-05,  1.5080e-04,  1.5725e-04,\n",
       "         -9.3699e-05,  2.1066e-04,  2.2944e-05,  1.9116e-04, -1.8037e-04,\n",
       "          9.2220e-05, -2.1732e-04,  1.8524e-04,  2.1351e-05, -7.7689e-05,\n",
       "          1.3056e-04, -1.9922e-04, -2.7930e-04,  3.5247e-04,  1.0630e-04,\n",
       "          6.0898e-04, -8.4117e-05, -2.4297e-04,  2.1484e-04, -1.1654e-04,\n",
       "         -3.3688e-05, -1.6006e-04,  8.8833e-05,  2.0555e-04, -1.5539e-04,\n",
       "          4.2244e-04, -1.4576e-04, -1.5287e-06,  2.5220e-06, -6.7350e-05,\n",
       "         -1.3280e-04,  1.0562e-04,  1.2069e-04,  1.8285e-04,  2.8648e-04,\n",
       "          2.2778e-04, -3.0042e-04, -1.5343e-04, -2.4101e-04,  1.8233e-05,\n",
       "          1.5176e-04, -4.8977e-05,  2.9980e-04, -3.8937e-05,  1.7088e-04,\n",
       "          5.3532e-04,  4.1977e-05, -5.0381e-05,  4.6900e-05, -6.3291e-05,\n",
       "          1.5723e-04, -2.8946e-04, -2.6402e-04,  1.1614e-04, -6.2465e-05,\n",
       "          2.5622e-04, -1.4912e-04, -3.4562e-04, -4.4070e-06, -1.9932e-04,\n",
       "          1.1336e-04,  3.9031e-04,  2.9846e-04,  4.2295e-04, -8.9757e-05,\n",
       "         -2.9746e-04,  3.6871e-04, -3.7448e-05,  1.4160e-04, -2.6647e-05,\n",
       "          1.0258e-04, -1.2357e-04, -7.7459e-05, -1.3418e-04, -5.8819e-04,\n",
       "          7.7637e-05, -1.7982e-04, -1.5416e-04,  1.3154e-05,  1.6252e-04,\n",
       "          2.4778e-04, -1.4238e-04, -1.8820e-04,  3.2234e-05, -1.4225e-04,\n",
       "          1.5191e-04, -1.2102e-04,  1.9439e-05, -1.7376e-04, -3.3006e-04,\n",
       "          2.5807e-04, -2.9010e-04, -1.7788e-05,  4.0402e-05, -8.2362e-05,\n",
       "          1.9501e-04, -9.1299e-05,  1.0809e-05, -1.0495e-04,  9.7752e-06,\n",
       "         -1.1797e-04,  1.3240e-04, -2.5659e-04, -6.6216e-05,  2.3186e-04,\n",
       "         -5.9262e-05, -3.0827e-07, -3.3948e-04, -1.5753e-04, -1.0346e-04,\n",
       "          1.1866e-04, -3.8792e-04, -1.1464e-04,  2.3927e-04, -2.0207e-04,\n",
       "          2.1743e-05, -2.1678e-04,  3.8028e-05, -1.9591e-04,  1.0912e-04,\n",
       "          6.0380e-06,  1.3044e-04, -2.4701e-04, -2.2200e-04,  1.2246e-04,\n",
       "          2.3306e-04, -7.5174e-05, -2.3120e-04, -2.6798e-04,  6.9506e-05,\n",
       "          8.7380e-05, -9.5628e-05,  2.1104e-05,  8.1966e-05, -3.1685e-04,\n",
       "         -1.9623e-04,  3.5139e-04,  3.5225e-04, -1.3545e-04,  2.3167e-04,\n",
       "          1.3919e-05,  1.3216e-04, -2.7770e-04, -1.4783e-04,  5.0995e-04,\n",
       "         -1.0990e-04,  8.1684e-05,  4.4645e-04,  9.4930e-05, -2.0635e-04,\n",
       "          5.0648e-05,  1.6178e-04, -3.9507e-05, -6.9809e-05, -2.1711e-04,\n",
       "          1.0650e-03, -2.0775e-04, -3.2920e-04,  6.5165e-06,  1.3191e-04,\n",
       "          1.6443e-04,  1.3486e-04, -1.6538e-04,  1.7044e-04,  4.4641e-04,\n",
       "         -1.8170e-04,  1.9684e-04, -3.6150e-04,  1.9834e-04, -1.7956e-04,\n",
       "          1.4093e-05,  1.0762e-04, -1.4520e-04, -5.1109e-04,  7.2417e-05,\n",
       "          3.1241e-04, -8.1619e-05, -3.6962e-05,  2.1689e-04, -2.5799e-04,\n",
       "         -5.3314e-05, -6.1523e-06,  2.3525e-04,  1.2427e-04,  4.4480e-05,\n",
       "          6.1460e-05,  6.2991e-05, -1.1596e-04, -1.5828e-04,  1.6021e-04,\n",
       "         -3.3451e-04, -2.4157e-04,  1.4565e-04,  1.3665e-04,  6.8724e-05,\n",
       "         -3.8035e-04, -7.8790e-05,  2.1012e-04,  2.3684e-04, -1.5716e-04,\n",
       "          8.0585e-05, -7.0383e-05, -4.8162e-05, -1.3987e-04, -2.0252e-04,\n",
       "          3.5614e-05,  7.5454e-05, -3.1089e-04,  3.3960e-05,  5.5458e-05,\n",
       "          3.7253e-04, -9.8127e-05,  1.4940e-04,  1.8993e-04, -3.3447e-04,\n",
       "         -2.2043e-04,  8.1116e-05]),\n",
       " tensor([ 1.6513e-03,  2.6221e-03,  1.2845e-03,  1.8938e-03,  1.9687e-03,\n",
       "          2.7359e-03,  4.0290e-03,  3.0965e-03,  1.7133e-03,  1.5916e-03,\n",
       "          1.9971e-03,  8.2326e-04, -1.3435e-03,  1.4315e-03,  3.9683e-03,\n",
       "          1.8357e-03,  9.3377e-04, -1.4538e-04,  8.1778e-04,  1.4769e-03,\n",
       "         -5.7513e-04,  2.5625e-03, -1.2840e-03,  1.5476e-03,  1.4186e-04,\n",
       "          1.8617e-03,  8.0734e-04,  8.8519e-04, -5.4812e-04,  3.0367e-03,\n",
       "          1.0353e-03,  3.0742e-03, -1.4955e-03,  3.1144e-03,  1.5248e-03,\n",
       "          2.7258e-03,  1.2255e-03, -9.6571e-04,  3.8261e-03,  4.8280e-05,\n",
       "          1.5979e-03,  1.2826e-03,  1.9488e-03,  1.3205e-03, -4.4513e-04,\n",
       "          1.2221e-03,  1.7334e-03,  2.6675e-03,  1.8350e-03, -4.5711e-04,\n",
       "          1.0551e-03,  2.4051e-03,  1.8404e-03, -5.7918e-04,  3.3721e-03,\n",
       "          2.2946e-03,  3.3144e-03,  1.2031e-03,  1.8352e-03,  4.1043e-03,\n",
       "          1.0993e-03,  1.4334e-03,  1.1361e-04,  3.3872e-03,  1.1593e-04,\n",
       "          1.8315e-03,  4.6629e-04,  6.3264e-04,  1.6386e-03,  2.0933e-04,\n",
       "          2.4146e-04,  1.6325e-03,  1.8525e-03,  9.9713e-04,  1.7309e-03,\n",
       "          1.7530e-03,  2.5788e-03,  1.1903e-03,  2.9123e-03,  1.7225e-03,\n",
       "         -3.4970e-04,  2.0902e-03,  1.5767e-03,  5.3847e-04, -1.5767e-03,\n",
       "          1.4529e-03,  2.4705e-03, -4.0787e-04,  2.9278e-03, -1.4974e-03,\n",
       "          3.6457e-03,  4.1741e-04,  9.5373e-04,  8.1980e-04,  1.9274e-03,\n",
       "          2.7973e-03,  3.3718e-03,  2.4173e-03, -3.4146e-03, -4.3333e-05,\n",
       "          1.2997e-03,  1.4285e-03,  2.6269e-03, -9.8783e-04,  1.2187e-03,\n",
       "          1.0368e-03,  1.4257e-04,  7.3111e-04,  7.1955e-04, -5.7817e-04,\n",
       "          2.0068e-03,  2.5373e-03,  1.8470e-03,  6.8474e-04,  1.1196e-03,\n",
       "          1.2554e-03,  1.1463e-03,  3.1257e-03,  1.2586e-03,  1.2680e-03,\n",
       "         -2.3704e-03,  1.5116e-03, -5.8180e-04, -6.2755e-03,  2.0207e-03,\n",
       "         -9.5642e-04, -9.4235e-05, -2.2835e-04,  2.9521e-03,  2.5601e-03,\n",
       "          1.5050e-04,  2.3004e-03,  3.7375e-03, -1.8937e-03,  5.8615e-04,\n",
       "         -1.6940e-03,  1.3681e-03,  1.9637e-03,  2.1554e-03,  2.1734e-03,\n",
       "          1.1914e-03,  1.7549e-03,  8.1980e-04,  2.0172e-03,  1.0320e-03,\n",
       "          1.4725e-03,  2.3580e-03,  2.5664e-03,  8.6898e-04,  1.4813e-03,\n",
       "          7.4130e-04,  2.0870e-03,  2.6488e-03, -5.0112e-03,  2.5792e-03,\n",
       "          1.2856e-03,  4.0020e-03, -5.0801e-04,  8.3160e-04,  1.5671e-03,\n",
       "          1.6659e-03,  1.3055e-03,  5.0054e-03, -1.2497e-03,  3.4681e-03,\n",
       "         -4.2796e-05,  3.3218e-04, -2.6391e-03,  2.0981e-04,  2.8932e-04,\n",
       "          1.7840e-04, -1.4507e-03,  2.0950e-03,  1.0734e-03,  1.4855e-03,\n",
       "          2.2429e-03,  2.5855e-03, -1.6595e-03,  3.2866e-03,  2.2671e-03,\n",
       "          1.8895e-05,  3.0233e-03,  5.9527e-04,  1.6677e-03,  1.7511e-03,\n",
       "         -7.1329e-04,  1.5760e-03,  1.7459e-03, -2.8968e-05,  7.2867e-04,\n",
       "          1.5264e-03,  2.8731e-03,  5.5331e-04,  9.4038e-04,  1.8957e-03,\n",
       "          2.1251e-03,  1.1849e-03, -6.7043e-04, -3.1066e-04, -1.2011e-03,\n",
       "         -1.8534e-03,  1.1314e-03,  1.1110e-03, -1.8722e-04,  1.6258e-03,\n",
       "          4.2582e-04, -9.4259e-04, -2.8551e-04,  6.2394e-04, -4.9818e-04,\n",
       "          7.5960e-04,  1.9847e-03,  2.8090e-03,  5.5200e-04,  2.4329e-03,\n",
       "         -8.2326e-04,  4.3792e-04,  9.3400e-04,  2.4593e-03,  1.6407e-03,\n",
       "         -2.3210e-04,  2.2803e-03,  2.2681e-03,  6.1619e-04,  2.2653e-03,\n",
       "          3.0112e-03,  1.3518e-04,  8.7738e-04,  5.6207e-04,  2.7546e-03,\n",
       "          2.7726e-03,  3.3962e-03,  1.9286e-03, -9.3710e-04,  3.3933e-04,\n",
       "         -4.5061e-05,  4.4382e-04,  2.7226e-03,  1.2007e-03, -3.4809e-05,\n",
       "          2.0185e-03, -2.0953e-03,  3.3694e-03, -2.6805e-03,  3.0684e-04,\n",
       "          2.7862e-03,  2.5974e-03,  2.1981e-03,  5.8138e-04,  3.4584e-03,\n",
       "          2.8571e-03,  1.1874e-03,  1.0866e-03,  2.3925e-04,  7.9197e-04,\n",
       "         -2.1726e-04, -4.8316e-04,  1.6843e-03,  1.8500e-03,  3.4965e-03,\n",
       "          2.2153e-03,  2.1275e-03,  1.2809e-03,  2.7249e-03,  1.6919e-03,\n",
       "          7.3665e-04,  1.4477e-03,  4.4405e-05,  3.4863e-04,  4.9782e-04,\n",
       "          1.3117e-03,  9.2685e-05,  1.9047e-03, -2.1994e-05,  1.0703e-03,\n",
       "          1.3923e-03,  1.1610e-03,  2.0540e-04,  1.7358e-03,  2.7050e-03,\n",
       "          1.7779e-03,  2.5246e-03, -1.0758e-03, -5.3012e-04,  1.2091e-03,\n",
       "          4.3803e-04,  1.5048e-03, -2.1750e-04,  2.6960e-03,  2.4331e-03,\n",
       "          4.9210e-04, -1.7487e-03,  2.0443e-03, -5.2260e-03,  2.1055e-03,\n",
       "          1.6170e-03,  3.1293e-03,  2.2741e-03,  4.1682e-04,  8.9902e-04,\n",
       "          3.1902e-03,  6.7180e-04,  7.7510e-04,  1.9909e-03,  2.1005e-04,\n",
       "         -1.6641e-03,  2.1815e-03,  1.0581e-03,  2.1239e-03,  1.7098e-03,\n",
       "          1.5388e-03,  3.1269e-03,  6.0189e-04,  3.6049e-04,  2.5634e-03,\n",
       "          8.4412e-04,  2.4200e-03,  2.0166e-03,  7.3171e-04,  4.0281e-04,\n",
       "          1.7803e-03,  1.1213e-03,  1.6245e-03, -9.9719e-05, -2.5994e-03,\n",
       "          4.4118e-03, -2.6196e-04, -8.2994e-04,  1.9339e-03,  1.8349e-03,\n",
       "          1.7089e-03,  5.8633e-04,  4.4996e-04,  1.1873e-03, -5.7793e-04,\n",
       "         -2.2280e-04,  9.2238e-04,  9.1189e-04,  3.8236e-04,  3.4833e-04,\n",
       "          1.5609e-03, -5.8536e-03,  3.1599e-03, -7.8583e-04,  4.6790e-04,\n",
       "          1.5216e-03,  6.2150e-04,  1.8379e-03,  2.4348e-03,  8.2225e-04,\n",
       "          5.0449e-04, -7.2300e-04,  2.4183e-03,  5.9336e-04, -2.8326e-03,\n",
       "         -1.2988e-04,  1.2811e-03,  1.8455e-03,  1.2336e-03,  2.8813e-03,\n",
       "          1.9473e-03,  1.4880e-03,  4.4286e-05,  3.5849e-03, -2.9606e-04,\n",
       "          5.3130e-03,  1.0927e-03,  3.5042e-04,  3.6299e-03,  3.3201e-03,\n",
       "         -2.0152e-03,  1.3377e-03, -3.9339e-04,  4.2527e-03, -6.8843e-04,\n",
       "          9.7632e-04,  4.4137e-04,  1.6437e-03,  2.5664e-03,  1.3931e-03,\n",
       "          1.4564e-03, -2.1533e-03,  1.1132e-03, -2.7204e-04,  6.7258e-04,\n",
       "          2.7167e-03,  1.8275e-03,  1.4592e-03,  2.6237e-03,  1.7033e-03,\n",
       "          2.5454e-03,  1.4709e-03, -1.1590e-03,  1.3803e-03,  2.8256e-03,\n",
       "          4.9347e-04,  5.1349e-04,  2.0900e-03,  1.4552e-03,  1.0062e-03,\n",
       "          2.9180e-03,  1.8691e-03,  1.6423e-03,  2.7744e-03,  2.0739e-03,\n",
       "         -8.9377e-04, -1.1239e-03,  1.2693e-03,  1.1039e-03,  2.7589e-03,\n",
       "          1.6369e-03,  8.5407e-04,  8.4126e-04,  1.7338e-03,  1.3313e-03,\n",
       "          5.9503e-04,  1.5974e-03,  2.4016e-03,  3.3996e-03, -1.5483e-03,\n",
       "          2.0721e-03, -1.6253e-03,  2.6788e-03,  3.6104e-03,  1.8819e-03,\n",
       "          2.0340e-03,  3.8965e-03, -5.1188e-04,  1.4394e-03,  2.3161e-03,\n",
       "         -5.6201e-04,  2.0759e-03,  4.2789e-03, -1.7086e-03,  4.4625e-03,\n",
       "          1.4421e-03, -1.1777e-03,  4.0359e-03,  9.2077e-04,  5.1159e-04,\n",
       "          1.4517e-03,  3.7731e-03,  1.5478e-03,  6.9994e-04,  1.8121e-03,\n",
       "         -1.5050e-04,  1.9669e-03, -1.2275e-03, -2.7412e-03, -1.6390e-03,\n",
       "          1.6050e-03,  7.0387e-04,  4.5137e-03,  1.9869e-03,  1.3623e-03,\n",
       "          1.3288e-03,  7.7087e-04,  3.2534e-03,  3.0328e-03,  1.1762e-03,\n",
       "          6.9201e-04,  2.8325e-03,  1.4071e-03,  9.5510e-04, -4.4405e-05,\n",
       "          3.3195e-03,  2.5868e-05,  2.5769e-03,  9.3752e-04,  1.0947e-03,\n",
       "         -1.8727e-03,  4.5857e-03,  2.0704e-03,  2.4344e-03,  2.0423e-03,\n",
       "          8.0585e-05, -8.6510e-04, -5.2345e-04, -2.5403e-03,  2.3125e-03,\n",
       "          1.6437e-03,  3.2258e-04, -4.2886e-04,  9.9736e-04,  1.4976e-03,\n",
       "          7.8356e-04,  2.0343e-03,  4.2959e-03,  1.5553e-03,  1.8114e-03,\n",
       "          2.5742e-03, -3.1257e-04,  3.6110e-03,  7.7003e-04,  1.0031e-03,\n",
       "          1.6193e-03,  1.6136e-03,  2.0775e-03,  9.8503e-04,  2.3894e-03,\n",
       "          2.3200e-03,  2.1904e-03,  8.5551e-04, -3.2097e-04,  1.9598e-03,\n",
       "          1.5375e-03,  3.7482e-03,  1.7745e-03, -2.1327e-04, -1.5749e-03,\n",
       "         -1.3387e-04, -2.3460e-04]),\n",
       " tensor([ 5.6083e-04, -1.8099e-03,  8.1733e-05,  7.8462e-05,  7.1852e-04,\n",
       "         -3.5990e-04,  1.4024e-04,  1.7366e-03, -3.9411e-04, -3.5135e-04,\n",
       "          1.3578e-04,  3.1786e-04,  1.3990e-04,  2.5490e-03,  9.6506e-04,\n",
       "         -4.0161e-04,  1.2994e-04,  5.5037e-04, -1.0900e-03, -7.1039e-04,\n",
       "         -1.0660e-03, -6.1543e-04, -3.2195e-05, -2.2770e-04,  2.6572e-04,\n",
       "         -3.7787e-04,  1.7905e-03, -2.9017e-05,  4.0471e-04, -8.8357e-04,\n",
       "         -2.3061e-03,  1.3617e-04,  4.1983e-04, -2.0897e-03, -5.7542e-04,\n",
       "         -1.2636e-03, -8.9355e-04,  1.3983e-04,  1.4025e-04,  1.7920e-03,\n",
       "          5.8796e-04, -1.4383e-03, -4.4559e-04, -6.2028e-04, -1.4762e-03,\n",
       "         -5.8664e-04,  1.7215e-05, -7.9855e-04,  4.8923e-04,  3.1659e-04,\n",
       "         -1.6056e-03,  1.4575e-03, -5.8520e-04,  7.3650e-04, -1.1741e-03,\n",
       "          1.7688e-03,  1.0683e-03, -2.8684e-04,  3.2756e-04,  2.5884e-03,\n",
       "         -1.1153e-03, -1.4760e-03, -4.0326e-06,  1.7112e-04, -8.3818e-04,\n",
       "         -5.1357e-04, -2.9105e-04,  1.4589e-04,  5.7035e-04,  2.6785e-04,\n",
       "         -1.4751e-03,  6.7873e-04, -1.6456e-03,  8.6848e-04,  1.1660e-03,\n",
       "         -4.4437e-04, -6.0418e-04,  1.2137e-03, -4.1271e-04, -3.8667e-04,\n",
       "         -6.1547e-04, -1.1023e-03,  4.4828e-04, -1.0337e-03, -1.9098e-05,\n",
       "          9.9635e-04, -2.8555e-04,  5.0094e-04, -9.7482e-04, -3.4869e-04,\n",
       "          1.4542e-03,  8.9342e-04, -2.2562e-04, -6.9113e-04, -2.7705e-04,\n",
       "         -1.2042e-03,  5.8259e-04, -5.6266e-04,  1.3078e-03, -3.2302e-04,\n",
       "         -5.4611e-04,  3.6807e-04,  2.8585e-04,  1.3807e-03, -1.6773e-03,\n",
       "         -9.5026e-04, -2.0643e-04,  8.2802e-04,  6.9553e-04, -4.5811e-04,\n",
       "         -1.0718e-03, -4.9385e-04,  5.7309e-04, -5.2373e-05,  1.1548e-03,\n",
       "          1.0542e-03, -7.4527e-04, -1.0509e-03, -1.3642e-03, -9.9304e-04,\n",
       "          9.9105e-04, -9.4050e-04,  9.8638e-05,  1.8367e-04,  1.4208e-03,\n",
       "         -6.7928e-04, -4.3533e-04,  7.4327e-04, -1.7588e-03, -2.1653e-04,\n",
       "         -7.6771e-04,  2.2855e-04,  2.4370e-04, -1.4796e-04,  9.9021e-04,\n",
       "         -1.3321e-03, -7.2027e-04,  9.7213e-04, -4.2595e-04, -8.9433e-05,\n",
       "          4.6004e-04,  3.0297e-04, -4.1785e-04,  1.0799e-03, -6.2190e-04,\n",
       "         -1.6497e-03,  1.1472e-03, -4.2235e-04,  7.8584e-04,  9.2198e-04,\n",
       "          3.8837e-04,  6.0556e-04,  1.6314e-03, -1.7374e-04,  1.7567e-03,\n",
       "         -7.8359e-04,  2.6926e-04, -6.2054e-06,  2.9118e-04, -6.1819e-04,\n",
       "         -8.0528e-04,  6.6740e-04, -2.5777e-04, -1.7501e-04,  2.4234e-03,\n",
       "          4.4903e-04,  4.4457e-04,  3.8243e-04, -4.2124e-04,  1.8543e-04,\n",
       "         -6.0910e-04, -3.2693e-04,  9.1469e-04, -5.3762e-04, -1.5769e-04,\n",
       "         -2.0675e-04,  2.0250e-03, -6.2744e-04,  1.2155e-03, -1.8328e-04,\n",
       "          1.4486e-03, -5.7916e-04, -3.4069e-04, -1.2489e-03,  1.2515e-03,\n",
       "         -9.1396e-05, -9.1373e-04,  5.9945e-04, -2.0629e-03, -6.3293e-04,\n",
       "         -8.1409e-04,  6.0388e-04,  2.3715e-04,  6.3181e-04,  7.3283e-04,\n",
       "          3.3250e-04, -1.8641e-05, -1.1991e-03,  8.4009e-04,  2.9594e-04,\n",
       "         -2.9972e-04,  2.7367e-04,  1.9829e-04,  6.1348e-04, -1.4634e-03,\n",
       "         -2.7551e-04, -5.2011e-04,  1.0273e-03, -7.3015e-04,  3.5162e-04,\n",
       "          2.5629e-04, -1.7855e-03, -3.9859e-04,  7.7247e-04,  4.9606e-04,\n",
       "          1.9236e-04, -9.0208e-05, -4.5841e-04, -2.8935e-04, -1.6520e-03,\n",
       "         -1.3985e-03,  8.9560e-04, -2.9821e-04, -1.5569e-04,  4.8471e-04,\n",
       "          6.9766e-04,  8.1304e-04, -9.8962e-04,  7.2136e-04,  3.8079e-04,\n",
       "          8.7748e-04,  7.0713e-04,  8.0778e-04, -2.7213e-04, -4.0689e-04,\n",
       "         -4.2443e-04, -7.7316e-04, -6.5711e-04, -2.6327e-03,  7.8202e-04,\n",
       "         -1.4644e-03,  5.9279e-04, -8.1395e-04,  5.6472e-06, -4.1016e-04,\n",
       "          2.3790e-05, -8.6580e-04,  6.9112e-04,  1.3836e-03,  8.0056e-04,\n",
       "          5.7876e-05, -4.3482e-05, -1.1272e-03, -7.2739e-04, -4.7321e-04,\n",
       "          3.8445e-05,  3.0285e-04, -4.7556e-04,  9.1355e-04,  5.4689e-04,\n",
       "         -2.2857e-03, -2.7972e-04,  6.0868e-04,  5.5597e-04,  2.8224e-04,\n",
       "         -2.1164e-04, -1.9791e-03, -1.3906e-04,  1.4695e-03, -2.9910e-04,\n",
       "         -5.5664e-04,  8.3341e-04,  1.5480e-03, -1.3212e-04,  2.2854e-04,\n",
       "          3.6274e-04, -7.4593e-05,  2.2991e-04, -6.3142e-04, -5.5751e-04,\n",
       "          1.6604e-03,  1.8178e-05,  3.8802e-04,  2.4109e-04,  1.3280e-04,\n",
       "         -1.3620e-05, -6.9011e-05, -6.0904e-04, -8.2966e-04,  4.4853e-04,\n",
       "         -5.4970e-04,  5.6275e-07,  1.3215e-03,  3.1149e-04,  1.1739e-03,\n",
       "          3.1829e-04,  6.2557e-04, -6.4630e-05, -3.9003e-04, -4.0884e-04,\n",
       "          3.7009e-04,  2.7564e-04,  9.0504e-04,  7.6562e-04, -1.1129e-03,\n",
       "          5.9562e-04, -9.2435e-04,  8.0233e-04,  9.1843e-04,  1.4434e-03,\n",
       "          1.8253e-03,  1.3939e-03,  1.6233e-03,  1.0924e-03,  9.6307e-04,\n",
       "          9.8797e-04, -6.8495e-04,  1.4793e-03,  9.5870e-04, -6.9405e-04,\n",
       "          1.0043e-03,  9.5124e-04, -8.4671e-04,  9.5483e-04, -4.3536e-04,\n",
       "         -1.2084e-04,  3.8804e-04, -8.6626e-04,  1.9807e-03,  1.5211e-03,\n",
       "          6.9207e-04, -1.3680e-03, -3.5057e-04,  5.7083e-04, -7.7754e-04,\n",
       "          2.2704e-04, -3.1814e-04,  4.7256e-04,  3.4083e-04,  4.5326e-04,\n",
       "          1.6880e-03, -5.2941e-04, -1.5904e-03, -1.8131e-04,  1.2926e-04,\n",
       "          1.3293e-03, -8.7497e-04,  1.6805e-04, -9.5432e-04, -5.5726e-04,\n",
       "          7.8527e-04, -2.2912e-04,  2.9971e-05,  1.2161e-03,  1.9342e-04,\n",
       "          3.0317e-04,  2.6259e-04,  2.1419e-03,  8.6352e-06,  8.6755e-04,\n",
       "         -1.4914e-04,  1.1289e-03,  4.8922e-04,  9.5792e-04,  2.3507e-04,\n",
       "         -8.4643e-04,  8.6413e-04, -5.3897e-04,  8.8761e-04, -5.6574e-04,\n",
       "          1.8222e-04, -1.1192e-03, -5.0940e-04,  1.5293e-03,  5.9675e-04,\n",
       "          1.2541e-03, -1.6922e-04, -1.3929e-03, -6.0851e-04,  1.4099e-04,\n",
       "          5.4757e-04, -1.2245e-03,  1.1806e-04,  4.9654e-04, -1.5473e-03,\n",
       "          1.5348e-03, -7.7775e-04, -6.4430e-04, -2.0002e-04, -3.8955e-05,\n",
       "         -2.8075e-04,  9.4493e-04, -8.5254e-04, -2.8252e-04, -5.6315e-05,\n",
       "          2.4906e-04,  7.4205e-04, -1.4185e-03, -1.0626e-03,  9.9497e-04,\n",
       "         -5.6691e-04,  7.8114e-04, -1.7116e-03, -4.8397e-04, -2.8186e-04,\n",
       "         -1.2631e-04, -1.0915e-03,  1.3081e-03,  1.0179e-03,  3.4295e-05,\n",
       "         -1.6314e-03, -1.9388e-03,  5.2822e-04, -4.4948e-04,  2.3349e-04,\n",
       "         -1.2501e-03,  4.1343e-04, -9.4899e-04, -1.1204e-03,  7.5584e-04,\n",
       "          3.8716e-04, -3.8402e-04, -4.4917e-04, -9.7915e-04, -1.6327e-04,\n",
       "          1.4121e-03, -1.6550e-04,  1.4941e-03,  2.7985e-04, -1.4592e-03,\n",
       "          6.8905e-04,  1.0184e-03,  1.5272e-03, -1.2857e-04,  9.1009e-05,\n",
       "          3.1772e-04,  4.1014e-04, -1.2661e-03, -2.8199e-04,  4.8719e-04,\n",
       "         -7.3370e-04, -4.7056e-04,  1.5066e-03,  1.3247e-03,  7.8995e-04,\n",
       "         -2.9990e-04,  5.0908e-04,  8.1700e-04, -3.3800e-04,  1.4215e-03,\n",
       "         -2.9984e-03, -4.7950e-04,  9.3363e-04,  7.6455e-04, -7.6836e-04,\n",
       "          3.5929e-04,  6.1759e-04,  3.1901e-04,  8.4007e-04,  1.5524e-03,\n",
       "         -3.5135e-04,  5.4192e-04, -7.2065e-04,  5.1264e-04, -9.5914e-04,\n",
       "         -1.0073e-03, -2.4482e-04,  6.5181e-04, -1.7752e-03, -1.1333e-04,\n",
       "          4.7877e-04,  1.3665e-04,  3.2688e-04,  8.4985e-04, -1.3815e-03,\n",
       "         -3.4995e-04,  5.0461e-04, -5.8996e-04,  1.2638e-03,  7.8069e-05,\n",
       "         -1.2472e-04,  2.3198e-04,  1.6424e-03,  5.2833e-05,  7.2174e-04,\n",
       "         -3.7755e-04, -5.8114e-04,  1.7433e-04,  8.2815e-04, -2.0526e-05,\n",
       "          4.5765e-04,  3.6885e-04,  9.5138e-04,  3.6847e-04,  1.0325e-04,\n",
       "          1.2924e-03,  1.1556e-03, -8.6511e-05, -1.1440e-03, -2.1173e-04,\n",
       "          1.5210e-04, -1.2569e-03, -8.3749e-04, -8.7933e-04,  5.8902e-04,\n",
       "          6.8722e-04, -4.5588e-04,  2.6539e-04,  1.1840e-03,  5.6597e-04,\n",
       "          5.6032e-04,  3.3142e-04]),\n",
       " tensor([[-8.7290e-04, -7.1642e-04, -1.0098e-03,  ...,  6.2008e-04,\n",
       "          -9.6436e-04,  1.5469e-03],\n",
       "         [ 1.2705e-03,  8.3477e-04,  7.9441e-04,  ..., -1.0970e-03,\n",
       "           6.0849e-04,  5.8255e-04],\n",
       "         [ 9.5068e-04,  1.2886e-03, -2.1136e-03,  ...,  4.3365e-04,\n",
       "           1.0831e-04,  1.8040e-03],\n",
       "         ...,\n",
       "         [ 4.7265e-04, -5.8087e-04,  4.5985e-04,  ...,  1.1945e-03,\n",
       "          -3.6534e-04,  1.0853e-04],\n",
       "         [-1.7557e-03, -2.5062e-03,  3.2254e-04,  ..., -1.3106e-04,\n",
       "           5.3590e-04,  1.4257e-03],\n",
       "         [-1.6338e-03,  4.8236e-04,  2.8892e-04,  ...,  1.0048e-03,\n",
       "           6.6403e-04,  7.8706e-05]]),\n",
       " tensor([ 6.3309e-04, -7.6897e-05, -2.5292e-03,  ..., -6.6043e-04,\n",
       "          6.8923e-04, -2.4907e-04]),\n",
       " tensor([[-2.3644e-04,  6.0864e-05, -3.9117e-03,  ...,  8.6179e-04,\n",
       "           5.3386e-04, -4.3623e-03],\n",
       "         [-1.5845e-04, -1.0456e-03,  2.3897e-03,  ...,  9.0329e-05,\n",
       "           6.2872e-04, -1.7121e-03],\n",
       "         [-2.0883e-04, -1.0150e-03,  5.3017e-04,  ...,  5.8537e-04,\n",
       "          -1.9268e-03,  2.7765e-05],\n",
       "         ...,\n",
       "         [-8.6479e-04,  1.7466e-04,  7.6548e-04,  ...,  8.8796e-04,\n",
       "          -3.0636e-03, -2.4113e-03],\n",
       "         [ 5.3397e-04, -1.6035e-03,  5.8785e-05,  ..., -5.0862e-04,\n",
       "          -1.7432e-03, -2.8494e-03],\n",
       "         [ 7.0554e-04, -2.0286e-03,  2.1611e-04,  ...,  1.3273e-03,\n",
       "           2.0076e-03,  2.7702e-03]]),\n",
       " tensor([ 9.4693e-05,  1.7472e-05,  7.0932e-06,  1.4901e-04, -5.0626e-05,\n",
       "          1.3626e-04, -1.7279e-04,  9.7525e-05, -1.3707e-04, -4.0411e-04,\n",
       "          1.8293e-04,  3.5762e-05,  1.2539e-04, -1.2506e-04,  1.4793e-04,\n",
       "          1.6123e-04,  3.3237e-04,  2.7915e-05, -2.6674e-04,  5.5654e-05,\n",
       "         -4.3278e-04,  4.6082e-05,  3.1991e-06,  8.6965e-05, -1.2612e-04,\n",
       "          1.8685e-04,  3.9897e-04,  1.4172e-04, -8.3055e-05,  5.4081e-05,\n",
       "         -2.8442e-04, -8.4840e-05,  1.7505e-04, -2.6937e-04, -1.9105e-04,\n",
       "         -5.0783e-05, -2.6839e-04,  7.7144e-05,  1.0789e-04,  2.2780e-04,\n",
       "         -2.0498e-05, -1.0477e-04, -6.6486e-05,  6.5317e-05, -9.5913e-05,\n",
       "         -2.5778e-04, -2.1600e-04,  1.3800e-04,  2.3181e-04,  2.0641e-04,\n",
       "         -2.4142e-04, -3.8431e-05,  6.3233e-05, -7.6009e-05, -3.6611e-05,\n",
       "         -3.1717e-04, -2.1977e-05, -3.8128e-04,  4.1107e-04,  4.8541e-06,\n",
       "         -2.2185e-04,  9.6256e-05, -2.1717e-04,  1.1662e-04, -1.6193e-04,\n",
       "         -2.3797e-04, -1.7610e-04, -7.8075e-05,  5.6401e-04, -5.0943e-07,\n",
       "          1.4736e-04, -6.5912e-05,  2.9948e-04,  1.9345e-04,  5.4976e-04,\n",
       "          7.7582e-05, -2.1036e-04,  2.5149e-04,  1.3999e-04, -7.9947e-05,\n",
       "          1.0172e-05, -1.2916e-04,  3.3731e-04, -1.4472e-04,  4.0006e-05,\n",
       "          2.5517e-04,  2.0050e-04,  1.8247e-04, -2.7429e-04, -3.4694e-05,\n",
       "          1.8390e-04, -2.9541e-04, -1.4560e-04, -4.1328e-05,  1.1733e-04,\n",
       "         -3.6465e-04, -4.3469e-05, -9.3203e-05,  4.8262e-06, -2.1764e-04,\n",
       "         -1.5818e-04,  9.6773e-05, -8.1256e-05,  1.8495e-04,  2.3138e-06,\n",
       "         -2.3092e-04, -1.9461e-04,  2.2596e-04,  5.3890e-05, -4.2796e-04,\n",
       "         -3.0814e-04, -2.1994e-04, -2.9734e-04, -1.0941e-04, -1.3094e-04,\n",
       "          4.0580e-04, -2.3767e-05,  1.5377e-04, -2.0511e-04, -3.1878e-04,\n",
       "          4.3156e-05, -1.5211e-04,  6.2097e-05, -2.8330e-04, -2.7139e-05,\n",
       "         -8.5769e-05,  1.4163e-04,  1.5432e-04, -1.0496e-04,  1.4642e-04,\n",
       "         -1.5851e-04, -1.4946e-05, -9.3497e-05,  7.4618e-06, -1.3298e-04,\n",
       "          3.3268e-04,  5.4048e-05, -1.4724e-04, -3.0582e-04,  3.5651e-04,\n",
       "          3.8653e-04, -6.2311e-05,  9.7814e-05, -8.5961e-05,  2.3246e-04,\n",
       "         -1.7234e-04,  1.8594e-04, -2.2638e-04, -6.9268e-05,  1.5240e-04,\n",
       "          7.9744e-05,  7.5777e-05, -5.6233e-05, -1.2487e-04,  2.9502e-04,\n",
       "         -2.0993e-05,  2.2723e-04, -2.2728e-04, -1.4806e-04,  3.5613e-04,\n",
       "          2.4754e-05, -3.7510e-04,  1.6807e-04, -9.1508e-05,  1.1171e-04,\n",
       "         -3.0488e-04,  3.8184e-04, -4.0482e-04,  1.5767e-04,  4.5143e-04,\n",
       "          1.6850e-04, -1.6734e-04,  1.5287e-04,  2.4169e-04,  2.0592e-04,\n",
       "         -7.8527e-05, -2.3188e-04,  1.8281e-04, -2.8495e-05, -1.4322e-04,\n",
       "          5.8319e-05, -1.3491e-05, -1.3756e-05, -2.9225e-06,  9.7670e-05,\n",
       "          7.8702e-05, -5.7310e-05,  2.4045e-04, -3.9890e-04, -5.9431e-05,\n",
       "          1.0145e-04, -5.2293e-05, -2.3589e-05, -2.2832e-04, -9.9320e-05,\n",
       "         -7.6736e-05,  5.7201e-04,  6.0930e-05,  1.2090e-04,  1.2413e-04,\n",
       "         -7.9786e-05, -1.8034e-05,  1.3325e-04, -2.5540e-05, -4.5743e-04,\n",
       "         -1.4352e-04,  3.1736e-05,  1.6553e-05, -5.5633e-05, -3.4117e-04,\n",
       "          1.1980e-04, -4.2843e-04, -6.0861e-04,  2.6761e-04,  3.3659e-05,\n",
       "          2.1935e-04, -1.2369e-04, -6.3122e-05, -2.8976e-05,  2.3498e-04,\n",
       "         -3.1020e-04, -3.7512e-04,  2.1443e-04,  5.8466e-04, -9.6320e-05,\n",
       "         -3.6738e-04,  4.0203e-04,  1.0200e-04,  6.5493e-05,  2.0005e-04,\n",
       "          1.8860e-05,  2.9777e-04,  5.0675e-05,  2.7735e-04,  1.3013e-04,\n",
       "         -2.0271e-05, -2.9516e-04,  8.5641e-05, -3.7916e-04,  7.1180e-05,\n",
       "         -1.6966e-04,  1.4906e-04,  9.4101e-06, -1.7948e-04, -1.3309e-04,\n",
       "         -8.8226e-05, -1.0158e-04,  1.1476e-05,  3.8917e-04,  8.2092e-05,\n",
       "          3.5746e-04, -2.4498e-04, -9.8601e-05,  1.7395e-04,  9.8795e-06,\n",
       "          4.3724e-05,  1.5279e-04, -2.8328e-04,  2.0425e-04, -3.8521e-04,\n",
       "         -3.3852e-04, -6.1752e-05,  8.1536e-05, -1.5974e-04,  2.2480e-04,\n",
       "         -1.2720e-04, -4.9000e-04,  1.0719e-04,  7.0844e-05, -4.3305e-05,\n",
       "          5.1276e-04, -3.0443e-04, -1.1693e-04, -7.5223e-05,  2.7219e-05,\n",
       "         -8.6252e-05,  3.7180e-04, -6.0840e-05,  1.1197e-04, -1.8239e-04,\n",
       "         -1.5366e-04,  8.0760e-05,  1.9218e-04, -2.5187e-04, -7.9647e-06,\n",
       "          4.0328e-04, -4.4897e-04,  1.3094e-05,  2.8015e-05, -2.9586e-04,\n",
       "         -7.3042e-05, -1.1143e-04, -1.7881e-04,  8.1558e-05,  4.0828e-05,\n",
       "         -1.0594e-04,  4.4570e-04, -6.4727e-07,  1.5097e-04, -1.1760e-04,\n",
       "          1.9765e-04, -1.4883e-04,  3.0721e-04,  5.6608e-05, -9.2871e-05,\n",
       "          8.3079e-05, -2.2473e-04, -1.7722e-04,  1.6645e-04, -1.7251e-04,\n",
       "          5.4102e-04, -1.6374e-04, -2.7391e-04,  1.5431e-04, -1.1861e-04,\n",
       "          1.6298e-06,  1.3066e-06,  2.9072e-05,  3.5770e-04, -1.0432e-04,\n",
       "          4.0114e-04, -1.9408e-05, -6.0750e-06,  2.4824e-05,  2.1158e-05,\n",
       "         -1.5647e-04,  5.2210e-05,  6.0875e-05,  1.2113e-05,  1.1650e-04,\n",
       "          1.8372e-04, -1.9875e-04, -1.9388e-04, -3.7497e-04,  2.5068e-04,\n",
       "         -9.4883e-06,  5.5936e-05,  1.5507e-04, -5.7440e-05, -5.0232e-05,\n",
       "          4.3984e-04,  2.1614e-04,  1.9477e-04,  2.8524e-05, -1.3947e-04,\n",
       "          8.0427e-05, -2.7671e-04, -2.5035e-04,  5.2566e-05,  2.3297e-06,\n",
       "          3.0715e-04, -7.8481e-05, -3.4789e-04, -4.8183e-05, -7.3090e-05,\n",
       "          1.7444e-04,  1.6664e-04,  5.3196e-05,  3.4848e-04, -1.7885e-04,\n",
       "         -3.9635e-04,  3.6811e-04, -9.5172e-05,  1.0821e-04,  1.4830e-04,\n",
       "          1.1466e-04, -1.6068e-04, -2.3095e-05, -1.5529e-04, -5.5064e-04,\n",
       "          2.8879e-04, -7.0319e-05, -4.0571e-04, -2.3251e-05,  1.5286e-04,\n",
       "          2.7973e-04, -5.7080e-05, -2.6386e-05, -9.3392e-05, -4.4988e-05,\n",
       "          3.2394e-04, -8.0518e-05,  1.9610e-04, -6.8482e-05, -2.8365e-04,\n",
       "          1.2603e-04, -2.9063e-04,  1.8364e-04,  1.0857e-04, -3.9233e-05,\n",
       "          2.1632e-04, -2.6378e-04,  2.2063e-04, -1.1128e-04, -2.5677e-05,\n",
       "         -1.2822e-04,  7.2706e-05, -1.2563e-04, -1.8962e-06,  2.2731e-04,\n",
       "          7.3403e-05,  9.6304e-05, -1.9361e-04, -2.1430e-05, -2.1135e-04,\n",
       "          1.7069e-04, -3.3252e-04, -5.0850e-04,  3.0118e-05, -1.1681e-04,\n",
       "          7.1992e-05, -1.2305e-04,  2.0647e-05, -2.5102e-04, -1.3200e-04,\n",
       "          8.0130e-05,  1.7685e-04, -2.3683e-04, -4.5315e-05,  5.9836e-05,\n",
       "          2.5630e-04, -2.7121e-04, -2.6247e-04, -2.1720e-04,  3.4917e-05,\n",
       "          3.8723e-05, -4.8510e-05,  1.1579e-05,  1.1671e-04, -1.5573e-04,\n",
       "         -1.3728e-04,  4.2482e-04,  2.3556e-04, -1.8620e-04,  1.8803e-04,\n",
       "         -8.3423e-05,  1.1606e-04, -1.6166e-04, -2.1297e-04,  3.6629e-04,\n",
       "          1.2714e-04,  3.4956e-04,  3.7847e-04, -5.0622e-05, -2.5810e-04,\n",
       "          1.3618e-04,  2.1469e-04, -2.2821e-05,  7.0781e-07, -5.5857e-04,\n",
       "          2.2927e-04, -2.0227e-04, -3.8878e-04, -9.9530e-05,  1.2156e-06,\n",
       "          1.2143e-04,  6.8324e-05, -2.2497e-04,  1.6380e-05,  4.1670e-04,\n",
       "          5.5964e-05,  1.3503e-04, -3.5711e-04,  6.3288e-06, -1.9856e-04,\n",
       "          9.5645e-05, -2.6785e-05, -1.0082e-04, -3.5903e-04,  2.1776e-05,\n",
       "          4.8404e-04, -2.1780e-04, -2.5444e-05,  1.4836e-04, -1.7712e-04,\n",
       "         -4.3592e-05, -1.7617e-04,  3.6282e-04, -1.0256e-05,  3.8318e-05,\n",
       "          2.1900e-04,  3.1022e-06, -3.1471e-04, -2.2776e-04,  1.5696e-04,\n",
       "         -2.2320e-05, -1.5905e-04,  2.3356e-04,  2.5443e-04,  1.8711e-04,\n",
       "         -4.7721e-04,  5.0012e-05,  2.8769e-04,  3.5368e-04,  2.2422e-05,\n",
       "          4.2092e-05,  4.5136e-05, -1.1322e-05,  7.5812e-05, -1.5165e-04,\n",
       "          1.4690e-04,  2.6896e-04, -1.8390e-04,  9.9242e-06, -2.3605e-05,\n",
       "          5.1553e-04, -1.2720e-04, -6.9659e-05,  8.1252e-05, -3.1380e-04,\n",
       "         -3.4415e-04,  1.3801e-04]),\n",
       " tensor([-8.8394e-04, -8.6492e-04,  7.3713e-04, -1.1682e-03, -1.9145e-03,\n",
       "          9.5677e-04,  2.8968e-05, -7.2753e-04,  6.6930e-04,  2.2918e-04,\n",
       "         -9.7173e-04, -1.6332e-03, -1.7047e-03,  1.2185e-03,  4.0132e-04,\n",
       "         -5.4657e-05, -1.0345e-03, -8.3023e-04, -2.6286e-04, -5.9372e-04,\n",
       "         -1.5246e-03, -7.4655e-04,  1.2305e-03, -6.2168e-05, -1.4629e-03,\n",
       "          4.0638e-04,  1.6868e-03, -2.9153e-04, -1.7936e-03,  6.4480e-04,\n",
       "          7.0459e-04, -5.3751e-04, -3.0881e-03, -1.9159e-03, -5.2732e-04,\n",
       "          1.1457e-03, -9.1982e-04, -6.0916e-04,  1.2958e-03, -1.4539e-03,\n",
       "         -9.6011e-04, -2.9016e-03, -7.0167e-04, -3.4462e-03, -1.8488e-03,\n",
       "         -2.1181e-03,  1.4521e-03,  5.3930e-04,  9.8923e-04, -2.8980e-03,\n",
       "         -1.3711e-03, -1.0256e-03,  7.7266e-04,  5.1379e-04,  1.4509e-03,\n",
       "         -1.3322e-04,  1.6552e-04, -3.4856e-03, -9.5844e-05, -9.5582e-04,\n",
       "          1.1503e-03, -5.9736e-04, -2.3098e-03,  6.8176e-04, -8.5872e-04,\n",
       "          1.0011e-03, -1.3162e-03, -1.6726e-03,  5.2291e-04,  6.3038e-04,\n",
       "         -1.9504e-03, -2.7418e-06,  9.8610e-04,  1.0095e-03,  2.5523e-04,\n",
       "         -2.8825e-04,  3.3712e-04,  5.5581e-04,  1.7711e-03, -3.5113e-04,\n",
       "         -2.2691e-03,  8.5685e-04,  1.6630e-04, -1.2757e-03, -1.0573e-03,\n",
       "          8.9192e-04,  9.1240e-04,  2.2227e-04, -1.0464e-03, -3.0701e-03,\n",
       "         -3.2419e-04, -1.8005e-03,  3.9440e-04, -9.0146e-04, -4.6927e-04,\n",
       "          8.2409e-04,  1.1287e-03,  2.7305e-04, -5.0259e-04, -1.3392e-03,\n",
       "         -8.5336e-04, -7.6044e-04,  1.7213e-03, -1.7598e-03, -2.4170e-04,\n",
       "          8.7821e-04, -2.2827e-03, -1.6143e-03,  6.4147e-04,  3.1924e-04,\n",
       "         -4.3422e-04,  3.6031e-04,  1.3274e-04,  3.6895e-04, -3.2597e-03,\n",
       "         -8.4060e-04,  4.4107e-04,  7.9596e-04, -2.7186e-03, -2.0486e-04,\n",
       "         -3.9665e-03, -3.0047e-04, -9.1195e-06, -5.3927e-03,  8.2469e-04,\n",
       "         -1.7070e-03, -7.0548e-04, -2.2854e-03, -5.4777e-05,  7.8741e-04,\n",
       "         -3.6895e-05,  1.3292e-05,  9.7471e-04, -3.6848e-04,  1.2571e-04,\n",
       "         -2.7009e-03,  1.1748e-04,  1.0623e-03,  1.2900e-03,  1.8211e-03,\n",
       "          8.3166e-04,  4.7493e-04, -6.2156e-04, -3.9861e-03, -1.1221e-03,\n",
       "          9.3406e-04, -2.6887e-03,  1.5799e-03, -4.7677e-03,  3.7968e-04,\n",
       "          7.0029e-04,  1.6744e-03,  1.0952e-03, -4.3196e-03,  4.4763e-04,\n",
       "         -1.0098e-03, -5.2840e-04, -2.4350e-03, -2.7493e-03,  1.4312e-03,\n",
       "         -5.9968e-04,  6.8071e-04, -1.9807e-04,  7.8678e-06, -1.0499e-03,\n",
       "         -1.1860e-03,  1.6724e-03, -2.5648e-04,  2.8843e-04, -2.7803e-03,\n",
       "         -1.6304e-03, -1.5714e-03,  1.4685e-03, -1.7780e-03, -4.5776e-04,\n",
       "          1.1924e-03,  2.9695e-04,  2.2596e-04, -1.1388e-03,  2.4259e-05,\n",
       "         -3.7866e-03, -1.0992e-03, -1.5791e-03, -2.4271e-04,  1.4271e-03,\n",
       "          1.1563e-05, -1.1464e-03, -3.3167e-03, -8.3196e-04, -1.6576e-04,\n",
       "         -5.5730e-05,  4.0329e-04,  4.4167e-05, -6.1834e-04,  5.5277e-04,\n",
       "         -9.5165e-04,  1.0109e-03, -6.0028e-04, -8.5574e-04, -1.1450e-03,\n",
       "          1.1687e-03,  3.2192e-04, -1.5823e-03,  5.4538e-04,  3.3861e-04,\n",
       "         -2.3172e-03, -2.1607e-04, -6.6191e-04, -2.2112e-03, -1.3238e-03,\n",
       "         -1.2707e-03, -6.3723e-04,  1.0616e-04,  2.5868e-04,  1.0993e-03,\n",
       "         -8.5473e-04, -8.6117e-04, -1.1534e-03, -3.6252e-04,  7.5382e-04,\n",
       "         -2.4199e-03, -7.3957e-04, -2.9175e-03,  3.3945e-04, -1.2573e-03,\n",
       "          1.5358e-03,  2.5368e-04, -1.0280e-03,  6.4915e-04,  5.0718e-04,\n",
       "         -2.1338e-04,  1.6839e-03,  8.7619e-06, -2.4402e-04,  2.3097e-04,\n",
       "         -8.5211e-04, -1.3334e-04,  1.7709e-03, -1.7674e-03, -2.1765e-03,\n",
       "          1.5677e-03,  5.5164e-04, -1.1504e-04, -1.1354e-03,  1.5773e-03,\n",
       "         -1.7424e-03,  1.9073e-04, -4.9967e-04,  1.7716e-03, -1.2207e-04,\n",
       "          1.8466e-03,  3.4493e-04, -8.4281e-04,  1.0328e-03,  1.6406e-03,\n",
       "          3.7169e-04, -2.3823e-03,  1.0270e-04,  8.0836e-04, -6.9267e-04,\n",
       "          9.2578e-04, -4.7094e-04,  3.5870e-04, -5.0604e-04, -7.8690e-04,\n",
       "         -2.3758e-04,  1.9860e-04, -8.3965e-04, -5.7166e-03,  8.5175e-05,\n",
       "          4.9990e-04, -1.7852e-04, -9.7024e-04,  5.0861e-04,  3.4219e-04,\n",
       "          4.6569e-04,  1.7355e-03,  2.7573e-04,  2.0385e-05,  3.7915e-04,\n",
       "         -1.1970e-03, -2.5763e-03, -3.1674e-03, -2.7581e-03, -9.5576e-04,\n",
       "         -1.2656e-03, -4.9400e-04, -1.4796e-03,  6.2060e-04,  1.5664e-03,\n",
       "          9.3567e-04, -3.7107e-03, -3.5375e-03, -3.6356e-03, -7.1210e-04,\n",
       "         -1.0921e-03,  1.1395e-03, -2.0647e-04, -1.3627e-03,  2.7877e-04,\n",
       "          1.9825e-04, -8.1486e-04, -3.2371e-04, -6.4731e-04, -8.5127e-04,\n",
       "         -9.7334e-04, -6.7431e-04, -1.3711e-03, -8.7810e-04, -1.7495e-03,\n",
       "          1.7327e-04,  8.2463e-04, -1.1824e-03,  1.1311e-03, -5.7745e-04,\n",
       "         -9.6333e-04, -3.3717e-03,  9.5749e-04,  7.2187e-04,  5.2208e-04,\n",
       "          1.2504e-03, -2.1267e-04,  1.0238e-03, -3.1191e-03, -1.9938e-03,\n",
       "          8.0734e-04, -5.6926e-03, -8.7833e-04, -7.7921e-04, -5.6088e-04,\n",
       "          8.0633e-04, -5.4137e-03, -5.2392e-04,  5.7119e-04, -3.1838e-03,\n",
       "         -2.4493e-03,  9.7430e-04, -3.1391e-04, -1.1641e-04, -2.4257e-03,\n",
       "          4.8029e-04, -3.1258e-03,  1.7976e-03, -2.5616e-03, -2.0685e-03,\n",
       "         -1.0651e-03, -3.3022e-03, -1.7043e-03, -7.1579e-04, -4.7332e-04,\n",
       "         -1.1659e-03, -2.2219e-03, -4.2105e-04, -1.4780e-03, -1.6972e-03,\n",
       "          2.0003e-03, -1.7983e-03, -1.4260e-03, -8.0627e-04, -2.8620e-03,\n",
       "          1.4954e-03,  1.5970e-03, -8.1640e-04,  3.9488e-04, -6.1589e-04,\n",
       "          1.0207e-03, -2.2775e-04,  3.5352e-04, -7.2581e-04, -7.7701e-04,\n",
       "         -2.8018e-03, -1.4007e-03, -1.9578e-03,  7.4160e-04, -2.2770e-03,\n",
       "          1.1594e-03,  3.4446e-04,  2.3043e-04,  2.9117e-04, -8.6057e-04,\n",
       "         -2.1068e-03, -2.2445e-03, -1.1283e-03, -1.1637e-03,  8.1253e-04,\n",
       "         -9.2191e-04,  8.3172e-04, -2.1687e-03,  9.8819e-04, -1.0514e-03,\n",
       "         -1.3177e-03, -2.1642e-04, -1.4086e-03,  3.8475e-04, -3.2490e-04,\n",
       "         -2.5352e-03, -2.4754e-03,  1.8716e-05, -8.5229e-04, -1.8597e-05,\n",
       "         -1.1880e-03,  1.0225e-03,  7.6121e-04, -2.6402e-03, -4.8012e-04,\n",
       "         -4.2856e-04, -1.9323e-03,  8.0043e-04,  6.9720e-04,  1.0976e-03,\n",
       "         -1.0017e-03, -2.6947e-04, -8.7732e-04, -1.6828e-03, -1.2547e-03,\n",
       "          5.6118e-04,  1.6147e-03,  4.4280e-04,  1.5215e-03, -8.0949e-04,\n",
       "          2.5679e-03, -4.8581e-03, -1.1665e-04,  7.6979e-04, -4.1699e-04,\n",
       "          4.6116e-04, -2.0915e-03,  1.8120e-05, -1.6510e-04, -2.9367e-03,\n",
       "         -9.2548e-04,  1.1808e-04,  1.0610e-04, -1.2789e-03, -3.1590e-06,\n",
       "         -8.7994e-04, -2.8808e-03,  6.0624e-04, -1.7893e-04,  1.3903e-03,\n",
       "         -3.2136e-03, -2.9534e-04, -1.0794e-03, -5.0902e-05,  4.7922e-05,\n",
       "          4.7058e-04,  1.1283e-03, -8.0776e-04, -2.6304e-03, -4.0396e-03,\n",
       "          2.1169e-04,  1.1151e-03, -4.5836e-05,  1.8654e-03, -3.1888e-04,\n",
       "         -4.7112e-04, -4.5061e-04,  9.9844e-04, -1.7667e-04,  1.2782e-03,\n",
       "         -2.2002e-03,  2.7519e-04,  2.7418e-06, -2.0860e-03,  9.9522e-04,\n",
       "         -4.5723e-04, -1.3129e-03,  2.5070e-04, -2.5781e-03, -7.5030e-04,\n",
       "         -3.8124e-03, -8.2123e-04,  1.5916e-03,  1.6788e-03,  1.7875e-03,\n",
       "         -1.5080e-03, -1.9968e-03, -1.4553e-03, -5.2937e-03,  1.0073e-04,\n",
       "          1.3329e-03,  8.0109e-05,  1.8191e-04,  1.4789e-03, -9.6089e-04,\n",
       "         -4.5723e-04,  9.5105e-04,  1.4558e-03,  5.4288e-04, -2.2328e-04,\n",
       "          2.4709e-03, -1.0749e-03,  7.6622e-04,  1.5149e-03, -2.0400e-03,\n",
       "          1.1637e-03,  2.7418e-06, -7.1752e-04, -1.3246e-03, -7.2366e-04,\n",
       "          3.3599e-04, -5.8460e-04, -2.0159e-03, -1.6274e-03, -1.7494e-05,\n",
       "          3.1513e-04,  1.0790e-03,  9.2155e-04,  8.5735e-04, -3.1722e-04,\n",
       "         -3.7001e-03, -1.6398e-03]),\n",
       " tensor([ 3.6097e-04, -2.4932e-05,  5.1689e-04,  3.8579e-04, -1.0124e-04,\n",
       "          2.8724e-04, -1.3210e-04, -2.2173e-05, -2.6343e-04, -3.9311e-05,\n",
       "          3.8540e-04,  1.2257e-04, -1.7255e-04, -3.9655e-04, -9.5263e-05,\n",
       "          5.8655e-06, -3.8144e-04, -1.6893e-04,  5.9195e-05, -3.1434e-04,\n",
       "          1.1900e-04, -3.8981e-04, -2.4486e-04, -1.5433e-04,  2.7634e-05,\n",
       "         -6.3479e-05,  1.9241e-04,  1.5627e-04, -4.4977e-04,  3.7926e-04,\n",
       "          3.0711e-04, -1.8549e-04, -1.8135e-05,  5.2406e-04,  7.7549e-05,\n",
       "         -1.3085e-05, -2.0646e-04, -3.3550e-04,  3.0754e-04,  3.5253e-04,\n",
       "         -1.1378e-04, -3.7311e-05,  4.7452e-04,  5.0597e-05,  1.4923e-04,\n",
       "         -4.6701e-04, -2.6931e-04,  5.4747e-04, -6.0208e-04, -1.8930e-04,\n",
       "          6.1713e-05,  1.3992e-04,  9.5837e-05,  2.0612e-04, -1.9363e-04,\n",
       "          1.5256e-05, -4.8832e-04, -2.3339e-04,  4.3630e-04, -2.1486e-04,\n",
       "         -2.5308e-04, -2.9273e-05,  2.4719e-04, -3.1328e-05, -1.5475e-04,\n",
       "         -5.8699e-05, -3.6171e-04,  6.6643e-04,  3.9962e-04, -4.3761e-05,\n",
       "          1.5645e-04,  9.1270e-07, -1.6569e-04,  7.4767e-05,  1.4115e-04,\n",
       "          1.1718e-04,  1.8687e-05, -1.3663e-04, -2.3483e-04, -2.7515e-04,\n",
       "         -2.9719e-04,  1.0803e-07,  1.8066e-05,  2.5405e-04,  2.9674e-04,\n",
       "         -4.0397e-04,  1.1366e-04, -5.0146e-05, -2.0665e-04,  8.4466e-05,\n",
       "          6.0730e-05, -1.6832e-04, -1.0220e-04,  3.7492e-04, -1.0175e-04,\n",
       "          1.7568e-04, -8.0433e-05, -2.0572e-04,  7.5307e-05, -7.8274e-04,\n",
       "         -1.5507e-04,  2.1664e-05, -5.9673e-04, -5.4963e-05,  2.2026e-04,\n",
       "          7.7266e-05, -3.4988e-04, -1.0928e-04, -4.8925e-04,  4.4820e-04,\n",
       "         -2.7144e-04,  5.1325e-04,  2.0613e-04,  5.5802e-04,  1.7722e-04,\n",
       "         -2.5156e-04,  2.4047e-04,  1.8040e-04, -3.0155e-04,  9.3160e-06,\n",
       "          2.6226e-04,  4.1582e-04,  3.8827e-04,  1.6427e-04, -3.8305e-04,\n",
       "         -1.6989e-04,  1.9771e-04,  1.8211e-04, -9.0044e-05, -3.6749e-05,\n",
       "          8.4110e-05,  9.4423e-05,  1.0121e-04,  2.8038e-04,  1.6758e-04,\n",
       "          4.8448e-04, -2.2676e-04,  9.3954e-05,  1.6154e-04, -4.3027e-05,\n",
       "          2.9608e-04,  1.9449e-04,  4.0505e-05, -6.5683e-04, -7.3325e-05,\n",
       "          2.4034e-04, -8.5129e-04, -4.6521e-05, -9.8310e-05,  4.3201e-04,\n",
       "          1.2713e-04, -8.7189e-05, -1.5090e-04, -6.4829e-05, -3.4337e-04,\n",
       "         -1.0482e-04, -2.3589e-04, -8.0211e-04, -3.7673e-04,  4.8681e-04,\n",
       "         -3.8940e-04, -2.5557e-04,  1.6783e-04,  4.4325e-04, -4.6846e-05,\n",
       "         -3.8417e-04,  1.0009e-04,  4.8377e-05,  1.9136e-04, -2.6823e-04,\n",
       "          4.4389e-05,  3.6370e-04, -1.5637e-04,  2.9201e-05,  1.9624e-04,\n",
       "          1.7686e-04, -2.6932e-04, -2.6652e-04,  4.8892e-05, -5.1796e-04,\n",
       "          1.3150e-05, -4.9561e-05,  2.6914e-04, -5.4517e-04, -1.1236e-04,\n",
       "         -1.0961e-04, -1.5389e-04,  3.1461e-04, -2.8676e-04, -2.0530e-04,\n",
       "          2.8220e-04, -2.1683e-04,  1.8515e-04, -1.1286e-04, -3.3356e-04,\n",
       "          2.5864e-05,  7.5497e-05, -2.1944e-04, -2.2308e-04,  5.2610e-04,\n",
       "          5.1621e-05,  1.1557e-04, -2.3674e-04, -9.8818e-05,  3.4310e-06,\n",
       "          5.8818e-04,  9.4872e-05,  9.7513e-05,  2.1743e-03, -1.3877e-04,\n",
       "         -2.6665e-04, -9.7662e-05,  9.6820e-05,  2.2885e-04,  5.6840e-05,\n",
       "          5.3576e-04,  4.1390e-04, -4.2268e-04,  5.0835e-05,  5.2641e-05,\n",
       "          4.9728e-05,  1.7560e-04, -3.7074e-05,  4.6687e-06,  2.6142e-05,\n",
       "          1.2949e-04,  3.5032e-04,  3.5410e-04,  1.5038e-04,  1.8276e-04,\n",
       "          1.4123e-04, -5.8092e-05, -2.2393e-05,  9.7640e-05, -6.0432e-04,\n",
       "         -2.3385e-04,  5.3152e-05, -5.2856e-05,  1.9707e-03, -4.3552e-05,\n",
       "         -1.5868e-04,  1.3431e-04, -1.6652e-04, -2.6512e-05,  3.4389e-04,\n",
       "         -4.1221e-04,  3.3371e-05, -1.1379e-04, -6.8791e-04,  3.3434e-06,\n",
       "         -2.1504e-04, -8.3677e-05,  5.2597e-04,  3.4668e-04, -2.7920e-04,\n",
       "         -7.9498e-05, -1.3521e-04, -5.1118e-05,  1.9702e-04, -3.2105e-04,\n",
       "          8.8118e-05,  2.0489e-04, -1.8864e-04,  1.5979e-04, -6.3777e-05,\n",
       "          7.4338e-06,  8.4440e-04, -1.3673e-06, -3.8616e-04,  9.7157e-04,\n",
       "          1.1607e-04, -3.3293e-04,  3.2379e-04,  1.4333e-04, -3.8929e-04,\n",
       "         -1.5945e-04, -3.2075e-04,  1.8324e-04,  1.4398e-04, -4.3813e-04,\n",
       "         -6.6768e-04,  4.1634e-04, -4.1988e-04, -5.2327e-04, -2.4261e-04,\n",
       "         -2.7490e-04, -2.1213e-04, -3.5824e-05,  1.0788e-04, -1.0397e-04,\n",
       "          3.7391e-04, -5.3637e-04, -3.0105e-04, -4.7394e-04,  2.8910e-04,\n",
       "          9.6621e-05,  5.9120e-05,  2.5842e-05, -1.4942e-05,  6.6148e-05,\n",
       "          2.9735e-04, -2.3250e-04,  8.3432e-05,  1.5181e-04,  7.6922e-04,\n",
       "         -5.0887e-05,  3.0981e-04, -3.6324e-04, -3.3182e-04,  7.5014e-05,\n",
       "         -9.1214e-04, -1.4628e-04,  1.7711e-04,  5.4034e-04, -9.4754e-04,\n",
       "          1.4216e-04,  6.3017e-05, -1.7367e-04, -4.4024e-04,  1.9792e-04,\n",
       "         -7.7887e-05, -3.7527e-04, -2.5817e-04, -5.5954e-06, -1.6714e-04,\n",
       "          1.2007e-04,  9.7206e-05,  3.2066e-04, -3.8090e-04, -2.2233e-04,\n",
       "         -1.8406e-04,  1.3224e-04, -8.4455e-05, -1.3821e-04, -4.9414e-04,\n",
       "          4.3837e-04,  1.3968e-04, -1.4609e-04,  4.9554e-04,  3.3501e-04,\n",
       "          6.0470e-04, -4.7635e-04,  1.0767e-04, -4.3932e-05,  2.7048e-04,\n",
       "         -3.0655e-04, -8.4706e-05,  2.3393e-04,  2.4776e-04,  3.3267e-04,\n",
       "          3.5179e-04,  6.5293e-05, -2.5226e-04, -1.5481e-04,  1.1838e-04,\n",
       "         -5.9219e-04,  5.1453e-04, -4.6765e-04,  1.5984e-04, -1.1287e-04,\n",
       "         -1.2934e-04, -3.1992e-04, -2.8292e-05, -4.6001e-04, -1.3627e-04,\n",
       "          9.5880e-05,  5.5782e-05,  4.4880e-04, -3.0824e-04, -3.8414e-04,\n",
       "         -2.2396e-04,  1.3528e-04, -1.2011e-04, -3.0513e-04, -1.8707e-04,\n",
       "          2.4823e-05,  1.1896e-04,  2.5660e-04,  3.2358e-04, -1.5151e-04,\n",
       "          1.6533e-04, -3.1906e-04,  4.3878e-04, -3.6734e-05,  6.8093e-05,\n",
       "          1.6839e-04,  2.0807e-04,  4.2639e-04,  1.8866e-04,  7.1374e-04,\n",
       "          2.8711e-05, -3.4557e-04,  2.5262e-04, -7.2768e-05,  1.5983e-04,\n",
       "         -5.4418e-04,  3.2274e-05, -1.7123e-04, -3.3316e-04, -1.6607e-04,\n",
       "          6.2427e-05,  7.8242e-05, -4.1902e-04,  1.4868e-05, -3.4094e-04,\n",
       "          2.4793e-04, -2.0003e-04, -2.0692e-04,  4.5742e-04, -5.0901e-05,\n",
       "         -1.7192e-04, -4.4115e-05,  5.3097e-05, -1.3518e-04, -1.5969e-04,\n",
       "          7.2883e-05, -1.4547e-04, -8.6506e-05, -2.1404e-04,  2.6762e-05,\n",
       "         -1.6184e-04,  4.0233e-07, -7.4904e-05, -3.0065e-05, -4.4676e-04,\n",
       "         -2.4959e-04, -6.6776e-07, -3.9378e-04,  5.5601e-05, -9.1062e-05,\n",
       "          2.6896e-04, -6.9783e-05, -4.4490e-04, -4.7882e-04,  6.7624e-05,\n",
       "          1.7438e-04, -9.2369e-05,  2.4956e-04, -2.0370e-04, -3.5798e-04,\n",
       "         -4.5043e-05,  1.6331e-04,  1.2313e-04,  2.4578e-05, -3.1007e-04,\n",
       "         -6.8383e-05,  3.4527e-04,  3.1261e-04,  5.5335e-05, -3.3917e-04,\n",
       "          6.7037e-04,  4.9695e-05,  5.6005e-04, -9.8736e-05,  3.5995e-04,\n",
       "         -2.8807e-04,  5.1815e-04, -5.3842e-05, -2.3555e-05,  2.4756e-04,\n",
       "         -9.8777e-05, -1.7985e-04,  1.4372e-05, -9.3665e-05,  1.5241e-04,\n",
       "          5.3560e-05,  5.4973e-04, -1.1583e-04, -1.2244e-04, -2.2866e-04,\n",
       "          1.7051e-05, -1.4762e-04, -2.5128e-04,  6.2443e-05,  6.8069e-05,\n",
       "          1.7146e-04, -5.9223e-04, -5.6308e-04, -5.9903e-04, -3.1177e-04,\n",
       "          4.8048e-04,  1.2778e-06,  5.3719e-05,  5.4654e-05,  1.6975e-04,\n",
       "         -2.2332e-04,  8.7246e-04, -1.5703e-04, -3.0059e-04, -4.7386e-06,\n",
       "          2.1966e-04, -4.6066e-04, -1.3082e-04,  3.0207e-04, -1.6156e-04,\n",
       "         -2.3337e-04, -4.1459e-04, -3.6228e-06,  5.9217e-04,  1.0584e-04,\n",
       "          2.3566e-04,  5.0155e-04,  6.5994e-06,  6.0678e-04, -3.5688e-05,\n",
       "         -3.9375e-04,  9.7658e-05, -3.1788e-04, -3.6795e-04,  2.3743e-04,\n",
       "         -2.4039e-05,  1.2006e-04]),\n",
       " tensor([[-1.7142e-03, -1.0672e-03,  3.9918e-03,  ...,  2.1949e-04,\n",
       "          -2.3680e-03,  2.9961e-04],\n",
       "         [ 2.2166e-03,  4.4090e-04,  7.7263e-05,  ...,  1.5939e-03,\n",
       "           2.8180e-04,  2.8159e-03],\n",
       "         [-7.9595e-04, -5.0573e-04, -1.1839e-03,  ...,  1.1885e-03,\n",
       "           7.2043e-04,  1.7776e-03],\n",
       "         ...,\n",
       "         [ 1.3491e-04,  2.0435e-04,  8.7352e-04,  ...,  6.8988e-04,\n",
       "          -7.8371e-04,  1.6385e-04],\n",
       "         [ 1.1279e-03, -1.2823e-03,  1.9757e-04,  ..., -2.1943e-03,\n",
       "           3.1305e-04, -6.5007e-04],\n",
       "         [ 1.0457e-03, -3.5918e-03,  8.7079e-04,  ..., -7.0012e-04,\n",
       "          -3.3638e-03, -1.1550e-04]]),\n",
       " tensor([ 4.3648e-04,  6.7993e-04,  1.4063e-03,  ...,  1.5005e-04,\n",
       "         -3.3498e-05,  9.7782e-05]),\n",
       " tensor([[-1.5406e-04, -1.7237e-04,  1.6308e-03,  ...,  8.0479e-04,\n",
       "           1.7578e-03,  1.4983e-04],\n",
       "         [ 7.0026e-04, -2.0246e-03, -1.1688e-03,  ..., -7.8563e-04,\n",
       "          -6.9487e-04, -2.7934e-03],\n",
       "         [ 2.9651e-04,  2.7507e-04,  9.9177e-04,  ..., -1.6877e-04,\n",
       "           8.4671e-04,  1.5260e-03],\n",
       "         ...,\n",
       "         [ 4.1681e-04, -2.8429e-04,  5.9445e-04,  ...,  2.9206e-04,\n",
       "          -1.7210e-03, -2.0815e-03],\n",
       "         [ 5.6440e-04, -9.0087e-05, -1.1413e-03,  ..., -2.6552e-04,\n",
       "          -3.1705e-04,  2.5272e-05],\n",
       "         [-7.5305e-04,  1.2047e-04, -1.3953e-03,  ..., -1.2584e-05,\n",
       "          -3.1059e-03,  1.6459e-03]]),\n",
       " tensor([ 6.9709e-07,  6.4336e-06, -1.3049e-04,  2.0633e-05,  1.7222e-05,\n",
       "          1.1350e-04, -7.3286e-05,  5.5498e-05,  3.7998e-05, -2.7936e-04,\n",
       "         -3.1738e-05,  8.0797e-05,  1.9837e-04, -3.7754e-05,  8.3441e-05,\n",
       "          1.3527e-04,  4.3097e-04,  1.2314e-05, -2.9153e-04,  2.2084e-04,\n",
       "         -4.1995e-04,  2.3646e-04,  4.6718e-05,  1.2225e-04, -1.5536e-05,\n",
       "          1.1655e-04,  2.6825e-04,  2.1894e-04,  1.7823e-04, -1.1637e-04,\n",
       "         -3.0884e-04, -5.6473e-05,  1.0496e-04, -4.0662e-04, -2.1417e-04,\n",
       "          1.8129e-05, -1.9360e-04,  1.7102e-04,  9.3824e-05,  4.2343e-05,\n",
       "          3.5322e-05,  4.5791e-05, -2.1541e-04,  6.1127e-05, -1.0848e-04,\n",
       "         -6.2218e-05, -9.2931e-05, -4.7760e-05,  3.2529e-04,  2.5964e-04,\n",
       "         -2.3970e-04, -1.0801e-04,  4.4372e-05, -1.2588e-04,  8.5542e-06,\n",
       "         -3.0982e-04,  1.8028e-04, -2.3716e-04,  1.7701e-04,  8.4162e-05,\n",
       "         -7.2444e-05,  1.9807e-04, -3.5825e-04,  1.8019e-04, -3.5258e-05,\n",
       "         -7.6641e-05, -1.5675e-04, -2.2013e-04,  3.8344e-04, -5.0387e-05,\n",
       "          1.8637e-04, -1.9377e-05,  3.2560e-04,  1.7066e-04,  4.5686e-04,\n",
       "          4.4629e-05, -2.3405e-04,  2.5174e-04,  1.6304e-04,  2.5718e-05,\n",
       "          1.1558e-04, -6.5085e-05,  2.8249e-04, -1.2117e-04, -1.3565e-04,\n",
       "          3.9624e-04,  1.8407e-04,  6.1411e-05, -1.2045e-04, -4.1261e-05,\n",
       "          6.0645e-05, -2.8327e-04, -7.8890e-05, -1.3789e-04,  8.8519e-05,\n",
       "         -4.5834e-04,  4.2377e-05,  8.2809e-05, -4.1998e-05,  3.5934e-05,\n",
       "         -6.6665e-05,  5.5145e-05,  1.4593e-04,  4.9670e-05, -2.1787e-05,\n",
       "         -2.4570e-04,  1.1286e-04,  2.2042e-04,  8.9975e-05, -5.3925e-04,\n",
       "         -2.0565e-04, -3.4100e-04, -3.9736e-04, -3.6260e-04, -1.6846e-04,\n",
       "          4.7905e-04, -2.5423e-05,  1.0237e-04, -1.3300e-04, -3.6767e-04,\n",
       "         -1.2910e-04, -2.4859e-04, -2.9334e-04, -3.3922e-04,  1.0452e-04,\n",
       "          9.8014e-05,  4.8300e-05,  5.2850e-05, -5.3306e-05,  1.5450e-04,\n",
       "         -1.7946e-04,  3.4386e-05, -1.1164e-04, -1.4830e-05, -2.3039e-04,\n",
       "          1.8655e-04,  1.5135e-04, -2.0019e-04, -3.4223e-04,  2.8374e-04,\n",
       "          2.1420e-04, -7.4344e-05,  1.2213e-04,  1.9626e-04,  2.7381e-04,\n",
       "         -3.8389e-04,  4.0941e-04, -2.1595e-04, -3.4370e-05, -4.8069e-05,\n",
       "         -6.8154e-05,  1.1026e-04, -7.4503e-05, -8.2762e-05,  4.6214e-04,\n",
       "          1.7156e-05,  2.8178e-04, -3.9529e-05, -1.2297e-05,  2.1029e-04,\n",
       "          1.6370e-04, -3.0183e-04,  1.5728e-04, -2.9850e-04,  9.3405e-05,\n",
       "         -2.2459e-04,  2.7874e-04, -3.4304e-04,  8.4545e-05,  5.0656e-04,\n",
       "          1.5066e-04, -3.5422e-04,  1.9104e-04,  2.8479e-04,  1.6811e-04,\n",
       "         -1.0607e-04, -5.5796e-05,  4.3645e-04, -6.9421e-05,  4.6265e-05,\n",
       "          7.4001e-05, -3.3291e-05, -1.5870e-04,  2.4428e-04,  1.2801e-04,\n",
       "          7.6942e-05, -7.2407e-05,  1.0169e-04, -1.7706e-04, -8.2369e-05,\n",
       "         -3.9287e-05, -4.0792e-05, -4.5388e-05, -1.7594e-04,  1.1033e-04,\n",
       "         -9.4658e-05,  5.3162e-04,  1.5496e-04,  1.4566e-04, -8.9779e-07,\n",
       "         -1.2608e-04, -1.5993e-04,  2.6552e-04,  6.4809e-05, -4.0551e-04,\n",
       "         -3.0845e-04, -5.1092e-06, -5.0310e-05, -5.6506e-04, -2.7088e-04,\n",
       "          2.7890e-04, -3.4463e-04, -5.8537e-04,  1.4949e-04,  3.5169e-05,\n",
       "         -1.2383e-05, -2.5147e-04,  1.0231e-04,  4.2233e-05,  1.8476e-04,\n",
       "         -2.2951e-04, -4.4736e-04,  4.1314e-04,  5.6632e-04, -1.0545e-04,\n",
       "         -4.1796e-04,  1.7561e-04, -2.4755e-05,  6.1200e-05,  1.2275e-04,\n",
       "         -1.2202e-04,  3.1944e-04,  9.4748e-05,  2.2307e-04,  3.0882e-04,\n",
       "         -2.8249e-05, -3.3954e-04,  1.5452e-04, -8.5484e-04,  6.8805e-05,\n",
       "          2.8719e-05,  4.5453e-05,  1.6350e-04, -1.9341e-04, -3.1185e-04,\n",
       "          8.8204e-05, -5.1405e-05,  6.0163e-07,  5.1189e-04,  1.2144e-05,\n",
       "          3.0097e-04, -2.4436e-04, -1.6529e-04,  8.6598e-05,  8.1573e-05,\n",
       "          1.9565e-04,  2.2212e-04, -1.9019e-04,  1.9715e-04, -2.8497e-04,\n",
       "         -3.1597e-04, -3.5946e-05,  5.0134e-05, -1.3623e-04,  1.5017e-04,\n",
       "         -1.3637e-04, -6.6323e-04,  1.2048e-04,  2.2306e-04, -1.7938e-04,\n",
       "          5.9602e-04, -1.9983e-04, -2.9101e-04, -1.9391e-04,  1.9459e-04,\n",
       "         -1.1910e-05,  6.0084e-04, -1.0240e-04,  1.3030e-04,  8.5179e-06,\n",
       "          6.2339e-05,  1.8075e-05,  2.8228e-04, -9.9378e-05,  1.2553e-04,\n",
       "          3.8104e-04, -3.7330e-04,  1.5518e-05, -1.9655e-05, -2.8750e-04,\n",
       "         -2.8244e-04,  8.6951e-05, -4.3832e-05,  2.3049e-04, -8.7700e-05,\n",
       "         -1.2424e-04,  3.9989e-04,  8.2459e-06,  6.2915e-05, -1.1206e-04,\n",
       "          1.1494e-04, -4.9716e-05,  3.7800e-04,  4.1898e-05, -2.8979e-04,\n",
       "          2.7165e-04, -3.5183e-04, -3.4375e-06,  3.1177e-04, -1.2680e-04,\n",
       "          7.8991e-04, -1.3477e-04, -2.7771e-04, -1.2511e-04,  1.0420e-04,\n",
       "         -1.8651e-04, -5.6120e-05,  1.6735e-04,  3.5702e-04, -1.4731e-04,\n",
       "          3.6909e-04, -5.9687e-05,  1.0784e-04, -1.6057e-05,  1.4197e-04,\n",
       "         -1.1724e-04, -7.0715e-05, -7.4254e-05,  1.1032e-04,  2.5566e-04,\n",
       "          2.4461e-04, -2.0956e-04, -2.3803e-04, -2.6945e-04,  4.5724e-04,\n",
       "         -1.6677e-04,  4.3136e-05,  2.8038e-04, -2.5031e-04, -1.1978e-04,\n",
       "          9.9935e-05,  4.2216e-04,  1.5166e-04,  7.5015e-05, -1.2592e-04,\n",
       "          2.1965e-04, -2.5698e-04, -3.0922e-04, -1.1711e-06, -1.0291e-04,\n",
       "          2.4162e-04, -1.1516e-04, -3.4306e-04, -1.4522e-04, -1.6555e-04,\n",
       "          3.5577e-04, -7.2211e-05,  8.6760e-05,  3.5102e-04, -1.1558e-04,\n",
       "         -1.7261e-04,  4.5288e-04, -1.7176e-05,  2.1385e-04,  1.7783e-04,\n",
       "          8.6342e-05, -2.0064e-04, -2.0504e-04, -7.8408e-05, -2.5153e-04,\n",
       "          4.0022e-04, -8.8228e-05, -2.9669e-04,  1.7747e-05,  2.3298e-04,\n",
       "          3.1847e-04, -1.7103e-04, -1.3075e-04, -1.3782e-04,  6.8067e-05,\n",
       "          8.2691e-05,  7.8401e-05,  7.9863e-05, -1.4038e-04, -2.6159e-04,\n",
       "         -2.5423e-05, -2.9825e-04,  1.0357e-04,  1.4077e-04, -2.7394e-04,\n",
       "          2.5422e-04, -1.9961e-04,  8.7631e-05, -3.4291e-05, -1.2082e-04,\n",
       "         -6.7614e-07,  8.0153e-05, -8.2832e-06,  2.8196e-04,  2.3979e-04,\n",
       "         -1.0962e-05,  9.3449e-06, -5.1281e-05,  1.4157e-05, -1.1007e-04,\n",
       "          1.0816e-04, -1.4308e-04, -3.6124e-04, -7.6868e-05, -1.7989e-04,\n",
       "          1.0799e-04, -8.4467e-05,  8.4175e-06, -1.5032e-04, -9.2495e-05,\n",
       "          1.6310e-04,  2.2050e-04, -2.3525e-04,  6.2047e-05,  1.4963e-05,\n",
       "          2.1606e-04, -2.6137e-04, -2.8944e-04, -2.1826e-04,  9.6245e-05,\n",
       "          1.4898e-04, -7.2960e-06,  8.1484e-05, -1.0632e-05, -1.2136e-04,\n",
       "         -2.1107e-04,  4.8158e-04,  4.0197e-04, -6.3065e-06,  1.9544e-04,\n",
       "         -1.2577e-04,  1.8165e-04, -2.9085e-04, -1.0175e-04,  4.4097e-04,\n",
       "          1.1113e-04,  3.0375e-04,  2.7356e-04,  1.9510e-05, -2.0934e-04,\n",
       "          2.1672e-04,  3.2380e-05, -1.2852e-04,  1.2465e-05, -1.5021e-04,\n",
       "         -1.4103e-03, -2.2894e-04, -4.5448e-04, -1.2916e-04, -1.1649e-04,\n",
       "          1.9109e-04, -1.5007e-05, -2.0884e-04,  8.4279e-05,  2.0943e-04,\n",
       "          1.2460e-04,  1.9312e-04, -2.4248e-04,  1.9509e-05, -1.6403e-04,\n",
       "          3.9387e-05, -1.4433e-04, -7.9865e-05, -2.1686e-04,  8.7202e-05,\n",
       "          3.5616e-04, -1.5032e-04,  1.0985e-04,  1.4923e-04, -1.7298e-04,\n",
       "          1.8530e-05,  6.9275e-05,  5.8232e-04,  6.6549e-05,  9.9258e-05,\n",
       "          9.4542e-05, -8.5486e-06, -2.4646e-04, -1.4044e-04,  1.8059e-04,\n",
       "          2.7328e-05, -4.3825e-04,  2.9971e-04,  3.4120e-04,  1.8645e-04,\n",
       "         -5.0772e-04,  1.2769e-04,  3.2103e-04,  2.0233e-04,  8.1440e-06,\n",
       "          9.0962e-05,  1.7508e-04, -6.6174e-06, -1.0981e-04, -2.7720e-04,\n",
       "          1.7916e-05,  9.5037e-05, -2.1884e-04, -2.5493e-04,  1.3766e-05,\n",
       "          6.7292e-04, -2.4958e-04, -2.3581e-06,  2.1553e-04, -3.9755e-04,\n",
       "         -2.8690e-04,  4.7055e-05]),\n",
       " tensor([ 5.5265e-04,  8.5986e-04,  6.3789e-04, -7.7993e-04, -2.5681e-03,\n",
       "          2.2131e-03,  4.1825e-03,  3.8123e-04,  1.0911e-03,  1.5108e-03,\n",
       "         -1.2451e-04, -3.8940e-03, -6.4325e-04, -4.3696e-04,  2.8735e-03,\n",
       "          1.8090e-04, -1.9401e-03, -1.2748e-03,  8.0299e-04,  8.3923e-04,\n",
       "          1.1213e-03,  1.3036e-04, -4.8697e-04,  2.1576e-03, -1.2174e-03,\n",
       "          9.0247e-04,  1.4478e-03, -2.8288e-04,  9.3514e-04,  2.6553e-03,\n",
       "         -7.8011e-04,  4.5667e-03, -3.3336e-03,  1.4771e-03,  2.4310e-03,\n",
       "          3.1771e-03,  1.1750e-03, -8.2493e-04,  2.6045e-03,  4.5633e-04,\n",
       "          5.3972e-04, -5.9247e-05, -2.5010e-04,  2.2362e-03, -4.3267e-04,\n",
       "         -1.6971e-03,  1.3396e-03,  2.0897e-03,  1.8021e-03,  5.6410e-04,\n",
       "          2.2535e-03,  1.0607e-03,  2.3746e-03,  1.8980e-03,  2.6541e-03,\n",
       "          2.6169e-03,  2.6845e-03, -5.3191e-04,  1.7688e-03,  1.5199e-03,\n",
       "          3.3057e-04,  1.5671e-03, -2.5046e-04,  3.1086e-03,  8.1825e-04,\n",
       "          5.3746e-04, -8.0466e-05, -1.0335e-03,  3.4527e-03,  2.3280e-03,\n",
       "         -5.1533e-03,  3.7720e-03,  3.8791e-03,  9.0712e-04,  1.5209e-03,\n",
       "         -4.8423e-04,  1.9002e-03,  2.0832e-03,  1.0807e-03,  3.8671e-04,\n",
       "         -6.8772e-04,  2.9708e-03,  1.4169e-03,  3.8302e-04,  6.9439e-04,\n",
       "          1.7022e-03,  2.0555e-03,  3.8066e-03, -7.7254e-04, -4.6942e-03,\n",
       "          1.5792e-03,  1.3482e-03, -2.9230e-04,  1.5494e-03,  3.2727e-03,\n",
       "          3.8440e-03,  3.9968e-03,  2.5296e-03, -4.4725e-03, -2.5350e-04,\n",
       "          2.0266e-04,  2.4712e-03,  2.4974e-03,  2.1124e-03, -6.0105e-04,\n",
       "          9.2626e-04,  3.5155e-04,  1.5011e-03, -3.5989e-04,  1.1560e-03,\n",
       "         -6.0201e-05,  1.0991e-03,  1.9593e-03,  9.5344e-04,  7.2086e-04,\n",
       "          6.1393e-04, -2.2852e-03,  2.3767e-03, -3.5125e-03,  1.5974e-04,\n",
       "         -8.2159e-04,  1.5669e-03, -1.3801e-03, -7.8424e-03,  2.5972e-03,\n",
       "         -1.2116e-03,  2.5769e-03,  2.9468e-03, -9.6208e-04,  1.7843e-03,\n",
       "          3.1185e-04,  1.6691e-03,  1.4277e-03, -8.2278e-04,  9.7632e-04,\n",
       "          1.6100e-03, -3.4124e-04,  8.3989e-04,  3.4690e-03,  1.2393e-03,\n",
       "         -2.4515e-04,  1.9292e-03, -1.3191e-03,  3.2769e-03,  2.1214e-03,\n",
       "          1.0308e-03, -1.2735e-03,  1.0970e-03, -5.0142e-03,  1.1995e-03,\n",
       "          1.8985e-03,  8.4996e-05,  4.0164e-03, -6.7011e-03,  1.1027e-04,\n",
       "          2.6665e-03,  2.6484e-03, -1.9372e-05,  5.2011e-04,  1.7081e-03,\n",
       "          1.1805e-03, -5.5653e-04,  8.6284e-04,  8.7172e-04,  1.7473e-03,\n",
       "          2.9418e-03,  1.4077e-03, -3.0468e-03, -2.7114e-03, -9.7382e-04,\n",
       "          9.6452e-04, -8.3005e-04,  2.8169e-04,  5.0700e-04,  3.3116e-03,\n",
       "          7.6997e-04, -2.3395e-04, -6.3026e-04,  2.3085e-03,  1.4263e-03,\n",
       "         -3.0599e-03,  8.6260e-04, -1.2069e-03, -5.4950e-04,  1.8436e-03,\n",
       "          7.0077e-04,  1.0903e-03, -2.6369e-04, -9.3096e-04,  4.5037e-04,\n",
       "          8.7571e-04,  3.2848e-03,  9.6643e-04, -2.9099e-04,  7.5704e-04,\n",
       "         -2.0504e-04,  2.8738e-03, -1.6335e-03, -2.3484e-03, -2.0768e-03,\n",
       "         -1.1964e-03,  1.8835e-05,  4.9925e-04,  1.5384e-03,  2.8068e-04,\n",
       "         -5.0938e-04, -2.0753e-03, -1.3850e-03,  1.5624e-03,  9.1094e-04,\n",
       "         -1.3525e-03, -3.9154e-04,  1.2653e-03,  1.8580e-03,  2.5604e-03,\n",
       "          9.9766e-04,  1.2507e-03, -2.1583e-04,  1.0504e-03,  1.0724e-03,\n",
       "         -1.4700e-03, -2.5368e-04,  2.2167e-03, -7.0494e-04,  1.6795e-03,\n",
       "          5.5683e-04,  1.4651e-04,  9.6142e-04, -1.0145e-04,  1.9789e-03,\n",
       "          1.0600e-03,  3.3262e-03, -4.6813e-04,  1.6018e-03, -1.7259e-03,\n",
       "         -1.3137e-03, -8.1474e-04,  2.7766e-03,  2.8775e-03, -3.7387e-03,\n",
       "          1.0967e-03, -5.0711e-03,  1.2202e-03, -2.5215e-03, -3.2204e-04,\n",
       "          1.7460e-03,  3.1118e-03,  6.6292e-04, -4.3774e-04,  2.0735e-03,\n",
       "          6.8796e-04,  1.9987e-03,  2.7370e-04, -2.0742e-04,  3.9002e-03,\n",
       "          1.1611e-03,  8.1801e-04,  1.2259e-03,  1.9161e-03,  1.8358e-03,\n",
       "          1.5385e-03, -8.6468e-04,  6.9165e-04,  1.0703e-03,  1.8167e-03,\n",
       "          1.7788e-03,  1.1680e-03,  2.1580e-03, -1.1609e-03, -2.1476e-04,\n",
       "          8.4841e-04,  5.9736e-04,  2.1533e-03,  3.1609e-03,  2.4732e-03,\n",
       "          2.4374e-03,  1.8786e-03, -6.0165e-04,  2.1349e-03,  2.0698e-03,\n",
       "          1.3700e-03,  1.2585e-03,  7.6348e-04, -3.9953e-03,  1.6673e-03,\n",
       "          1.9850e-03,  1.9010e-03, -6.8617e-04,  7.0858e-04,  1.7562e-03,\n",
       "          2.5961e-03, -4.1562e-03, -3.4729e-03, -2.9970e-03,  8.3613e-04,\n",
       "         -1.8311e-04,  6.6441e-04,  6.8188e-04,  2.2500e-03, -2.1762e-04,\n",
       "          3.2582e-03,  2.4999e-03,  2.1445e-03,  1.6513e-03, -1.1797e-03,\n",
       "          8.1962e-04,  8.2529e-04,  2.5314e-03,  4.9919e-03,  1.2193e-03,\n",
       "          1.8251e-04,  3.3166e-03,  1.1897e-03,  3.1118e-03,  1.1444e-03,\n",
       "          1.1587e-03,  2.6261e-03,  2.4840e-03, -2.3782e-04,  3.1110e-03,\n",
       "          1.4391e-03,  2.7144e-04,  3.1488e-03, -2.7831e-03, -3.3111e-03,\n",
       "          1.7005e-03, -8.3899e-04,  2.3037e-04,  1.4544e-03, -6.3163e-04,\n",
       "          2.9972e-03, -2.0121e-03,  8.0097e-04,  1.1687e-03,  2.0522e-04,\n",
       "         -3.1229e-03,  2.0937e-03,  2.1324e-03,  1.0631e-03,  2.5645e-03,\n",
       "          3.9506e-04, -3.7334e-03,  2.4598e-03, -2.7493e-03, -3.1376e-04,\n",
       "         -3.3087e-04, -6.2430e-04,  2.2671e-03,  4.7088e-04,  3.3367e-04,\n",
       "         -1.0049e-04, -1.9547e-03,  8.4758e-05,  8.8012e-04, -1.9256e-03,\n",
       "          1.9190e-03, -2.6262e-03,  2.6422e-03,  1.9789e-03,  3.3140e-04,\n",
       "          9.7847e-04,  9.1481e-04,  1.4388e-03,  3.5136e-03, -7.2956e-04,\n",
       "          3.5542e-03,  3.2198e-04,  5.3322e-04,  1.4600e-03,  9.2810e-04,\n",
       "         -1.4474e-03,  2.3429e-03,  1.3952e-03,  4.2534e-03, -2.2875e-03,\n",
       "          2.6548e-04,  3.8339e-03,  1.2076e-03,  6.1572e-05,  2.1569e-03,\n",
       "          1.5372e-03, -1.4991e-03,  2.0720e-03, -8.8215e-05,  2.7100e-03,\n",
       "          9.8765e-04,  2.2448e-03, -1.6421e-04,  7.9948e-04,  3.6966e-03,\n",
       "         -1.9884e-03, -1.8722e-03,  2.3246e-05,  1.0427e-03,  4.4418e-03,\n",
       "         -4.9472e-06,  3.2160e-03, -6.4653e-04,  4.1193e-03,  1.3244e-03,\n",
       "          7.8416e-04,  3.1933e-03,  3.8296e-04,  2.2325e-03,  3.1838e-03,\n",
       "         -2.6341e-03,  3.1841e-04,  1.0507e-03,  8.7708e-04,  3.0448e-03,\n",
       "          1.4938e-03, -7.6938e-04,  7.2819e-04, -1.8167e-04, -7.2372e-04,\n",
       "          1.0303e-03,  2.2179e-03,  1.6417e-03,  2.9299e-03,  1.5134e-03,\n",
       "          1.4583e-03, -2.3417e-03,  2.2875e-03,  1.2464e-03,  1.6192e-03,\n",
       "          1.6369e-03,  3.3687e-03, -4.7189e-04,  6.2466e-04, -1.1011e-03,\n",
       "         -1.6290e-04,  2.2385e-03,  4.4098e-03, -2.8538e-03,  4.6343e-04,\n",
       "         -3.3312e-03, -4.9889e-05,  2.2590e-05,  2.6739e-04,  1.2467e-03,\n",
       "         -4.8459e-05,  3.1429e-03,  9.6679e-05,  7.8022e-04,  2.6923e-03,\n",
       "         -7.6318e-04,  1.6813e-03, -2.6506e-03, -7.7678e-03, -2.6790e-03,\n",
       "          2.7763e-03,  1.2794e-03,  2.7306e-03,  1.4679e-03, -9.9665e-04,\n",
       "          1.7290e-03,  5.7697e-04,  3.9260e-03,  2.3847e-03, -2.8920e-04,\n",
       "          9.3865e-04,  1.0864e-03, -6.7073e-04, -3.7034e-03,  3.1450e-03,\n",
       "          2.9057e-03,  3.6955e-06,  2.8961e-03,  1.8817e-04,  7.5871e-04,\n",
       "         -3.7758e-03,  2.0482e-03, -7.9954e-04,  1.3804e-03,  1.2400e-03,\n",
       "          1.3583e-03, -9.9182e-04,  1.8274e-03, -1.7828e-03,  1.7129e-03,\n",
       "          2.6008e-03,  3.6462e-03,  2.0170e-04,  2.5082e-03,  1.3195e-03,\n",
       "          1.4032e-03,  1.5494e-03,  2.5467e-03,  3.7932e-04,  5.3084e-04,\n",
       "          1.4033e-03,  5.5778e-04,  4.2538e-03,  2.5570e-03, -1.7417e-03,\n",
       "          3.5912e-03, -3.8922e-04, -3.8981e-05,  5.3442e-04,  2.4135e-03,\n",
       "          2.2853e-03,  1.8032e-03,  6.6090e-04,  1.4143e-03,  4.0841e-03,\n",
       "          1.8269e-03,  4.2790e-03,  1.3072e-03,  5.3328e-04, -7.7307e-04,\n",
       "         -1.6422e-03,  2.0683e-05]),\n",
       " tensor([-1.3942e-04,  3.6637e-04, -5.6313e-04,  4.7947e-04, -3.0464e-04,\n",
       "         -1.0043e-04, -2.5722e-04,  3.0844e-04,  9.0607e-05, -1.5129e-04,\n",
       "          2.1705e-04, -3.0100e-04,  4.8919e-04,  1.3126e-03,  3.1461e-04,\n",
       "          9.4739e-05,  1.1184e-03,  8.7932e-05, -1.1507e-03, -7.2681e-04,\n",
       "         -5.2974e-04, -2.0982e-04,  5.6517e-04,  5.7541e-04,  1.4071e-04,\n",
       "         -2.7930e-04,  6.6488e-04,  4.8675e-05,  2.9232e-04, -5.2118e-04,\n",
       "         -1.5173e-03, -3.0598e-04,  9.8319e-04, -1.3240e-04, -9.3281e-05,\n",
       "         -6.0054e-04,  1.6283e-04,  4.9106e-04, -3.7579e-04,  1.8601e-04,\n",
       "          4.7139e-04, -3.0729e-04, -3.7820e-04, -2.4548e-04, -7.2901e-04,\n",
       "         -8.5624e-05,  2.5219e-04, -7.7664e-04,  1.0676e-03,  4.2530e-04,\n",
       "         -1.1506e-03,  7.1429e-04,  7.4679e-05, -3.8791e-04, -7.1998e-04,\n",
       "          1.2356e-04, -2.3969e-05,  4.3341e-04,  7.4920e-04,  7.5558e-04,\n",
       "          4.6354e-05,  3.0689e-05,  1.2738e-04, -2.2236e-04, -2.8649e-04,\n",
       "         -5.7106e-04,  1.0171e-03, -4.9068e-04, -1.5019e-04,  4.0265e-04,\n",
       "         -2.4263e-04,  5.1055e-04, -6.0661e-05,  3.0281e-04, -7.0713e-05,\n",
       "          4.2354e-04, -4.7037e-04, -5.9105e-04,  5.8994e-04, -2.6154e-04,\n",
       "          2.6530e-04, -7.3751e-04,  2.8878e-04, -4.7185e-04, -5.3110e-04,\n",
       "          1.1419e-03, -1.8866e-04, -3.0041e-04, -8.2343e-04, -6.4351e-05,\n",
       "         -1.2789e-04,  4.3512e-04, -9.1540e-04,  2.8272e-04, -3.2564e-04,\n",
       "          2.4282e-04,  8.0434e-04,  3.9074e-04,  6.8469e-04,  4.1952e-04,\n",
       "          5.5679e-06, -6.5237e-05,  8.0116e-04,  3.9790e-05,  5.1658e-04,\n",
       "         -9.0532e-05, -1.2480e-05,  8.6737e-04,  1.1146e-03, -4.1848e-04,\n",
       "         -1.8612e-05,  7.3381e-05, -6.1801e-04, -1.6825e-04,  2.1198e-05,\n",
       "          1.1445e-03, -1.1624e-03, -2.6368e-04,  1.1038e-04, -6.0156e-04,\n",
       "         -4.0215e-04, -8.9776e-05, -4.3758e-04,  9.2074e-04,  6.9401e-04,\n",
       "         -4.3470e-04, -1.7449e-04,  9.7004e-05,  4.6382e-04, -7.1373e-05,\n",
       "         -3.8015e-04, -2.8481e-04, -1.8990e-04,  1.2286e-04,  2.5877e-04,\n",
       "          5.0904e-04, -4.5746e-04,  1.6870e-05, -3.2013e-04,  5.2497e-04,\n",
       "          9.8884e-05,  2.4836e-04, -4.9834e-05,  1.5848e-03, -1.8521e-04,\n",
       "         -2.3836e-04, -2.3003e-04,  8.0282e-04,  3.0304e-04, -6.7961e-04,\n",
       "          1.1482e-04, -7.3068e-05,  8.0170e-04,  1.8009e-04,  4.5952e-04,\n",
       "          5.6101e-04, -6.8599e-05,  3.7775e-05,  4.2105e-04, -2.1512e-04,\n",
       "          7.2396e-04, -4.5203e-05, -6.7263e-04,  4.5881e-05,  9.1034e-04,\n",
       "         -1.9846e-04, -3.3760e-04,  5.3041e-04, -5.5708e-04,  4.4621e-04,\n",
       "         -9.9014e-05,  2.5004e-04,  3.1911e-04, -7.4583e-04, -4.8512e-04,\n",
       "          6.5591e-06,  4.7766e-04, -1.1044e-03,  5.0941e-05,  1.4144e-04,\n",
       "         -2.6951e-04,  2.7233e-04, -1.6185e-04, -4.3259e-04,  5.6621e-05,\n",
       "         -1.1986e-04, -6.2978e-04, -1.3237e-04, -1.1487e-04, -2.2792e-05,\n",
       "         -2.6959e-04,  5.9219e-04, -8.5329e-04,  5.3523e-04,  5.5778e-04,\n",
       "         -2.2659e-04,  1.1648e-04, -2.3637e-06,  5.2270e-05, -6.8157e-04,\n",
       "          6.0777e-04, -3.4101e-05, -2.0106e-04, -2.5377e-04, -1.4341e-03,\n",
       "         -5.1043e-04, -5.6715e-04,  3.3943e-04, -1.7442e-03,  6.9856e-04,\n",
       "         -4.6010e-04, -1.1579e-03, -7.3572e-04, -1.6967e-04,  1.2156e-04,\n",
       "         -1.0843e-03,  3.4271e-04, -2.7306e-04,  6.8015e-04, -5.3137e-04,\n",
       "         -1.8005e-04, -5.1333e-04, -4.6314e-04, -3.3118e-06,  5.1852e-05,\n",
       "          3.2423e-04,  7.6243e-04, -9.6127e-05,  2.0670e-04,  5.7100e-04,\n",
       "          1.6273e-04,  6.6219e-04,  5.8202e-04,  6.4516e-04,  7.0447e-04,\n",
       "          8.8124e-05, -9.1835e-04,  1.9841e-05, -1.7445e-03, -3.5492e-04,\n",
       "         -5.6443e-04,  9.5675e-05,  8.8498e-05, -5.8701e-05, -1.6975e-04,\n",
       "          1.2545e-03,  1.2086e-04,  7.1474e-04,  9.8127e-04,  5.7710e-04,\n",
       "          8.3093e-04,  1.8260e-05, -9.8287e-04, -4.9265e-04, -1.5508e-04,\n",
       "          3.2589e-05,  1.0278e-03, -5.1488e-04,  3.9601e-04,  4.2600e-04,\n",
       "         -2.9466e-04, -9.4401e-04,  8.8879e-05, -2.3224e-04,  3.9515e-04,\n",
       "          3.3652e-04, -6.7066e-04,  5.4155e-05,  3.7303e-04, -1.2439e-03,\n",
       "         -3.5763e-04, -4.1632e-04, -2.8419e-04, -3.3084e-04,  3.4546e-04,\n",
       "          1.2668e-04,  6.1904e-04, -1.0995e-03,  4.0351e-04, -1.0369e-05,\n",
       "          7.0456e-04, -2.4243e-04,  4.3638e-04, -3.5080e-04, -1.1148e-03,\n",
       "          8.3320e-04, -6.8215e-04,  6.1695e-04, -7.6930e-04,  8.0615e-05,\n",
       "         -2.9460e-05,  4.9324e-04,  1.4661e-03, -8.4212e-04, -2.0016e-04,\n",
       "         -2.0435e-04, -2.7520e-04, -8.8956e-04, -4.3353e-04,  2.0149e-04,\n",
       "          6.2568e-04,  5.6176e-04,  2.1391e-04,  1.0749e-03, -1.3936e-04,\n",
       "         -2.5148e-04, -4.5863e-04,  1.8912e-03, -5.0567e-04, -2.2561e-04,\n",
       "          1.0782e-03,  3.8729e-04,  3.3423e-04,  3.3789e-04,  6.1169e-04,\n",
       "          6.0172e-04, -8.8312e-05,  4.5391e-04,  1.1858e-03, -4.9734e-04,\n",
       "          1.3431e-04,  5.0727e-04, -4.9265e-04, -3.7909e-04,  8.3422e-04,\n",
       "          4.6525e-04, -3.8844e-04,  2.7347e-04,  3.6811e-04,  1.0390e-03,\n",
       "          5.8108e-04, -3.0386e-04, -7.8205e-04, -1.0804e-03,  6.4965e-04,\n",
       "         -5.2892e-05,  2.2087e-04, -5.6596e-04,  2.7283e-04, -6.9305e-04,\n",
       "          2.9603e-04,  9.5444e-04, -3.2891e-04,  1.1627e-03, -2.7403e-05,\n",
       "          7.7294e-04, -7.1530e-04, -7.2256e-05, -9.0577e-04, -9.5643e-04,\n",
       "          9.1707e-04,  2.7405e-04, -3.3617e-04,  2.4387e-04, -1.6819e-04,\n",
       "          8.2523e-04, -3.0106e-05, -1.4456e-04, -6.3334e-04, -2.6211e-05,\n",
       "         -8.0328e-05,  1.8421e-04,  7.7311e-04,  1.4673e-03,  5.5398e-04,\n",
       "         -9.7866e-04, -4.1169e-04,  3.2545e-04,  4.2189e-04, -2.7683e-04,\n",
       "         -2.5212e-04, -9.0892e-04, -1.6569e-04,  2.0505e-04,  2.7204e-04,\n",
       "          7.3780e-04,  5.2842e-04, -1.1041e-04, -4.7921e-04, -2.5462e-04,\n",
       "         -2.3186e-05, -1.1161e-03, -3.2327e-04,  7.6815e-05,  5.8290e-04,\n",
       "          3.5138e-04,  2.0443e-04, -1.8091e-04,  3.4440e-04,  2.2419e-05,\n",
       "          5.5005e-04,  3.8771e-04, -2.7733e-04, -1.1180e-04, -1.2385e-03,\n",
       "         -3.7372e-05,  2.3848e-04,  5.9273e-04,  5.9935e-04,  1.5034e-04,\n",
       "          5.6922e-04,  2.2099e-04, -4.8044e-04,  1.1361e-04, -2.4505e-04,\n",
       "          5.9765e-04, -7.8971e-04, -7.1282e-04, -5.1631e-04,  5.5404e-04,\n",
       "         -9.7258e-04, -1.0132e-04,  4.4210e-04, -6.2227e-04, -8.1176e-04,\n",
       "         -7.0312e-04,  1.6569e-04, -1.5121e-04, -2.8080e-04, -2.0998e-04,\n",
       "          7.1733e-04,  4.5923e-04, -8.7715e-04,  9.8709e-05,  1.0892e-03,\n",
       "          6.3996e-04,  5.1902e-04,  7.2118e-05,  9.0092e-05, -4.8822e-04,\n",
       "          5.0684e-04,  2.1175e-04,  1.2036e-03, -8.3194e-04,  7.7942e-04,\n",
       "          6.9684e-04,  4.7383e-04, -2.6049e-04,  4.8291e-04, -7.3524e-05,\n",
       "          8.4357e-05, -3.0082e-05,  1.3344e-04, -4.4850e-04,  1.2023e-03,\n",
       "         -3.4460e-04,  2.6917e-04, -4.6735e-04,  2.7804e-05, -9.3619e-05,\n",
       "         -2.7525e-03, -1.9717e-04, -2.9027e-04,  7.5995e-04, -3.2775e-04,\n",
       "         -1.6989e-04, -2.1991e-05,  4.0259e-05, -6.9058e-04,  7.4223e-05,\n",
       "          4.7771e-04,  6.1912e-04, -4.4109e-04, -4.9615e-04, -6.9663e-04,\n",
       "         -3.9486e-04, -6.8614e-04, -4.9020e-04,  2.2265e-04,  5.0977e-05,\n",
       "          7.4873e-04, -4.4095e-04, -9.4583e-05, -2.8663e-04, -3.7578e-04,\n",
       "         -7.5400e-05,  2.8720e-04,  3.4637e-04,  1.2154e-03,  8.5918e-04,\n",
       "         -1.8826e-04,  1.0195e-03, -3.1386e-06, -5.8879e-04,  2.0803e-04,\n",
       "          3.3738e-04, -9.2287e-04,  1.4818e-03,  6.0868e-04, -2.0069e-04,\n",
       "         -1.6400e-04,  4.6970e-04,  7.4198e-04,  3.4814e-04,  8.1378e-04,\n",
       "          4.6550e-04,  7.2940e-04,  5.9398e-05, -5.7140e-04, -3.8153e-04,\n",
       "          2.4743e-04, -3.4630e-05, -5.2829e-04, -1.2911e-04, -2.1377e-04,\n",
       "          6.5849e-04, -9.2395e-05,  5.1349e-04,  3.1069e-06, -2.8718e-04,\n",
       "         -1.9307e-04, -4.8965e-05]),\n",
       " tensor([[ 6.7717e-04,  3.2087e-03, -2.1312e-03,  ..., -4.3817e-04,\n",
       "           1.3946e-03, -2.1276e-03],\n",
       "         [ 1.2257e-03,  1.6102e-03,  1.4836e-03,  ...,  7.8254e-04,\n",
       "          -3.0173e-03, -2.2069e-03],\n",
       "         [ 1.0253e-03, -1.2596e-03,  9.0357e-04,  ..., -1.9613e-04,\n",
       "           2.1845e-03, -2.2458e-03],\n",
       "         ...,\n",
       "         [-2.4788e-04, -2.4793e-03, -4.3755e-04,  ..., -7.7713e-05,\n",
       "           1.3841e-03, -3.3619e-04],\n",
       "         [-8.8844e-04,  7.9381e-04, -1.4148e-03,  ..., -2.5607e-03,\n",
       "           3.1384e-04, -2.0793e-04],\n",
       "         [ 6.4166e-04, -5.4269e-04, -8.5557e-04,  ...,  2.1460e-04,\n",
       "          -1.2219e-03,  1.3956e-03]]),\n",
       " tensor([ 1.3524e-03, -1.6064e-04,  6.8482e-05,  ...,  1.8734e-03,\n",
       "          1.6742e-03, -6.9587e-04]),\n",
       " tensor([[-1.0523e-03,  1.7885e-05, -1.5987e-03,  ...,  1.2231e-04,\n",
       "          -1.9179e-03, -1.9203e-03],\n",
       "         [ 2.6331e-03, -3.3285e-03, -1.0303e-03,  ...,  1.9464e-03,\n",
       "          -1.5603e-04, -3.2365e-04],\n",
       "         [ 3.2469e-03,  1.5602e-04,  2.3413e-04,  ...,  2.4191e-03,\n",
       "           1.0684e-03,  2.1022e-03],\n",
       "         ...,\n",
       "         [ 5.3319e-03,  1.5245e-03,  1.5538e-03,  ...,  1.1268e-03,\n",
       "          -3.8623e-03,  1.3642e-03],\n",
       "         [-9.0074e-04,  1.5491e-03,  4.3860e-04,  ..., -2.8619e-03,\n",
       "          -3.2744e-04,  1.4786e-03],\n",
       "         [-1.7456e-03, -1.4690e-03,  3.3301e-03,  ..., -1.1209e-03,\n",
       "           1.5939e-03,  4.6876e-04]]),\n",
       " tensor([ 2.7985e-05, -1.8374e-05, -2.1846e-05, -5.4237e-05,  5.3330e-05,\n",
       "          2.1864e-04, -3.0416e-05, -7.0594e-06,  7.2409e-05, -1.8512e-04,\n",
       "         -6.7960e-05,  1.4107e-04,  7.7680e-05, -3.8390e-04,  1.7917e-05,\n",
       "          8.8033e-05,  2.5504e-04, -2.4993e-05,  1.5877e-05,  4.6328e-04,\n",
       "         -1.9367e-04,  3.4820e-04, -4.7518e-05,  3.8412e-05,  8.0233e-05,\n",
       "          2.7820e-04,  8.8604e-05,  2.1835e-04,  1.4547e-04,  1.6092e-05,\n",
       "          8.0690e-05, -2.6404e-05, -4.0330e-05, -3.7506e-04, -1.5759e-04,\n",
       "          9.8802e-05, -2.0177e-04,  1.1473e-04,  1.7111e-04, -9.1037e-05,\n",
       "         -5.9556e-05,  7.2005e-05, -1.3338e-04,  5.1945e-05,  4.7932e-05,\n",
       "         -4.7837e-05, -9.1273e-05,  9.5615e-05,  2.6458e-04,  1.6516e-04,\n",
       "         -8.0779e-05, -2.6058e-04,  6.5693e-05, -8.1315e-05,  1.1983e-04,\n",
       "         -3.7670e-04,  2.0390e-04, -3.4473e-04,  1.6585e-05, -6.8584e-05,\n",
       "         -5.4642e-05,  3.7191e-04, -4.1915e-04,  2.2815e-04,  5.5743e-05,\n",
       "         -2.4028e-05, -4.1550e-04, -1.8403e-04,  4.2087e-04, -1.4653e-04,\n",
       "          2.4541e-04, -1.8615e-04,  3.0619e-04,  1.1055e-04,  5.0850e-04,\n",
       "          1.1092e-05, -9.3015e-05,  3.3054e-04,  5.4798e-05,  7.5498e-05,\n",
       "          1.4697e-04, -6.6860e-06,  2.9175e-04,  8.0482e-05, -1.0708e-05,\n",
       "          4.0535e-04,  1.7702e-04,  1.8287e-04,  5.9241e-05, -4.1183e-06,\n",
       "          4.2774e-05, -3.4560e-04,  2.4236e-04, -2.3008e-04,  7.9212e-05,\n",
       "         -5.9340e-04, -1.0799e-04,  5.8813e-05, -1.1085e-04,  5.2270e-05,\n",
       "         -6.7023e-05,  3.5595e-06,  7.1549e-05,  5.6428e-05, -1.3169e-04,\n",
       "         -1.7248e-04,  6.4882e-05,  1.2443e-04, -1.8315e-04, -4.2881e-04,\n",
       "         -1.9192e-04, -3.4682e-04, -2.8580e-04, -3.6246e-04, -2.4614e-04,\n",
       "          2.3180e-04,  2.4843e-04,  1.4632e-04, -1.0010e-04, -1.9169e-04,\n",
       "          6.7472e-05, -1.9442e-04, -1.7432e-04, -4.6613e-04, -1.4530e-05,\n",
       "          2.6417e-04,  8.1791e-05,  1.6510e-05, -2.3246e-04,  1.6409e-04,\n",
       "         -7.8499e-05,  1.6850e-05, -9.5703e-05, -7.1323e-05, -2.7311e-04,\n",
       "          7.0583e-05,  2.5622e-04, -1.8915e-04, -3.2130e-04,  1.4925e-04,\n",
       "          1.2505e-04, -1.4590e-04,  1.2376e-04, -1.6038e-04,  2.5436e-04,\n",
       "         -3.7897e-04,  4.9118e-04, -2.5382e-04, -8.0174e-05,  1.6712e-04,\n",
       "         -2.3169e-05,  9.3680e-05, -1.1253e-04, -1.0019e-04,  5.5787e-04,\n",
       "         -6.0711e-05,  3.0123e-04,  4.2947e-07, -5.3888e-05,  1.8251e-04,\n",
       "          8.9854e-05, -2.6744e-04,  2.4767e-04, -3.4313e-04, -1.1791e-04,\n",
       "         -3.0157e-04,  2.7053e-04, -5.2020e-04,  1.8300e-04,  4.3850e-04,\n",
       "          1.2631e-04, -4.6634e-04,  2.2732e-04,  4.1217e-04,  1.9843e-04,\n",
       "         -9.1802e-05, -1.6468e-04,  6.6166e-04, -1.0791e-04,  3.8304e-05,\n",
       "          9.3638e-05, -1.6435e-05, -1.9570e-04,  3.6064e-04,  1.0667e-04,\n",
       "          1.5515e-04,  7.4732e-05,  8.7716e-05, -1.6546e-04, -5.8719e-05,\n",
       "          8.5030e-07, -1.0584e-04,  1.6579e-04, -2.5348e-04,  5.3430e-05,\n",
       "         -2.9164e-05,  4.8502e-04,  2.0012e-04,  2.1147e-04,  2.0006e-04,\n",
       "         -2.1395e-04, -1.7967e-04,  3.7493e-04,  1.9601e-04, -1.6386e-04,\n",
       "         -1.4258e-04,  6.5356e-05, -1.5113e-04, -2.2325e-04, -4.1452e-04,\n",
       "          4.2213e-04, -1.1690e-05, -3.9273e-04,  1.3947e-04, -2.6489e-05,\n",
       "          3.0550e-04, -4.0161e-04,  2.0723e-04, -5.2015e-05,  3.0121e-04,\n",
       "         -1.4002e-04, -3.4824e-04,  5.3380e-04,  6.9055e-04, -5.0867e-05,\n",
       "         -5.3300e-04, -2.6510e-05, -4.1083e-05,  7.1736e-05,  1.3495e-05,\n",
       "         -1.8497e-04,  2.1653e-04, -9.7691e-06,  1.5294e-04,  1.4001e-04,\n",
       "         -1.1442e-05, -9.0767e-05,  1.1802e-04, -5.3448e-04,  1.2818e-04,\n",
       "          1.8761e-04,  6.1944e-05,  1.5378e-04, -1.6432e-04, -2.6735e-04,\n",
       "         -1.0994e-04, -4.0486e-05, -1.2926e-04,  4.3186e-04, -2.9625e-05,\n",
       "          4.3605e-05, -2.0873e-04,  1.6180e-04,  1.6034e-04,  1.6522e-04,\n",
       "          2.2739e-04,  1.2892e-05, -4.9988e-05,  1.4929e-04, -2.8849e-04,\n",
       "         -2.3010e-04,  3.3710e-05, -6.2641e-06, -1.0958e-04,  1.1655e-04,\n",
       "         -2.5943e-04, -6.2170e-04,  1.2377e-04,  1.6020e-04,  1.0818e-05,\n",
       "          6.5359e-04, -1.2918e-04, -2.3295e-04, -1.2184e-04,  2.3035e-04,\n",
       "          3.3637e-05,  4.9947e-04,  2.1380e-04,  2.1970e-05,  5.9789e-05,\n",
       "         -2.1336e-04,  5.6874e-05,  1.5051e-04, -1.7462e-05,  3.3809e-04,\n",
       "          2.8311e-04, -1.8034e-04, -9.2845e-05,  6.9387e-05, -3.2201e-04,\n",
       "         -2.7870e-04, -8.4545e-06, -3.4452e-04,  3.8545e-04, -1.1905e-04,\n",
       "         -7.7358e-05,  4.2107e-04,  1.8096e-04,  2.4873e-04, -1.3049e-04,\n",
       "          3.9897e-05, -6.3343e-05,  3.2914e-04, -2.4822e-04, -3.0981e-04,\n",
       "          4.4018e-04, -2.7954e-04, -5.0675e-04,  4.3969e-04, -5.3311e-05,\n",
       "          7.2816e-04, -2.0771e-04, -4.2987e-04, -3.2693e-04,  4.0326e-05,\n",
       "         -4.9859e-04,  2.6845e-05,  1.5353e-04,  9.0465e-05,  8.9535e-05,\n",
       "          3.3788e-04, -1.4373e-04,  3.1725e-04,  3.5805e-05, -2.7750e-05,\n",
       "         -1.8006e-04, -2.3412e-05, -1.3288e-04,  1.1441e-04, -3.1414e-05,\n",
       "          5.8868e-05,  1.0394e-06, -1.0420e-04,  1.0621e-04,  3.2350e-04,\n",
       "         -1.3001e-04, -4.3216e-05,  3.8690e-04, -3.8611e-04,  1.5703e-04,\n",
       "         -1.8492e-04,  1.0302e-04,  2.0138e-04, -5.2763e-05, -1.9239e-04,\n",
       "          5.9087e-05, -7.4259e-05, -3.3546e-04,  1.2271e-04,  5.9003e-05,\n",
       "         -2.7365e-05, -2.0206e-04, -1.2970e-04, -2.2780e-04, -1.4665e-04,\n",
       "          2.0202e-04, -9.1635e-05,  1.6507e-04,  4.7320e-04, -1.5265e-04,\n",
       "         -1.0985e-04,  3.8922e-04, -1.5294e-04,  6.5355e-05,  6.8072e-05,\n",
       "          9.0477e-05, -9.6986e-05, -3.4729e-04, -2.4402e-04, -2.1891e-04,\n",
       "          5.6118e-04,  1.0710e-04, -2.4769e-04, -1.2916e-05,  1.8352e-04,\n",
       "          8.3583e-05, -3.2431e-04, -1.2611e-04, -4.7004e-05,  1.0111e-04,\n",
       "          1.3597e-04,  2.0142e-04,  1.4521e-04, -2.1148e-04, -3.6549e-04,\n",
       "         -5.5281e-05, -2.7523e-04,  1.1470e-04,  9.7979e-05, -3.7102e-04,\n",
       "          1.5176e-04, -2.1490e-04,  1.6901e-04, -4.8799e-05,  3.5872e-05,\n",
       "         -2.3001e-06,  5.7344e-05, -1.1251e-04,  1.9877e-04,  2.5088e-04,\n",
       "         -1.5227e-04,  1.4385e-05,  5.0659e-06,  3.8985e-05, -2.6275e-05,\n",
       "          7.7110e-05,  1.0763e-04, -1.0449e-04,  3.8973e-05, -1.9310e-04,\n",
       "          3.5968e-04, -1.4776e-05, -8.8990e-05,  1.9445e-05,  1.5811e-04,\n",
       "          2.7016e-04,  2.6643e-04, -2.3434e-04,  1.7514e-04,  6.4798e-05,\n",
       "          1.4681e-04, -4.2862e-04, -3.8454e-06, -2.3182e-04, -2.0601e-05,\n",
       "         -3.0972e-05, -1.3971e-04,  2.0367e-05,  2.0105e-05,  7.6255e-05,\n",
       "         -2.7200e-04,  4.5018e-04,  2.7677e-04,  1.3075e-04,  5.3822e-05,\n",
       "         -3.0477e-04,  1.4428e-04, -2.1249e-04, -1.2664e-04,  5.0334e-04,\n",
       "          1.0822e-04,  3.4154e-04,  2.2212e-04,  5.3497e-05, -5.1317e-04,\n",
       "          3.6358e-04, -8.6799e-06, -3.0309e-05, -1.6700e-05, -6.9065e-05,\n",
       "         -2.2638e-03, -9.3332e-05, -3.4320e-04, -2.5332e-04, -2.5217e-05,\n",
       "          3.0492e-04, -8.8708e-06, -2.2400e-04,  3.2619e-04,  7.5754e-05,\n",
       "          2.0702e-05,  1.6963e-04, -9.8914e-05,  1.0090e-04,  1.4910e-05,\n",
       "          7.1974e-05, -7.2604e-05,  1.7671e-05, -2.7525e-04,  6.5107e-05,\n",
       "          2.1033e-04, -9.1788e-05,  1.1352e-04,  2.8417e-04, -1.1920e-04,\n",
       "          8.3109e-05,  2.8655e-05,  5.2576e-04, -1.6568e-04,  1.9993e-05,\n",
       "          7.5952e-05, -1.3705e-04, -3.0069e-04, -1.5920e-04,  1.2685e-04,\n",
       "          6.6499e-06, -3.8704e-04,  8.8911e-05,  2.8097e-04,  3.2951e-04,\n",
       "         -4.7028e-04,  5.6845e-05,  2.9659e-04,  1.1717e-04, -1.8123e-04,\n",
       "          6.0719e-05,  7.7356e-06,  2.3624e-05, -6.7861e-05, -2.2420e-04,\n",
       "         -1.1216e-04,  1.6936e-04, -3.7902e-05, -2.1841e-04,  3.9771e-05,\n",
       "          5.9485e-04, -2.4999e-04, -5.8841e-05,  2.1264e-04, -3.8294e-04,\n",
       "         -3.2368e-04,  6.3006e-05]),\n",
       " tensor([-1.3049e-03, -7.2759e-04, -3.7754e-04, -3.3194e-03, -4.6057e-03,\n",
       "          1.1022e-03,  4.9525e-04,  3.4863e-04,  2.9534e-04,  4.7255e-04,\n",
       "         -8.3482e-04,  2.0593e-04, -1.5332e-03,  2.3246e-06, -6.6638e-05,\n",
       "          7.0858e-04, -2.0051e-03, -3.7249e-03, -2.1789e-03, -6.4570e-04,\n",
       "         -2.9430e-03,  5.3900e-04, -1.8632e-04,  5.1212e-04, -7.6890e-04,\n",
       "         -2.5886e-04, -5.2381e-04, -1.7619e-03,  1.5308e-03, -2.3830e-04,\n",
       "         -6.1464e-04, -7.9912e-04, -5.7579e-03, -2.0876e-03,  6.7681e-04,\n",
       "          5.7340e-04,  3.4416e-04, -8.8441e-04,  1.5275e-03, -1.6174e-03,\n",
       "         -7.4208e-04, -4.3666e-04, -6.1405e-04, -3.6685e-03, -1.7876e-03,\n",
       "         -1.1192e-03,  1.3165e-03,  7.8064e-04,  1.9312e-03, -1.9290e-03,\n",
       "         -9.3454e-04, -1.7993e-03, -2.6287e-03, -2.8696e-03,  1.7513e-03,\n",
       "         -9.3079e-04,  1.5090e-03, -7.3898e-04,  1.2875e-04, -1.1671e-03,\n",
       "         -1.7713e-03, -2.6191e-03, -6.3735e-04,  1.0003e-03, -3.2776e-03,\n",
       "         -2.5505e-04, -3.4768e-03, -2.0540e-03,  4.8012e-04, -1.9777e-03,\n",
       "         -5.4187e-04, -3.8844e-04,  8.4060e-04,  4.1902e-05, -3.8528e-04,\n",
       "         -4.0995e-03, -1.4186e-05,  1.0255e-03, -3.7128e-04, -5.9319e-04,\n",
       "         -2.0546e-04,  7.6064e-04,  1.0866e-04, -1.5224e-03,  3.6108e-04,\n",
       "          1.0103e-04,  1.6371e-03,  8.4144e-04, -9.7048e-04, -2.3513e-03,\n",
       "         -1.2441e-03, -1.7202e-03, -3.1888e-04, -2.1122e-03, -2.4574e-03,\n",
       "         -9.2804e-05, -4.6134e-05, -1.1037e-03, -3.7045e-03, -8.8352e-04,\n",
       "         -4.1829e-03, -1.1331e-04,  1.4693e-04, -1.0358e-03, -1.8761e-03,\n",
       "         -1.9113e-03, -5.9021e-04, -1.8322e-03,  6.1575e-04, -1.3375e-04,\n",
       "         -3.8087e-04, -5.3489e-04, -3.0932e-03, -9.2459e-04, -5.9426e-03,\n",
       "         -1.4155e-03, -1.7126e-03, -9.9173e-04, -4.4753e-03, -4.8065e-04,\n",
       "         -2.7360e-03, -1.7226e-04, -1.2252e-03, -7.2749e-03, -1.3715e-04,\n",
       "         -3.2224e-03, -2.2379e-03, -2.4889e-03, -2.4621e-03,  1.3729e-03,\n",
       "         -3.7960e-03, -8.1211e-04, -6.8545e-06, -1.2873e-03,  2.7484e-04,\n",
       "         -8.1980e-04, -1.9222e-04, -2.8694e-04,  1.8494e-03, -7.9823e-04,\n",
       "         -1.2593e-03, -3.3969e-04, -2.0284e-03, -1.1837e-04,  2.7978e-04,\n",
       "          6.9499e-04, -5.7105e-03,  1.1442e-03, -4.1503e-03, -1.0443e-04,\n",
       "          1.0270e-03,  6.2734e-04,  7.2145e-04, -2.5126e-03,  8.7595e-04,\n",
       "         -2.3687e-04, -7.2694e-04, -2.8973e-03, -4.3844e-03,  1.5526e-03,\n",
       "         -2.0512e-03, -2.7012e-03, -1.6282e-03, -2.6495e-03,  7.1108e-05,\n",
       "         -1.6001e-03,  2.5320e-03, -2.5805e-03,  2.7478e-05, -1.9267e-03,\n",
       "         -4.8485e-03, -2.5942e-03,  1.7091e-03, -2.3350e-03,  3.1227e-04,\n",
       "          2.9665e-03, -2.7031e-03, -9.2089e-05,  3.4881e-04, -4.8202e-04,\n",
       "         -5.6877e-03, -1.8574e-03,  4.7332e-04, -4.7988e-04,  1.4472e-04,\n",
       "         -9.9337e-04, -1.9118e-03, -2.4313e-03,  3.1704e-04, -4.5717e-05,\n",
       "         -2.3901e-05,  2.2787e-04, -2.5845e-04, -5.3072e-04, -1.8497e-03,\n",
       "         -2.9051e-04,  9.8246e-04, -2.7250e-03, -3.5905e-03, -3.5317e-03,\n",
       "         -4.1533e-04, -1.8413e-03, -2.9941e-03,  5.4008e-04, -1.6785e-03,\n",
       "         -2.2113e-03, -1.5962e-03, -7.5561e-04, -1.5071e-03, -2.2929e-03,\n",
       "         -3.0417e-03, -1.9417e-03, -9.4420e-04,  3.5876e-04,  1.4127e-03,\n",
       "         -1.9997e-03, -2.3957e-03, -9.7436e-04, -9.4259e-04,  2.9582e-04,\n",
       "         -1.3855e-03, -2.0719e-03, -2.2104e-03, -3.1663e-03, -1.6506e-03,\n",
       "          1.1533e-04, -1.0415e-03, -9.0909e-04, -1.3527e-03, -6.4397e-04,\n",
       "          6.5821e-04,  4.6927e-04, -1.8843e-03, -1.5069e-03, -3.5719e-03,\n",
       "          5.4806e-04, -1.8001e-04,  1.5101e-03, -8.0299e-04, -3.4673e-03,\n",
       "          5.4753e-04, -2.7397e-03, -5.9298e-04, -2.0730e-03, -1.9827e-03,\n",
       "         -4.3732e-04,  7.0989e-05, -8.5115e-04,  1.9228e-03,  1.2080e-03,\n",
       "         -1.5174e-03, -2.5046e-04, -1.7315e-03, -1.0626e-03, -8.9496e-04,\n",
       "         -6.7669e-04, -1.7894e-03, -7.9310e-04,  4.7654e-04, -5.7238e-04,\n",
       "         -2.6226e-04, -2.4902e-03, -1.1654e-03, -1.8488e-03,  9.4980e-04,\n",
       "         -7.7361e-04,  7.4482e-04,  1.1784e-03, -3.0270e-03, -9.0057e-04,\n",
       "          1.9443e-04, -1.0068e-03, -2.1458e-06,  6.1387e-04, -1.1160e-03,\n",
       "          3.2586e-04,  2.0927e-04, -2.5645e-03,  2.7561e-04,  2.2019e-03,\n",
       "          7.2044e-04, -3.3876e-03, -1.8534e-03, -8.1736e-04,  4.4650e-04,\n",
       "         -3.3925e-03, -2.5159e-03, -7.8779e-04, -2.7938e-03,  1.9053e-03,\n",
       "          2.3890e-04, -4.2709e-03, -3.3059e-03, -1.1927e-03,  7.5817e-04,\n",
       "         -1.0230e-03, -2.1261e-04, -4.5294e-04, -1.1115e-03, -4.0537e-04,\n",
       "          1.5935e-03, -2.3268e-03, -7.3940e-04,  4.1872e-04, -2.2908e-03,\n",
       "         -1.6792e-03, -4.1854e-04, -3.1050e-03, -6.9684e-04, -1.5538e-03,\n",
       "          1.3588e-03,  2.3249e-03, -6.0797e-05,  1.2511e-04, -9.8825e-05,\n",
       "         -1.3756e-03, -3.0860e-03,  9.8050e-04, -2.6971e-04, -2.3121e-04,\n",
       "         -1.8764e-03,  7.7426e-04,  1.6456e-03, -3.0196e-04, -3.8821e-03,\n",
       "         -2.1031e-03, -5.1630e-03, -5.0521e-04,  4.0853e-04, -1.2400e-03,\n",
       "          1.0346e-03, -2.2370e-03, -6.2841e-04, -5.8353e-05, -1.8342e-03,\n",
       "         -3.0490e-03, -7.4863e-04, -1.1995e-03, -1.7571e-03, -1.0558e-03,\n",
       "         -7.8619e-05, -1.8713e-03,  9.8294e-04, -3.1125e-03, -1.1883e-03,\n",
       "          3.6901e-04, -2.2371e-03, -1.8547e-03, -1.9864e-03, -1.1356e-03,\n",
       "         -1.8357e-03, -1.6403e-03, -8.0645e-04, -1.8290e-03, -3.7920e-03,\n",
       "         -8.2481e-04, -1.4647e-03, -3.0607e-04, -3.4610e-03, -3.4865e-03,\n",
       "         -1.8597e-03,  2.1677e-03, -1.3461e-03,  1.6366e-03, -1.8674e-03,\n",
       "          9.6309e-04, -8.7821e-04,  1.1243e-03, -3.1726e-03, -6.4647e-04,\n",
       "         -1.7252e-03, -1.5423e-03, -6.3384e-04,  1.6420e-03, -3.0175e-03,\n",
       "         -7.9846e-04,  1.0130e-03,  4.8655e-04, -1.8781e-04, -1.2640e-03,\n",
       "          1.0514e-04, -2.2331e-03, -1.0362e-03,  3.5405e-04,  6.9022e-04,\n",
       "         -6.9028e-04, -1.2460e-04, -3.6467e-03, -1.1175e-03, -7.4613e-04,\n",
       "         -1.8485e-03, -2.9215e-03, -3.1909e-03, -4.9239e-04, -4.3041e-04,\n",
       "         -2.3462e-03, -2.6996e-03,  1.3968e-03, -8.0460e-04,  9.4557e-04,\n",
       "         -4.4954e-04, -1.8719e-03, -1.4378e-03, -1.3000e-03,  5.8311e-04,\n",
       "         -3.4770e-03, -3.2949e-04,  1.6199e-03, -1.2033e-03, -5.6434e-04,\n",
       "          5.9974e-04, -8.1545e-04,  7.7069e-05, -3.1235e-03, -4.4379e-03,\n",
       "         -7.2038e-04,  2.4939e-04, -3.9523e-03,  5.0086e-04, -3.0027e-03,\n",
       "          5.9408e-04, -4.3992e-03,  5.4759e-04,  5.7426e-04,  6.0189e-04,\n",
       "         -9.2906e-04, -4.5431e-04,  1.7624e-03, -3.6812e-04,  5.0724e-04,\n",
       "         -2.2744e-03, -3.3695e-04,  1.1367e-03, -3.6974e-03, -4.3277e-03,\n",
       "         -5.3612e-03, -3.8683e-03, -2.6117e-03,  1.1336e-03, -8.7202e-05,\n",
       "         -3.5975e-03, -6.1339e-04, -4.5947e-03,  2.1732e-04, -2.5034e-06,\n",
       "         -1.6674e-03, -9.7555e-04, -1.9780e-03, -2.2429e-03, -3.1604e-03,\n",
       "          3.8657e-04, -1.5038e-03, -1.3431e-03,  1.5608e-03,  6.2758e-04,\n",
       "          3.9244e-04, -4.9314e-03,  2.1935e-03, -5.8365e-04, -7.8064e-04,\n",
       "         -7.3838e-04,  9.4831e-05, -9.7764e-04, -5.6225e-03,  8.3101e-04,\n",
       "          4.8572e-04, -1.8698e-04,  1.0874e-03, -9.7162e-04, -1.8184e-03,\n",
       "         -3.2870e-03, -2.3102e-03, -5.6666e-04,  5.2047e-04, -9.0718e-05,\n",
       "         -1.2050e-03, -3.0760e-03, -9.0456e-04, -3.3542e-03,  8.9413e-04,\n",
       "          1.8021e-03, -4.9639e-04, -3.7110e-03, -6.0081e-04, -1.3407e-03,\n",
       "         -1.4520e-03,  3.1877e-04,  9.6381e-04, -3.2651e-03, -9.3651e-04,\n",
       "          1.8236e-03, -3.3095e-03,  5.2708e-04,  5.3626e-04, -3.0603e-03,\n",
       "         -4.6718e-04, -1.0976e-03, -6.1077e-04, -2.1183e-04, -5.6994e-04,\n",
       "         -3.3677e-05,  3.3110e-04, -3.1796e-03, -1.0502e-03,  1.8506e-03,\n",
       "          1.6397e-04, -4.2522e-04, -2.0576e-04, -8.2612e-04, -1.8903e-03,\n",
       "         -1.6937e-03, -9.9474e-04]),\n",
       " tensor([ 3.1359e-05,  6.7692e-05,  3.0191e-04,  1.2992e-04,  3.0200e-04,\n",
       "         -1.0031e-04, -2.1336e-04,  2.7509e-05,  1.0918e-04,  2.8609e-04,\n",
       "          2.0448e-05, -1.6513e-04, -7.6579e-04, -2.4343e-04, -6.9232e-04,\n",
       "          1.0621e-04,  2.7886e-04, -1.2630e-04,  4.3323e-04,  2.3332e-04,\n",
       "          3.3239e-04, -4.7387e-04,  1.6120e-04, -9.1292e-05,  3.7184e-04,\n",
       "         -8.5086e-06, -9.7349e-05, -1.0933e-04, -4.4924e-04,  1.4599e-05,\n",
       "          7.3257e-04, -1.4520e-04, -4.4804e-04,  2.6086e-04, -3.1073e-05,\n",
       "          3.3402e-04, -2.4636e-04, -1.6655e-04,  7.5049e-05,  1.1582e-04,\n",
       "         -3.1288e-04, -3.2878e-05,  3.4752e-04,  1.8362e-04,  2.0861e-04,\n",
       "         -5.1342e-04, -3.0127e-04,  3.6651e-04, -9.4131e-04,  1.6120e-04,\n",
       "          5.5988e-04, -1.0272e-04,  1.3456e-04,  6.7987e-06,  1.5851e-04,\n",
       "          2.5039e-05, -9.1098e-05, -1.3198e-04,  3.4198e-04,  3.2343e-04,\n",
       "         -5.4887e-05,  1.8515e-06,  1.7036e-04,  3.1240e-05,  1.4483e-04,\n",
       "         -4.4294e-05, -2.5365e-04,  2.2528e-04,  5.3139e-05,  2.6745e-04,\n",
       "          1.1279e-04,  3.8043e-05, -3.1638e-04,  7.6368e-06,  3.0714e-04,\n",
       "          7.4786e-05,  1.7178e-04,  3.8583e-05, -6.2504e-04,  4.3724e-05,\n",
       "         -1.9301e-04, -5.1530e-05,  3.5602e-04,  4.2943e-04, -2.0072e-05,\n",
       "         -2.1696e-04,  1.0036e-04,  2.2792e-04,  3.8920e-04,  2.1330e-04,\n",
       "          4.0500e-04, -2.3149e-04,  1.1048e-04,  6.7627e-05,  2.6498e-04,\n",
       "          1.6685e-04, -2.7132e-04, -6.5624e-04, -1.4529e-04, -6.5660e-04,\n",
       "         -4.9112e-05, -4.7471e-05, -1.8668e-04,  1.2546e-04,  3.5462e-04,\n",
       "          7.0202e-05,  2.5906e-04, -3.4620e-04, -8.3213e-04, -4.0311e-05,\n",
       "          1.4457e-04,  5.5393e-05, -1.9268e-04,  3.2306e-04, -6.0951e-05,\n",
       "         -2.7600e-04,  7.6175e-04,  1.2102e-04, -8.7991e-05,  3.3870e-04,\n",
       "         -6.8597e-05,  6.0976e-05,  4.8210e-04,  5.4740e-04, -2.8773e-04,\n",
       "          3.9357e-04,  2.8406e-04, -3.6010e-04, -6.7910e-05,  9.6630e-05,\n",
       "         -2.7683e-05,  1.7953e-04,  3.6565e-04, -7.8432e-05,  1.2643e-04,\n",
       "          1.4983e-05,  1.1766e-04,  1.3615e-04, -1.5627e-04,  3.7410e-04,\n",
       "         -8.7945e-05,  2.1688e-04, -2.4687e-05, -4.8050e-04,  4.7660e-04,\n",
       "         -4.5612e-05, -3.1063e-04, -1.8581e-04, -1.4473e-05, -3.2087e-04,\n",
       "          1.1702e-04,  2.6802e-04,  2.5941e-04,  2.1640e-04,  1.8178e-04,\n",
       "         -4.7742e-05,  1.2594e-04, -3.6514e-04, -3.2138e-04, -1.0838e-04,\n",
       "         -4.5260e-05, -1.5033e-04, -1.7821e-04,  1.5471e-05, -1.9797e-04,\n",
       "         -9.2886e-05, -4.2785e-04,  1.0091e-03, -2.7528e-04, -4.8377e-04,\n",
       "          4.4605e-05,  1.6980e-04, -4.3488e-04,  1.6104e-04, -8.7172e-07,\n",
       "          5.8770e-04, -3.7415e-04,  2.4079e-04,  1.3243e-04, -2.5824e-05,\n",
       "          7.3682e-05, -2.1417e-04,  3.1074e-04, -2.4208e-04, -1.4364e-04,\n",
       "         -4.6994e-04,  9.6520e-05, -1.1464e-04, -1.8686e-04, -3.8370e-04,\n",
       "          2.6352e-04,  3.5521e-05,  6.5145e-04, -6.0081e-05, -4.5784e-05,\n",
       "          4.6094e-04, -3.9579e-04, -3.6331e-05, -3.0512e-04,  5.0985e-04,\n",
       "         -1.3100e-04,  9.3289e-05,  7.3467e-05, -3.9149e-04, -7.2002e-05,\n",
       "          8.3403e-05,  3.4371e-05, -3.4463e-04,  1.2409e-03,  3.1255e-05,\n",
       "         -3.5017e-04,  5.7577e-04,  7.5057e-05,  1.9244e-04, -2.8126e-05,\n",
       "          9.8640e-04, -1.5216e-04,  1.0254e-04, -1.0291e-04,  1.7632e-04,\n",
       "         -1.3178e-04,  1.1039e-04,  5.5906e-04, -5.5885e-05,  1.1870e-04,\n",
       "          3.2293e-04,  3.4117e-04,  1.0876e-03,  2.0189e-04,  1.6515e-04,\n",
       "          4.0282e-04, -2.7838e-04, -3.7538e-04, -2.7161e-04, -1.7246e-04,\n",
       "          1.6463e-04,  3.8838e-04, -5.1975e-05,  8.6039e-04, -3.5837e-06,\n",
       "          4.6413e-04, -1.3082e-04, -2.4691e-05, -4.5227e-05,  1.7268e-04,\n",
       "         -1.0418e-04, -1.4691e-04,  2.8002e-04, -6.2034e-04, -3.3299e-04,\n",
       "         -1.8979e-04,  8.3188e-05,  1.6426e-04,  4.5957e-04,  1.4012e-04,\n",
       "         -4.6205e-04, -7.2619e-04,  4.9936e-04, -2.6535e-04, -1.2200e-04,\n",
       "          9.0376e-05,  5.6152e-05, -3.2817e-04, -1.3899e-04, -1.9586e-04,\n",
       "          2.9398e-04,  8.2090e-05,  3.3079e-04, -3.4027e-05,  7.0353e-04,\n",
       "          1.9361e-04, -4.2252e-05,  5.7974e-04,  4.5941e-05, -3.2299e-04,\n",
       "          2.7034e-04, -4.7571e-04,  2.1309e-04, -7.2737e-05,  1.2705e-04,\n",
       "         -3.4111e-04,  1.9642e-04, -7.0804e-04, -6.4798e-05, -3.0607e-04,\n",
       "         -4.4126e-05,  1.7545e-04, -1.5652e-04,  2.9876e-04, -5.8164e-04,\n",
       "          8.8246e-05, -4.1447e-04, -2.3822e-04,  1.9495e-04,  3.4834e-04,\n",
       "         -4.4689e-04, -6.0822e-05, -1.2868e-04,  6.1466e-04, -1.8074e-04,\n",
       "         -9.2974e-05, -5.1177e-04, -7.9942e-04, -1.1025e-04,  2.1437e-04,\n",
       "         -5.1863e-04,  7.0983e-04, -5.2743e-04,  3.5949e-05,  1.1778e-04,\n",
       "         -1.8364e-04, -2.0012e-04, -7.0145e-05, -6.1348e-05, -3.8595e-04,\n",
       "         -3.9353e-04, -2.6800e-05, -7.2002e-05, -2.1945e-04,  3.5983e-04,\n",
       "          8.4093e-05, -1.6345e-04, -1.5773e-05,  8.8364e-06,  4.6976e-06,\n",
       "         -3.5087e-04,  8.1440e-05, -3.5286e-05, -7.0566e-04, -1.0826e-04,\n",
       "         -2.9719e-04, -5.5262e-05,  2.0549e-04,  8.4493e-05,  1.2913e-04,\n",
       "          2.1309e-05,  5.6185e-04,  1.2409e-04,  5.7103e-04,  9.8167e-05,\n",
       "          7.2925e-04, -2.3812e-05, -1.3110e-04, -5.0064e-05,  7.1920e-05,\n",
       "         -7.0482e-05,  2.4510e-04,  3.4989e-04,  6.5563e-05,  1.5190e-04,\n",
       "          2.6562e-04, -3.2291e-04, -2.4687e-04,  9.1881e-05, -3.0547e-05,\n",
       "         -2.2622e-04, -1.6281e-04, -2.1946e-04, -2.5811e-04, -2.9380e-04,\n",
       "         -6.1911e-05, -2.9968e-04, -3.3098e-04, -6.3618e-04,  6.3616e-04,\n",
       "          1.9421e-04, -2.0728e-04,  2.7373e-04, -1.1617e-04, -4.0471e-04,\n",
       "         -2.2613e-04, -1.4587e-04, -1.7634e-05,  1.6030e-04, -1.9157e-05,\n",
       "          8.4969e-05, -3.7979e-05,  3.1255e-05, -1.9269e-05, -3.5837e-04,\n",
       "         -4.2446e-05,  1.2853e-04,  2.2321e-04, -8.2195e-05,  1.0220e-04,\n",
       "          5.7951e-05,  3.9295e-04,  2.9293e-05,  2.3896e-04,  3.4609e-04,\n",
       "         -1.3645e-04, -1.3573e-04,  2.7512e-04, -3.9633e-05,  2.5450e-04,\n",
       "         -4.8012e-04,  8.3099e-05, -2.6014e-05, -3.9578e-04,  6.0882e-05,\n",
       "         -2.0386e-04,  6.1812e-05,  1.2586e-04,  1.7419e-04,  1.4014e-04,\n",
       "          4.5916e-04, -1.9320e-04, -3.3762e-05,  4.2103e-04, -1.4019e-04,\n",
       "          8.5596e-05,  1.9191e-04,  5.5718e-04,  4.4461e-05,  3.7063e-04,\n",
       "         -1.2585e-05,  3.3612e-04,  6.2575e-05, -2.7182e-05, -1.4980e-04,\n",
       "         -4.4480e-05, -6.4081e-04,  6.0187e-04, -3.9812e-04, -1.4970e-04,\n",
       "         -5.5997e-04,  2.1488e-04, -5.4234e-04,  1.3535e-04, -2.8823e-04,\n",
       "         -2.2775e-05, -2.9616e-05, -2.7898e-04, -7.7505e-04,  2.6495e-04,\n",
       "          3.0678e-04, -7.3912e-04,  2.1527e-04,  8.3769e-05, -8.0869e-05,\n",
       "         -1.5709e-04,  1.4891e-04,  3.0888e-04,  4.0343e-05,  3.4407e-04,\n",
       "          3.9997e-05,  4.8551e-04,  4.1390e-04,  6.9722e-05, -5.8746e-04,\n",
       "         -2.3538e-04, -1.2737e-05,  3.0782e-04,  4.8383e-05,  4.6231e-04,\n",
       "         -3.7519e-04, -2.6186e-04,  3.0465e-04,  1.9986e-04,  2.1318e-04,\n",
       "         -1.2103e-04,  7.3276e-06, -1.3993e-04,  2.4959e-04, -7.8577e-05,\n",
       "          4.0793e-04,  1.7258e-04,  1.6772e-04, -2.9426e-04, -4.7348e-06,\n",
       "         -3.1774e-04, -1.6842e-04,  1.5764e-04, -1.2253e-04,  2.9313e-05,\n",
       "          6.6574e-05, -1.9066e-04, -5.2606e-04, -2.5202e-04, -2.6695e-04,\n",
       "         -3.0677e-04,  8.4713e-05, -4.3921e-06, -9.1247e-05, -1.0515e-04,\n",
       "         -3.0402e-04,  9.0793e-04, -6.4541e-04,  3.8525e-04, -5.0753e-05,\n",
       "          5.2374e-04, -4.5776e-05,  4.9867e-05, -1.6399e-04, -4.3706e-04,\n",
       "         -6.0610e-06, -4.9986e-04, -1.7639e-04,  3.1655e-04, -1.6078e-04,\n",
       "         -2.6469e-04,  2.1773e-04, -9.7452e-05,  3.8424e-04,  1.5516e-04,\n",
       "         -4.1831e-04,  2.1673e-04, -3.8972e-04,  3.4641e-04,  4.8008e-04,\n",
       "          4.0423e-05,  4.0991e-04]),\n",
       " tensor([[ 1.5860e-03,  5.8482e-04, -3.2027e-03,  ..., -3.3216e-03,\n",
       "           5.7973e-04,  3.0795e-03],\n",
       "         [-1.1437e-03, -2.9793e-03,  8.5533e-05,  ...,  1.7454e-04,\n",
       "           9.7382e-04,  1.2117e-03],\n",
       "         [ 3.6081e-05, -1.5414e-03, -1.4489e-04,  ..., -1.5402e-03,\n",
       "           9.0479e-04, -2.1374e-03],\n",
       "         ...,\n",
       "         [-2.8201e-04,  2.3592e-03, -9.2651e-04,  ...,  2.7200e-03,\n",
       "           5.7997e-04,  8.3996e-04],\n",
       "         [ 3.5693e-04,  8.0053e-04,  9.8149e-04,  ...,  8.3096e-05,\n",
       "           1.1194e-03, -5.1794e-04],\n",
       "         [-2.6610e-05,  4.5400e-04,  4.1147e-04,  ..., -7.3243e-05,\n",
       "           1.6302e-03,  1.8326e-03]]),\n",
       " tensor([-5.9405e-04,  8.8506e-04, -1.4476e-04,  ...,  1.6422e-04,\n",
       "         -9.3754e-05,  8.5370e-06]),\n",
       " tensor([[ 9.1551e-04, -1.4787e-03, -4.4455e-04,  ..., -1.5070e-03,\n",
       "          -5.1933e-04,  3.2277e-04],\n",
       "         [-1.9405e-03, -7.5403e-04,  2.1929e-03,  ..., -2.8493e-05,\n",
       "          -1.1049e-03, -5.5630e-04],\n",
       "         [-1.0733e-04, -1.4248e-03, -5.6976e-04,  ...,  8.5145e-05,\n",
       "          -7.8821e-04, -2.6591e-04],\n",
       "         ...,\n",
       "         [ 2.0190e-03, -1.2389e-03, -1.3664e-03,  ..., -2.4914e-03,\n",
       "           7.0290e-04,  1.8268e-04],\n",
       "         [-7.0304e-04, -6.8269e-04,  1.7642e-03,  ..., -4.1161e-03,\n",
       "           5.8855e-04, -1.4993e-03],\n",
       "         [ 2.5958e-03, -7.4743e-04, -6.2127e-04,  ..., -1.0128e-03,\n",
       "           1.8644e-04,  5.2020e-05]]),\n",
       " tensor([ 1.7275e-05, -4.0479e-05, -8.2958e-05, -1.4617e-04, -3.0893e-05,\n",
       "          2.2658e-04,  4.3668e-05, -5.1776e-05, -2.0628e-05, -9.8317e-05,\n",
       "         -7.5094e-05,  1.4267e-04,  4.4415e-04, -4.2448e-04,  3.4800e-04,\n",
       "         -1.2324e-04,  1.7708e-04,  6.7914e-05, -9.0046e-05,  3.8962e-04,\n",
       "         -3.0134e-04,  4.6514e-04, -2.0848e-04,  8.8047e-05,  3.7577e-05,\n",
       "          3.0399e-04, -4.9278e-05,  3.0039e-04,  3.3998e-04,  7.0108e-05,\n",
       "         -7.4901e-05,  1.8403e-04,  1.6354e-04, -5.6682e-04, -1.5082e-04,\n",
       "          9.7692e-05, -9.0051e-05,  1.7670e-04,  1.5963e-04, -2.2331e-05,\n",
       "         -5.2870e-05,  8.1423e-05, -2.5212e-04, -7.8880e-05, -1.8225e-05,\n",
       "          1.7831e-04,  9.7624e-05, -1.8256e-04,  3.2005e-04,  1.1804e-04,\n",
       "         -2.3647e-04, -2.1285e-04,  1.7168e-05, -5.6088e-05,  1.1210e-04,\n",
       "         -2.7871e-04,  3.3260e-04, -2.5056e-04, -1.6258e-04, -2.9656e-04,\n",
       "         -5.9581e-05,  2.2293e-04, -5.1799e-04,  1.5568e-04, -4.2114e-06,\n",
       "         -4.7435e-06, -3.4372e-04, -2.5460e-04,  3.2047e-04, -4.0727e-04,\n",
       "          1.4317e-04, -2.0676e-04,  3.3903e-04,  7.0902e-05,  2.2622e-04,\n",
       "         -5.9986e-05, -9.7493e-05,  3.0071e-04,  2.7124e-04,  2.5923e-05,\n",
       "          2.7061e-04, -3.2720e-05,  1.1099e-04, -1.3471e-04, -2.2858e-05,\n",
       "          5.2197e-04,  1.7552e-04,  1.3540e-04, -5.9352e-05, -1.7743e-04,\n",
       "         -1.1759e-04, -2.4416e-04,  2.1942e-04, -2.4411e-04, -1.3535e-05,\n",
       "         -5.3740e-04, -8.0019e-06,  2.7313e-04, -4.6031e-05,  2.4359e-04,\n",
       "          2.9711e-06, -3.6415e-06,  1.2818e-04, -9.3437e-05, -2.8685e-04,\n",
       "         -2.0683e-04, -4.9144e-05,  3.0267e-04, -1.0653e-04, -4.2569e-04,\n",
       "         -2.3391e-04, -3.9100e-04, -2.1818e-04, -5.6531e-04, -2.1972e-04,\n",
       "          2.5189e-04,  3.6286e-05,  6.0515e-05, -2.0385e-05, -3.7698e-04,\n",
       "          7.2786e-05, -2.8769e-04, -3.9584e-04, -7.5194e-04,  1.0394e-04,\n",
       "          3.1762e-06, -1.6025e-04,  2.0595e-04, -8.2393e-05,  1.3710e-04,\n",
       "         -1.2211e-04, -8.0560e-05, -2.9002e-04,  3.5892e-05, -2.1417e-04,\n",
       "          7.7668e-05,  2.2770e-04, -2.5740e-04, -1.7480e-04, -2.1183e-04,\n",
       "          1.2110e-04, -1.7762e-04,  1.1150e-04,  2.8331e-05,  3.9071e-05,\n",
       "         -3.7047e-04,  6.5872e-04, -1.8790e-04,  1.4162e-05,  2.3452e-04,\n",
       "         -6.2436e-05, -4.5597e-05, -1.4224e-04, -2.6511e-04,  4.8559e-04,\n",
       "         -3.5163e-05,  2.9071e-04,  1.4571e-04,  8.5333e-05,  1.9788e-04,\n",
       "          9.4694e-05, -2.2672e-04,  3.1380e-04, -3.4385e-04, -4.0054e-05,\n",
       "         -1.2121e-04,  3.0082e-04, -9.6863e-04,  4.0266e-04,  6.2201e-04,\n",
       "          9.8653e-05, -4.9881e-04,  2.6190e-04,  3.2678e-04,  2.4002e-04,\n",
       "         -1.6879e-04,  6.6055e-05,  4.4277e-04, -1.7521e-04, -3.3187e-05,\n",
       "          3.1945e-05,  7.6786e-05, -2.9730e-04,  5.1668e-04,  1.7483e-05,\n",
       "          4.3674e-04, -7.2621e-05,  1.7766e-04, -3.2406e-05,  9.6100e-05,\n",
       "         -2.2866e-04, -1.3846e-04,  1.1862e-04, -1.3478e-04,  2.8234e-05,\n",
       "         -1.4348e-04,  4.9549e-04,  1.4820e-04,  3.3734e-04,  4.9221e-05,\n",
       "         -2.0903e-04, -1.6542e-04,  2.6504e-04,  3.6578e-04, -1.1431e-04,\n",
       "         -1.7710e-04,  2.0565e-05, -3.5889e-05, -6.5239e-04, -4.0048e-04,\n",
       "          4.9578e-04, -1.2398e-04, -4.1277e-04,  7.6098e-05,  6.5999e-05,\n",
       "         -1.5338e-04, -1.8137e-04,  1.7884e-04,  7.1454e-05,  1.5792e-04,\n",
       "          3.3146e-06, -2.9340e-04,  3.1866e-04,  6.1890e-04, -5.3896e-05,\n",
       "         -6.3772e-04, -2.0781e-04, -5.3823e-04, -4.3788e-05, -6.7488e-05,\n",
       "         -2.2803e-04,  1.8438e-04,  3.5965e-04,  1.7112e-04,  2.5884e-04,\n",
       "         -8.8979e-05, -1.8180e-04,  8.6002e-05, -7.0468e-04,  4.9432e-05,\n",
       "          9.8451e-05,  1.9715e-04,  2.2070e-04, -7.0992e-05, -3.5097e-04,\n",
       "         -6.5010e-05, -1.6736e-06, -1.7577e-04,  3.2258e-04,  8.6226e-05,\n",
       "          1.4762e-04, -1.9467e-04,  1.6458e-04, -6.6112e-05,  2.0105e-04,\n",
       "          3.2747e-04,  2.9652e-04, -2.3388e-04,  1.1212e-04, -2.7806e-04,\n",
       "         -2.1671e-04,  3.3762e-05,  5.6101e-05, -1.3767e-05,  1.4519e-04,\n",
       "         -3.1057e-04, -6.1323e-04, -1.6320e-04,  1.1277e-04, -3.0207e-05,\n",
       "          4.5948e-04, -4.0770e-05, -4.9695e-04, -1.6107e-04,  3.9024e-04,\n",
       "         -5.3409e-05,  5.7373e-04,  9.4797e-05, -2.0999e-05,  6.1728e-06,\n",
       "          7.7040e-05, -5.3101e-05,  6.6666e-04,  5.7135e-05,  6.1906e-04,\n",
       "          2.3277e-04, -1.7833e-04,  7.5698e-05,  4.2895e-05, -3.2472e-05,\n",
       "         -3.5694e-04,  1.7996e-04, -1.2031e-04,  2.4315e-04, -2.5090e-04,\n",
       "          2.3487e-04,  4.6119e-04,  2.9163e-04, -1.1574e-04, -1.3664e-05,\n",
       "         -2.0137e-05,  2.0875e-04,  4.2967e-04, -2.3611e-04, -3.5197e-04,\n",
       "          6.5579e-04, -5.4458e-04, -3.1984e-04,  3.7678e-04, -1.6049e-04,\n",
       "          7.0019e-04, -1.1921e-04, -2.7144e-04, -4.4855e-04,  1.5462e-04,\n",
       "         -2.6336e-04,  2.0209e-05,  2.1498e-04,  1.2552e-04, -1.2289e-04,\n",
       "          2.6074e-04,  2.1266e-05,  2.4922e-04,  1.1951e-04, -6.7448e-05,\n",
       "         -1.9694e-05,  2.6971e-06, -1.7833e-04,  4.1261e-04,  9.6257e-05,\n",
       "          2.2865e-04,  1.2705e-04, -1.9346e-04,  1.3506e-04,  2.0210e-04,\n",
       "         -2.2266e-04, -2.3649e-04,  2.8995e-04, -6.5730e-04,  1.3937e-04,\n",
       "         -5.2753e-04,  1.5189e-04,  1.7040e-04, -6.5954e-05, -1.9577e-04,\n",
       "          2.2096e-04, -1.6247e-04, -3.9813e-04,  1.2172e-04, -1.3683e-05,\n",
       "         -1.2266e-04,  2.5805e-05,  9.6247e-05, -2.9435e-04, -8.9081e-05,\n",
       "          2.1306e-04, -1.4457e-04,  2.6726e-04,  5.7361e-04,  6.4652e-05,\n",
       "         -2.2370e-05,  4.4973e-04, -3.1811e-05,  1.5162e-04, -1.4718e-04,\n",
       "          3.2846e-05,  5.4050e-05, -4.8527e-04, -1.0604e-04,  4.8058e-05,\n",
       "          6.8364e-04,  9.4801e-05, -2.1077e-04, -3.6193e-05,  3.3043e-04,\n",
       "         -7.5796e-05, -3.4028e-04, -2.2187e-04, -7.8271e-05,  3.4839e-04,\n",
       "          6.9448e-05,  1.4641e-04,  4.7307e-05, -9.9271e-05, -4.5523e-04,\n",
       "         -9.2424e-05, -2.9374e-04, -3.7393e-05,  1.6971e-05, -4.9904e-04,\n",
       "          2.0358e-04, -7.4970e-05,  1.4408e-05,  2.3857e-05, -1.6061e-05,\n",
       "          1.0476e-04,  8.3784e-07, -3.6270e-05,  3.3652e-04,  1.9071e-04,\n",
       "         -7.3556e-05, -4.8500e-05,  1.2573e-07, -4.9720e-05, -6.3459e-06,\n",
       "         -5.2695e-05,  2.5325e-04, -7.9971e-05, -2.4844e-04, -1.8034e-04,\n",
       "          3.7304e-04, -6.8980e-05, -3.1865e-04,  1.1355e-04,  9.2400e-05,\n",
       "          1.9178e-04,  1.8855e-04, -1.9427e-04,  2.1903e-04,  8.5881e-05,\n",
       "          2.2094e-04, -2.2473e-04, -1.5175e-04, -1.8801e-04, -2.1489e-05,\n",
       "          3.1529e-04, -2.2891e-04,  2.7939e-04,  1.2211e-05,  1.7402e-04,\n",
       "         -3.0073e-04,  3.6779e-04,  4.0157e-04,  4.4616e-04, -1.9482e-04,\n",
       "         -5.2101e-04,  4.5358e-04, -4.0921e-04, -1.0827e-04,  5.1040e-04,\n",
       "          2.0093e-04,  2.3025e-04, -3.4574e-05,  7.2088e-05, -6.3599e-04,\n",
       "          3.5144e-04, -2.5166e-04, -2.9472e-04, -4.8440e-05,  1.3043e-04,\n",
       "         -1.1667e-03, -7.1695e-05, -4.5622e-04, -2.5563e-04, -3.0992e-04,\n",
       "          4.5076e-04,  8.1241e-05, -3.0441e-04,  1.7043e-04, -8.8835e-05,\n",
       "          1.1264e-04,  1.1035e-04,  1.3018e-04, -2.6594e-05,  1.1780e-05,\n",
       "         -9.1760e-05, -1.9867e-04, -5.9711e-05, -1.7592e-04, -3.1363e-05,\n",
       "          3.6866e-04,  3.3404e-05,  4.2040e-05,  3.9268e-04, -2.0994e-05,\n",
       "          1.1626e-04,  1.1280e-04,  7.4323e-04,  1.0579e-04,  1.9952e-04,\n",
       "          6.5472e-06, -1.0511e-04, -3.4103e-04, -1.5571e-04,  1.5095e-04,\n",
       "          2.2015e-04, -7.5862e-04,  2.3737e-04,  6.7978e-05,  3.2586e-04,\n",
       "         -4.7934e-04, -1.6580e-05,  2.6859e-04,  1.2385e-04,  8.2977e-05,\n",
       "          3.6424e-05,  1.6664e-04,  1.6733e-04, -2.0636e-04, -7.0829e-05,\n",
       "          8.5856e-05, -1.0367e-05,  9.2698e-05, -3.9980e-04,  4.3244e-05,\n",
       "          7.4301e-04, -2.9282e-04,  1.0351e-05,  7.9889e-06, -5.4862e-04,\n",
       "         -3.1355e-04, -2.2142e-04]),\n",
       " tensor([-1.4287e-03, -3.2425e-05, -1.3584e-03, -1.9130e-03, -1.1044e-03,\n",
       "          6.7115e-04,  1.6592e-03, -2.8526e-03, -2.5761e-04,  1.5312e-04,\n",
       "          4.3035e-05, -4.1533e-04,  2.5010e-04, -4.4942e-04,  1.9261e-03,\n",
       "         -1.5749e-03,  1.6475e-04, -2.3819e-03, -4.1342e-04, -3.6713e-03,\n",
       "         -3.1768e-03, -2.2970e-03,  1.6689e-05,  4.5764e-04, -4.8387e-03,\n",
       "         -1.0477e-03, -7.8964e-04, -3.8161e-03, -5.3120e-04,  1.0827e-03,\n",
       "         -4.6885e-04,  7.8011e-04,  5.5480e-04, -8.1730e-04,  8.2588e-04,\n",
       "          2.0957e-04,  8.8155e-04,  7.0035e-04,  1.4443e-03,  6.1202e-04,\n",
       "         -1.6673e-03,  2.3414e-03, -1.9851e-03,  1.1508e-03, -2.2333e-03,\n",
       "         -2.6464e-04, -5.7995e-05, -2.9109e-03,  1.3840e-04, -1.1899e-03,\n",
       "          2.6108e-03, -2.1703e-03,  5.0795e-04, -1.8144e-04,  2.2230e-03,\n",
       "          1.0748e-03, -9.8836e-04, -2.7907e-03, -6.4015e-04, -9.5415e-04,\n",
       "          7.8309e-04,  1.6633e-03, -4.4725e-03, -1.1734e-03,  3.5691e-04,\n",
       "          4.8703e-04,  3.4785e-04, -3.2577e-03,  2.3015e-03,  1.0636e-03,\n",
       "         -6.0042e-03,  2.6619e-04,  1.6170e-03, -1.3273e-03,  5.3859e-04,\n",
       "         -5.3844e-03,  1.8303e-03,  6.7264e-04, -8.8704e-04, -1.9776e-03,\n",
       "         -6.1917e-03,  2.3741e-03,  1.5501e-03, -2.0810e-03,  1.5016e-03,\n",
       "          8.0377e-04,  1.1398e-03,  3.9494e-04,  9.7740e-04, -7.1986e-03,\n",
       "          6.5184e-04, -4.0115e-03,  2.1451e-03,  1.6880e-04, -1.4932e-03,\n",
       "          2.3222e-03,  1.2565e-03,  6.2585e-05, -1.6214e-03,  4.7147e-04,\n",
       "         -1.6444e-03,  8.4639e-05,  2.0075e-04, -1.0220e-03, -7.0703e-04,\n",
       "         -9.5642e-04,  1.4639e-04,  3.1514e-03,  4.3452e-05, -2.5074e-03,\n",
       "         -1.5044e-03,  7.9882e-04, -1.0788e-03, -1.3303e-03, -5.4891e-03,\n",
       "         -1.4679e-03,  8.2088e-04,  6.3699e-04, -4.1211e-04, -1.4330e-03,\n",
       "         -2.0700e-03, -9.6703e-04,  1.9789e-05, -1.1194e-03, -7.8046e-04,\n",
       "          2.4045e-04,  3.2067e-05,  6.7592e-04, -1.1705e-03,  4.5705e-04,\n",
       "          2.6274e-04,  6.2066e-04, -1.1580e-03,  2.3651e-04, -1.1034e-03,\n",
       "         -3.3633e-03, -5.7775e-04, -2.0027e-05,  1.6505e-03, -1.3932e-03,\n",
       "         -1.1090e-03, -2.1219e-05, -4.1296e-03,  6.4909e-04, -6.0725e-04,\n",
       "          2.4015e-04, -5.6422e-04,  7.0471e-04, -5.4871e-03, -1.0161e-03,\n",
       "          1.2646e-03,  6.5601e-04, -2.2620e-04, -4.2732e-03, -1.0073e-04,\n",
       "          7.0924e-04,  6.5506e-04,  1.6594e-03, -3.1310e-03, -1.3912e-04,\n",
       "         -6.8629e-04, -4.6465e-03,  1.4975e-03, -5.0831e-04, -1.3703e-03,\n",
       "          1.8848e-03,  1.4669e-04, -4.1953e-03, -1.5922e-03, -7.5793e-04,\n",
       "         -5.8079e-04, -1.4342e-03,  1.1886e-03, -3.4771e-03,  1.0669e-03,\n",
       "         -6.9511e-04, -1.9010e-03, -1.6823e-03, -1.7921e-03, -1.0026e-03,\n",
       "         -3.4909e-03, -3.3243e-03, -4.8244e-04, -5.5659e-04,  7.9036e-05,\n",
       "          8.8847e-04, -7.7486e-05, -3.4517e-03, -2.5369e-03,  1.9205e-04,\n",
       "         -1.3975e-03,  2.2106e-03,  2.9527e-03, -2.3162e-04, -1.3759e-03,\n",
       "         -2.8569e-04,  7.9441e-04,  6.2132e-04, -5.1522e-04, -1.5062e-03,\n",
       "         -3.1624e-03, -4.4620e-04,  6.8045e-04,  1.9932e-03, -9.7859e-04,\n",
       "         -2.3671e-03, -9.6667e-04, -2.9613e-03,  6.3556e-04, -1.3264e-03,\n",
       "         -1.0356e-03, -2.5652e-03,  6.6650e-04,  1.5374e-03,  1.2381e-03,\n",
       "          9.4807e-04, -2.0707e-04,  2.1112e-04,  5.1153e-04, -5.3823e-04,\n",
       "          7.3230e-04,  1.2473e-03,  8.4758e-04, -3.3391e-04, -1.6955e-03,\n",
       "         -1.3989e-04,  7.4446e-04, -1.7466e-03, -1.5894e-03, -6.1095e-04,\n",
       "          6.2567e-04,  1.4278e-03, -1.1778e-03,  9.9778e-04, -2.2281e-03,\n",
       "          2.7132e-04, -5.3418e-04,  2.5876e-03,  8.6015e-04, -4.9318e-03,\n",
       "         -1.1122e-04, -2.0586e-03,  1.5302e-03, -2.5166e-03, -2.5582e-04,\n",
       "          2.1980e-03, -3.5536e-04, -1.4505e-03, -1.6648e-04,  2.0251e-03,\n",
       "          1.1351e-03,  8.7017e-04, -1.6202e-03, -5.4014e-04,  1.7434e-03,\n",
       "          7.4899e-04, -3.6818e-04,  5.5301e-04, -1.0973e-04,  1.0588e-03,\n",
       "         -1.0371e-03, -3.8950e-03, -9.7615e-04,  7.2157e-04, -1.4448e-04,\n",
       "         -2.1446e-04, -1.4204e-04, -1.2337e-03, -1.3694e-03, -5.1129e-04,\n",
       "         -6.9141e-06, -6.3062e-05,  7.0506e-04,  4.7207e-05, -1.6371e-03,\n",
       "          1.0707e-03,  1.5066e-03, -2.1614e-03, -1.8203e-04,  9.7835e-04,\n",
       "          3.5167e-05, -1.9859e-03, -9.0122e-05, -4.0305e-04, -4.4888e-04,\n",
       "          7.9787e-04,  2.6560e-04, -1.2212e-03, -2.0661e-03,  1.9418e-03,\n",
       "         -2.1672e-04, -3.8317e-03,  8.0967e-04, -3.3514e-03,  1.1246e-03,\n",
       "         -8.5962e-04, -3.6014e-03, -4.7970e-04, -1.2883e-03, -2.3085e-03,\n",
       "          1.3303e-03,  7.0989e-04,  1.3012e-03,  7.2575e-04, -2.6922e-03,\n",
       "         -9.7108e-04, -1.6260e-03,  2.6493e-03,  1.2521e-03, -6.7580e-04,\n",
       "         -1.2261e-04,  1.3372e-03, -1.1396e-03,  1.3711e-03,  1.3540e-03,\n",
       "          9.7287e-04, -3.0637e-05,  1.2833e-03, -2.0149e-03,  4.8661e-04,\n",
       "         -2.7051e-03, -7.6354e-04,  2.0599e-03,  7.4542e-04, -1.3946e-03,\n",
       "         -6.7103e-04, -4.2984e-03, -3.7671e-03, -1.0681e-04, -7.7713e-04,\n",
       "          1.0384e-03, -1.1326e-03,  7.7903e-04,  2.5630e-06,  4.7743e-04,\n",
       "         -3.9426e-03,  2.1625e-04, -3.0106e-03, -3.7991e-03,  1.4149e-03,\n",
       "         -3.7122e-04,  1.8631e-03,  2.9939e-04, -2.8198e-03, -5.8496e-04,\n",
       "         -8.9121e-04, -2.3167e-03, -5.0038e-04,  4.1127e-05, -2.7713e-03,\n",
       "         -1.2556e-03, -1.9588e-03,  7.3814e-04, -2.4056e-04, -2.0969e-03,\n",
       "          1.5450e-04, -3.7515e-04,  3.5000e-04, -2.6333e-04,  2.0528e-04,\n",
       "         -3.0146e-03, -2.8935e-04, -2.2593e-03,  6.5845e-04, -2.7192e-03,\n",
       "          1.0908e-03, -1.6762e-03, -9.2411e-04,  9.0301e-04,  1.3522e-03,\n",
       "         -4.9794e-03,  1.4789e-03,  4.2152e-04,  2.2808e-03, -1.1598e-03,\n",
       "         -1.0967e-05,  2.4296e-03,  4.9698e-04, -8.3447e-06, -1.2264e-03,\n",
       "         -1.0014e-04, -3.7241e-03, -3.3998e-04, -2.5475e-03,  4.8351e-04,\n",
       "         -2.2167e-03,  2.3252e-04, -1.6457e-03, -2.2123e-03,  2.1214e-03,\n",
       "         -2.4980e-03, -6.4433e-04,  1.3919e-03,  1.9886e-03,  4.5466e-04,\n",
       "         -2.6183e-03, -3.3753e-03, -7.0691e-05,  1.1631e-03,  1.4791e-03,\n",
       "          6.2203e-04,  3.6907e-04, -3.9423e-04,  2.7649e-03, -1.7627e-03,\n",
       "          1.8990e-04, -4.3557e-03, -1.0818e-04, -1.2659e-03,  3.6805e-03,\n",
       "          6.7222e-04, -1.6311e-03, -1.7154e-04, -3.3500e-03,  1.6928e-04,\n",
       "         -3.8568e-03,  1.6740e-03, -2.4420e-03,  3.8981e-04, -1.3009e-03,\n",
       "         -1.3934e-03, -4.6408e-03,  5.3275e-04,  1.4251e-04,  7.2420e-04,\n",
       "         -4.1083e-03, -5.2953e-04,  5.6231e-04, -1.6085e-03, -2.4879e-03,\n",
       "         -4.0013e-03,  8.7547e-04,  1.7291e-03, -1.9025e-03,  2.1720e-04,\n",
       "         -1.2431e-03, -3.0526e-03, -2.2437e-03,  3.0947e-04,  1.4424e-03,\n",
       "         -2.0425e-03,  3.6824e-04,  1.4079e-04,  6.1715e-04, -2.3197e-03,\n",
       "         -2.2206e-03,  4.6706e-04, -2.1116e-03, -3.7735e-03, -1.8314e-03,\n",
       "          1.0839e-03, -1.0283e-03,  2.8789e-04,  4.1020e-04, -2.6950e-03,\n",
       "          8.3256e-04, -6.5053e-04,  1.1018e-03,  4.4227e-04, -6.0189e-04,\n",
       "          2.5082e-04, -1.0893e-03, -1.3802e-03, -6.1101e-03,  2.2316e-04,\n",
       "         -5.8448e-04,  3.5182e-03, -1.1036e-03,  6.2084e-04, -2.3249e-03,\n",
       "         -2.3050e-03,  3.9440e-04, -1.3149e-04,  1.4858e-03,  9.9659e-04,\n",
       "         -1.3696e-03, -3.9741e-03, -7.9405e-04, -2.5165e-04,  1.1742e-04,\n",
       "          1.5302e-03,  4.5949e-04,  4.4954e-04,  1.2795e-03,  1.5628e-04,\n",
       "          8.6021e-04,  1.3952e-03,  4.0025e-04, -1.0574e-03,  9.5844e-05,\n",
       "          7.2098e-04,  2.7168e-04,  9.1287e-04,  2.4865e-03, -4.7821e-03,\n",
       "          6.8462e-04, -1.6122e-03,  1.3070e-03,  1.4710e-03, -1.0185e-03,\n",
       "         -1.3641e-03, -1.0233e-03, -4.9829e-05, -3.1006e-04,  8.3619e-04,\n",
       "          5.0616e-04,  1.7084e-03,  2.5294e-03, -1.6901e-03, -1.4472e-03,\n",
       "         -2.0413e-03, -3.0606e-03]),\n",
       " tensor([ 2.2389e-04, -2.1211e-05,  1.5881e-04, -5.9972e-04,  2.3141e-04,\n",
       "         -5.5477e-05, -1.0595e-04,  5.5918e-05, -2.3526e-04,  1.3627e-05,\n",
       "          1.7595e-05,  3.1373e-04,  5.3344e-04,  2.6341e-04, -1.8487e-04,\n",
       "          1.0202e-04,  3.2593e-04,  3.4973e-04, -3.5042e-04,  3.2108e-05,\n",
       "          5.0689e-04,  7.5485e-04, -5.1490e-04,  2.2453e-04,  1.4780e-04,\n",
       "          4.6875e-04, -4.3879e-05, -2.1458e-06,  7.4320e-04, -3.8835e-04,\n",
       "         -7.2930e-05,  1.2055e-04,  2.9085e-04, -7.2896e-05,  4.5375e-04,\n",
       "         -1.0886e-04,  1.9722e-05,  5.3041e-04, -9.6183e-05,  4.9719e-04,\n",
       "          2.9249e-04,  2.4022e-04, -2.1071e-04, -5.4452e-04,  3.2195e-04,\n",
       "          2.2607e-04,  1.5119e-04, -5.0366e-04,  2.6342e-04, -2.0810e-04,\n",
       "         -2.7799e-04, -9.9018e-05,  5.6582e-04, -1.1287e-04,  1.3654e-04,\n",
       "         -5.2825e-06, -2.9791e-05,  7.9548e-04,  1.6804e-04,  4.3770e-04,\n",
       "         -2.8013e-04,  2.1888e-05, -1.6930e-04,  5.7652e-04, -1.6935e-04,\n",
       "          3.4096e-04, -2.6501e-04, -4.1573e-04,  3.8874e-04,  5.5555e-05,\n",
       "          1.7997e-04,  1.1575e-04, -2.8508e-04,  3.3875e-04, -3.1844e-05,\n",
       "          1.1955e-04, -3.6317e-04, -1.6655e-04,  1.8306e-04, -9.3572e-05,\n",
       "         -2.6323e-04,  2.1354e-04,  8.0429e-06,  1.5802e-04, -7.9788e-05,\n",
       "          5.2181e-04,  1.1523e-04, -1.6678e-04,  3.3965e-04,  5.3511e-04,\n",
       "         -3.7819e-05, -1.8711e-04, -4.9777e-05,  2.2776e-05,  3.9838e-05,\n",
       "          1.1658e-04,  1.1929e-04,  3.4634e-04,  6.5367e-04,  3.9229e-04,\n",
       "          2.3205e-05, -1.2481e-04,  3.9458e-04, -7.5953e-05,  6.6139e-05,\n",
       "          1.8976e-04,  1.4567e-04,  3.8723e-04,  6.7116e-04,  1.0096e-04,\n",
       "         -2.7176e-05, -4.6350e-04,  2.3824e-04,  1.5633e-04,  2.4075e-06,\n",
       "          3.2078e-04, -1.2686e-04, -8.4161e-04,  4.3719e-05, -4.4584e-05,\n",
       "          2.2350e-04, -3.3798e-04, -4.0445e-04, -4.8771e-04,  3.8477e-04,\n",
       "          2.9872e-04, -4.3228e-05, -7.6303e-04,  3.4939e-05, -6.7245e-05,\n",
       "         -2.4567e-04,  2.2337e-05, -1.6831e-04,  3.3770e-05, -1.3786e-04,\n",
       "         -1.5855e-05, -3.8104e-04, -6.0368e-05, -3.8381e-04, -1.7454e-04,\n",
       "          3.2676e-04,  1.2943e-04,  4.1730e-04,  3.3165e-04,  2.6968e-04,\n",
       "         -7.5543e-05,  1.7237e-04,  1.8483e-04,  1.5078e-04,  2.7919e-04,\n",
       "         -1.6156e-04, -5.7402e-05,  4.8717e-05, -3.0811e-05,  1.9411e-04,\n",
       "          2.1313e-04,  2.4741e-04,  3.9402e-04, -1.7828e-04,  2.6367e-04,\n",
       "         -3.3888e-05, -2.7196e-04,  3.1145e-04, -5.7540e-04,  3.3707e-04,\n",
       "         -5.8127e-05,  2.2523e-04, -3.5380e-04, -7.2137e-05,  3.2494e-05,\n",
       "         -3.4876e-05, -5.6900e-05,  1.5556e-04,  1.3748e-04, -2.0189e-04,\n",
       "         -3.1755e-04,  1.9581e-05, -6.3308e-05,  1.1673e-04,  5.3797e-04,\n",
       "          1.7591e-04, -1.8959e-04, -5.5353e-04, -5.9975e-05,  9.6716e-05,\n",
       "         -1.2191e-04, -5.8562e-04, -6.9626e-05,  1.3572e-04,  2.5416e-04,\n",
       "          2.9035e-04, -3.9644e-05, -5.6010e-05, -3.9060e-06,  2.3637e-04,\n",
       "         -4.5386e-04, -1.8288e-04,  6.5764e-04,  7.5836e-05,  1.3756e-04,\n",
       "         -8.7501e-04, -4.9528e-04, -1.6832e-04,  7.1985e-04, -5.3721e-05,\n",
       "          2.8964e-04, -9.5986e-05, -2.1143e-04, -5.4738e-04,  4.1360e-04,\n",
       "          8.5214e-04, -3.7980e-04, -7.0074e-04,  3.3910e-04, -5.9485e-05,\n",
       "         -2.5748e-04, -1.1031e-05, -3.8268e-05,  2.2729e-04, -3.5351e-04,\n",
       "          2.5013e-04,  3.3410e-04,  1.5181e-04,  4.1256e-04, -2.0087e-05,\n",
       "         -1.8581e-04,  8.6474e-05, -1.5074e-04,  3.8222e-04,  2.5937e-04,\n",
       "         -1.0995e-04,  1.6776e-04,  3.7897e-04, -7.5883e-05,  3.9193e-04,\n",
       "          4.0352e-05, -3.0894e-04, -3.1105e-04, -5.6906e-04, -2.3589e-04,\n",
       "         -6.7955e-04,  3.3946e-04, -2.2241e-04, -1.2823e-04, -2.1912e-04,\n",
       "         -2.2393e-04,  5.6281e-04, -5.8237e-05,  1.1296e-04, -1.0476e-04,\n",
       "         -2.3694e-04, -5.8165e-05, -1.9268e-04, -7.6521e-05,  7.2159e-05,\n",
       "          2.3347e-04,  7.5698e-04,  2.5250e-04,  1.2384e-04,  1.8986e-04,\n",
       "         -3.1155e-04,  3.2957e-04, -2.3631e-05,  3.6261e-04,  1.1591e-04,\n",
       "         -3.0237e-04, -3.7439e-07,  1.7094e-04,  2.1614e-04, -1.2984e-04,\n",
       "          4.0962e-04,  2.1657e-04,  6.5072e-06, -3.0831e-05,  4.2341e-04,\n",
       "          4.3325e-04,  3.7774e-06,  1.6936e-04, -3.7661e-04,  4.1784e-04,\n",
       "          2.0876e-04, -4.4054e-04,  1.8315e-04,  1.0044e-04,  2.7179e-04,\n",
       "         -6.4792e-05,  1.3692e-04,  3.5727e-04,  1.0741e-04, -2.5612e-04,\n",
       "         -3.5109e-04,  1.1763e-04,  3.8831e-04,  3.0959e-05, -9.0726e-05,\n",
       "          3.0217e-04, -2.3049e-04, -5.6719e-04,  6.6284e-05, -1.3138e-04,\n",
       "         -4.3502e-04,  6.8597e-04,  4.4680e-04, -1.5354e-04,  2.0971e-04,\n",
       "          4.5123e-04, -5.7579e-04, -1.1325e-04,  2.8435e-04,  2.1019e-04,\n",
       "          2.3593e-04,  4.5805e-04, -3.2265e-04, -6.2259e-04,  2.4964e-04,\n",
       "         -3.6796e-04, -6.4013e-04,  3.5673e-04, -1.4205e-04, -3.0810e-04,\n",
       "         -8.0547e-05, -2.7418e-04,  4.8449e-04,  2.7689e-04, -2.8129e-04,\n",
       "         -5.8752e-05,  1.1940e-05,  3.0177e-04,  4.7787e-04,  6.0327e-05,\n",
       "          1.9469e-04,  5.7233e-04, -2.6770e-04,  3.7887e-04, -3.5923e-04,\n",
       "         -1.7174e-06,  8.5518e-05,  1.0872e-05, -5.3464e-04,  6.7186e-05,\n",
       "         -1.8185e-04, -2.5129e-04, -3.1618e-04,  5.8256e-04, -1.4651e-04,\n",
       "          3.7050e-04, -2.5805e-04, -5.6395e-04, -2.2629e-05, -1.6694e-04,\n",
       "         -1.9397e-04,  2.4145e-04,  3.7800e-04, -2.7875e-04, -4.5337e-05,\n",
       "         -5.7906e-05, -3.3332e-04,  3.6319e-04, -6.6811e-05, -8.2664e-06,\n",
       "          4.7483e-04,  4.5196e-04,  4.1835e-06,  2.8423e-04,  5.1256e-05,\n",
       "          2.6433e-04, -4.2637e-04, -7.0684e-04, -1.9134e-04,  3.3362e-05,\n",
       "         -5.0485e-05,  4.7861e-04,  3.9963e-04, -6.8620e-05,  1.4810e-04,\n",
       "          1.2782e-04, -3.3746e-04, -1.5945e-04, -3.8587e-05, -2.4470e-04,\n",
       "         -1.9334e-06, -3.5344e-04, -5.7494e-04, -4.6985e-04,  2.3127e-04,\n",
       "         -3.8446e-04, -1.9736e-04,  1.1303e-04,  2.0023e-04, -4.6188e-04,\n",
       "          7.7828e-04, -1.2253e-04,  1.8397e-04, -1.7643e-05,  2.3140e-04,\n",
       "          3.4890e-04, -1.4350e-04, -1.1108e-04,  5.1481e-04, -1.0971e-04,\n",
       "         -5.0218e-04, -8.0465e-04, -2.0236e-04,  7.4505e-05, -1.0021e-05,\n",
       "          3.1431e-04, -9.2094e-05,  3.7282e-04, -1.3395e-04,  4.5965e-04,\n",
       "          2.5808e-04, -1.4438e-04, -2.8523e-04,  1.6476e-04,  3.3332e-04,\n",
       "          9.5094e-05,  1.9526e-04,  2.6410e-05, -5.4447e-05,  1.2194e-04,\n",
       "          2.6806e-04,  3.4412e-04, -5.3046e-04, -4.5057e-04,  2.6739e-04,\n",
       "          4.3803e-04, -1.5150e-04,  3.8761e-04, -4.6499e-05,  6.4164e-04,\n",
       "         -4.3312e-04,  1.6427e-04,  5.8593e-04,  4.0231e-04,  1.7375e-05,\n",
       "         -4.5278e-04,  9.2831e-04, -1.6603e-04,  2.1653e-04,  3.7924e-04,\n",
       "         -4.4853e-04, -1.2283e-04,  3.0980e-04,  1.3518e-04,  1.8534e-04,\n",
       "          1.6162e-04, -4.2630e-04,  2.0142e-04, -4.8199e-04,  3.5170e-04,\n",
       "         -1.1112e-03, -7.2926e-04, -1.6692e-04, -2.7174e-04,  3.4159e-05,\n",
       "          3.5621e-05,  4.1741e-05,  1.4086e-04,  2.9839e-04, -1.6627e-04,\n",
       "          1.6693e-04,  7.6599e-05,  1.5215e-04,  9.8288e-05, -1.1494e-04,\n",
       "         -4.2573e-04, -5.5004e-04, -5.9530e-06, -2.0216e-04, -3.1885e-04,\n",
       "          7.4489e-04,  1.4526e-04,  1.7311e-04,  6.4857e-04, -7.8403e-05,\n",
       "          5.3733e-04, -1.3551e-04,  2.6101e-04,  9.0663e-05,  4.5171e-04,\n",
       "         -4.5323e-04, -3.0106e-04, -7.9662e-05,  1.0302e-04,  2.7094e-05,\n",
       "         -1.0041e-04, -6.2705e-04,  5.4759e-04, -3.1435e-04, -2.4394e-04,\n",
       "         -2.3220e-04,  3.5338e-04,  3.5204e-05,  3.9484e-05, -1.0906e-04,\n",
       "          2.6313e-04, -2.6752e-04,  2.8935e-04, -2.5321e-04, -4.5680e-04,\n",
       "          1.8279e-04, -1.4398e-04,  5.8633e-04, -3.0014e-04,  4.3414e-04,\n",
       "          2.0813e-04, -1.8785e-04,  2.6559e-04, -3.6273e-04, -3.7441e-04,\n",
       "         -2.6010e-04,  5.4031e-04]),\n",
       " tensor([[-1.3031e-03,  1.1150e-03, -1.6076e-03,  ...,  2.0309e-03,\n",
       "          -6.6942e-05,  1.3300e-03],\n",
       "         [ 1.4397e-03,  7.0475e-04, -1.5673e-03,  ..., -1.1004e-03,\n",
       "          -2.9339e-03, -4.4615e-03],\n",
       "         [ 8.0649e-04,  1.1093e-03, -8.6971e-04,  ..., -3.1683e-04,\n",
       "          -1.6024e-03, -2.6114e-03],\n",
       "         ...,\n",
       "         [-3.5867e-03, -2.0239e-04,  2.5993e-04,  ...,  2.1473e-03,\n",
       "           1.2119e-03, -7.1051e-06],\n",
       "         [ 1.4871e-04,  1.9490e-03,  9.1959e-04,  ...,  1.3355e-03,\n",
       "           3.9207e-04, -2.0839e-04],\n",
       "         [ 8.5480e-04, -6.8691e-05,  3.3733e-03,  ..., -9.1146e-04,\n",
       "          -3.6589e-03,  2.9862e-03]]),\n",
       " tensor([-0.0011, -0.0004,  0.0002,  ..., -0.0004, -0.0002, -0.0003]),\n",
       " tensor([[ 1.6508e-03, -8.9134e-04,  2.8557e-03,  ..., -1.3744e-03,\n",
       "          -2.0877e-03,  2.5912e-03],\n",
       "         [ 2.0274e-03,  1.3439e-04,  5.6665e-05,  ..., -2.6769e-03,\n",
       "           3.3136e-03,  7.7566e-04],\n",
       "         [ 1.9203e-03, -1.2452e-03, -4.5403e-04,  ...,  2.2853e-03,\n",
       "           7.3292e-05, -5.8000e-04],\n",
       "         ...,\n",
       "         [ 1.3792e-03, -1.5945e-04,  8.6880e-04,  ..., -1.1624e-03,\n",
       "           5.8701e-04,  7.1375e-04],\n",
       "         [-2.5638e-04,  1.2754e-03, -2.2946e-03,  ...,  1.3898e-03,\n",
       "          -1.6559e-03,  1.5230e-03],\n",
       "         [ 3.9228e-04, -5.2303e-04, -1.0484e-03,  ...,  2.1426e-03,\n",
       "           1.2748e-04, -7.0972e-04]]),\n",
       " tensor([-1.2893e-04, -9.8580e-06, -9.6589e-05,  1.3665e-04, -2.2924e-04,\n",
       "          1.6648e-04,  1.3317e-04, -9.8675e-05,  1.2978e-04, -1.0776e-04,\n",
       "         -6.0309e-05,  1.3506e-05,  2.5468e-04, -6.6822e-04,  3.4475e-04,\n",
       "         -1.6778e-04, -9.6098e-05, -4.0283e-04,  1.9687e-04,  2.1745e-04,\n",
       "         -5.1735e-04,  1.0459e-04,  1.1267e-04,  8.4318e-05,  1.5742e-05,\n",
       "          1.1980e-04,  1.5776e-05,  2.3648e-04, -5.0891e-04,  4.0153e-04,\n",
       "          7.4014e-05,  4.2941e-05, -4.2270e-05, -4.6143e-04, -3.4727e-04,\n",
       "          1.8581e-04,  2.8857e-05, -7.7665e-05,  2.0920e-04, -4.5015e-04,\n",
       "         -2.2461e-04, -8.9895e-05, -3.6308e-06,  2.1030e-04, -2.3680e-04,\n",
       "          1.0299e-04, -2.2374e-05,  1.5704e-04,  2.0776e-04,  1.6738e-04,\n",
       "         -3.9999e-05, -7.9009e-05, -3.0389e-04, -3.3677e-06,  1.5558e-04,\n",
       "         -3.4184e-04,  3.4450e-04, -5.4624e-04, -3.7390e-04, -5.7313e-04,\n",
       "          1.8012e-04,  3.1169e-04, -2.6858e-04, -1.7566e-04,  1.1504e-04,\n",
       "         -2.2175e-04, -1.8961e-04, -8.8320e-05, -9.6895e-06, -2.9991e-04,\n",
       "          7.5907e-05, -2.1743e-04,  5.0756e-04, -2.2260e-04,  2.4377e-04,\n",
       "         -8.4252e-05,  1.6686e-04,  4.1341e-04,  2.1889e-04,  1.8522e-04,\n",
       "          3.9955e-04, -4.7074e-05,  1.0545e-04, -1.5065e-04,  6.8733e-05,\n",
       "          2.0640e-04,  1.6155e-04,  3.8876e-04, -2.1318e-04, -4.6056e-04,\n",
       "         -1.2495e-04, -5.5522e-05,  2.8902e-04, -2.7017e-04, -5.2658e-05,\n",
       "         -6.2065e-04, -1.2306e-04,  1.0364e-05, -4.2924e-04, -7.4360e-05,\n",
       "         -8.4080e-06,  1.3418e-04, -2.0177e-04, -5.0235e-05, -3.3387e-04,\n",
       "         -2.6660e-04, -9.5504e-05,  1.8898e-04, -3.6901e-04, -4.6699e-04,\n",
       "         -2.1628e-04, -1.0971e-04, -3.1193e-04, -5.8749e-04, -2.5675e-04,\n",
       "          5.3890e-05,  1.7646e-04,  2.7347e-04, -1.4436e-05, -1.6782e-04,\n",
       "         -5.7424e-06, -1.3972e-04, -8.9757e-05, -5.3151e-04,  7.6611e-06,\n",
       "         -2.2082e-04, -8.1366e-05,  5.7307e-04,  7.2859e-05,  2.4730e-04,\n",
       "          1.1709e-04, -5.4525e-05, -1.6555e-04,  8.2456e-05, -9.8960e-05,\n",
       "          6.3846e-05,  4.0666e-04, -1.6303e-04,  6.7872e-05, -3.7344e-05,\n",
       "         -1.0649e-04, -2.7175e-04, -2.1475e-05, -2.6624e-04, -7.6720e-05,\n",
       "         -3.4420e-04,  4.9050e-04, -2.6026e-04, -1.4706e-05,  5.4669e-06,\n",
       "          6.2393e-05,  2.5498e-05, -1.7898e-04, -2.1224e-04,  4.9443e-04,\n",
       "         -1.4279e-04,  1.9000e-05, -8.4683e-05,  1.8542e-04,  1.3489e-04,\n",
       "          1.4881e-04, -2.0089e-04,  9.7573e-05,  3.3006e-05, -2.0318e-04,\n",
       "         -7.4611e-05,  2.2691e-04, -5.5496e-04,  4.6837e-04,  6.7312e-04,\n",
       "          2.1690e-04, -3.8156e-04,  1.5486e-04,  1.9605e-04,  3.6561e-04,\n",
       "         -1.3262e-04,  1.3650e-04,  4.2878e-04, -2.2798e-04, -2.7913e-04,\n",
       "         -1.1709e-04,  1.5668e-04,  2.5064e-04,  5.9355e-04,  1.9719e-05,\n",
       "          4.1234e-04,  1.4907e-04,  1.8509e-04, -6.3349e-06, -4.5482e-05,\n",
       "         -3.2554e-04, -2.0358e-04,  1.7486e-04, -1.5168e-04, -1.4127e-04,\n",
       "          1.7329e-04,  5.4479e-04, -2.2933e-04,  3.3111e-04,  4.8601e-06,\n",
       "          3.1945e-04,  1.9418e-04,  3.8961e-04, -5.9085e-05,  5.9139e-07,\n",
       "         -3.3007e-04,  1.0165e-04,  1.5047e-04, -1.8035e-04, -6.4147e-04,\n",
       "          1.0031e-04,  1.2997e-04,  7.7542e-05, -3.2776e-04,  7.7006e-05,\n",
       "          1.1572e-04, -1.9025e-04,  3.4692e-04, -5.2406e-05,  5.2371e-04,\n",
       "         -9.4459e-05, -5.4646e-04,  2.0719e-05,  1.3618e-04, -4.5450e-05,\n",
       "         -5.9499e-04, -2.3643e-04, -3.6752e-04, -2.0846e-04, -1.9837e-04,\n",
       "         -1.8612e-04,  6.4597e-05,  1.1633e-04,  1.5912e-04,  1.1242e-05,\n",
       "         -9.1361e-05,  1.3361e-05,  3.8339e-04, -1.6427e-04,  1.2736e-04,\n",
       "          5.3317e-04,  1.3851e-04,  2.8451e-04,  1.1246e-04, -9.5981e-05,\n",
       "          6.5908e-05, -3.2391e-04, -1.2584e-04,  2.7103e-04,  1.4186e-04,\n",
       "          3.0393e-04, -9.3141e-05,  2.8508e-04,  5.4179e-05,  2.2818e-04,\n",
       "          2.8451e-04, -1.3137e-04, -2.8881e-04,  5.9225e-05, -3.8541e-04,\n",
       "          2.1160e-04, -4.8016e-05,  4.5720e-05, -2.8456e-04, -8.5626e-06,\n",
       "         -6.4638e-05, -5.5425e-04, -2.9066e-04,  2.0637e-05, -1.8230e-05,\n",
       "          5.0249e-05, -2.6461e-04, -5.1728e-04, -1.1856e-04,  1.6164e-04,\n",
       "         -2.2988e-04,  6.5583e-04, -3.8430e-05,  2.2316e-04, -2.5990e-04,\n",
       "          7.5307e-06,  1.7966e-04,  3.9519e-04,  5.3443e-05,  3.5737e-04,\n",
       "          2.3715e-04, -2.2765e-04, -1.9561e-04,  1.4504e-05,  2.3650e-04,\n",
       "          5.7141e-05,  6.2438e-05, -5.2015e-04,  1.3290e-04, -2.4786e-04,\n",
       "         -8.6443e-05,  4.9100e-04,  5.4756e-04, -8.2594e-05,  9.9004e-05,\n",
       "          1.2782e-04, -8.7213e-05, -5.0562e-05, -2.3488e-04, -4.7190e-04,\n",
       "          3.6321e-04, -2.9601e-04, -2.0319e-04,  3.8818e-06, -2.9896e-04,\n",
       "          5.5900e-04, -5.0017e-04,  5.2519e-05, -1.5055e-04, -5.1474e-05,\n",
       "          1.9343e-04,  3.1871e-04, -1.8163e-04,  1.8150e-04,  1.5564e-04,\n",
       "          2.8674e-04,  3.4047e-04,  3.7765e-06, -1.7284e-04,  2.2541e-04,\n",
       "         -1.3082e-05,  2.5460e-05, -3.4534e-04,  2.2048e-04,  1.7317e-05,\n",
       "          5.8355e-05, -1.9934e-04,  9.1776e-05, -1.8940e-04,  4.1069e-04,\n",
       "         -1.4526e-04, -2.4335e-04,  2.1444e-04, -3.5185e-04,  1.7619e-04,\n",
       "         -5.1076e-04,  2.2000e-04,  4.1927e-04, -3.6513e-04, -6.7547e-05,\n",
       "         -3.1492e-05,  6.8890e-05,  7.3282e-05,  1.1192e-04,  1.1075e-04,\n",
       "         -2.5906e-05, -5.7524e-05, -1.1600e-04, -2.4806e-04, -1.4554e-05,\n",
       "          2.1239e-04, -3.3246e-05, -9.4759e-05,  4.6001e-04,  1.6536e-05,\n",
       "         -3.1147e-04,  3.1147e-04, -4.6651e-05,  2.6535e-05, -2.2360e-04,\n",
       "         -2.9451e-06,  3.0993e-04,  8.8410e-06,  1.2277e-04,  8.1571e-05,\n",
       "          7.9368e-04, -1.9027e-04, -4.6651e-04, -2.1392e-05,  6.4425e-05,\n",
       "         -2.3161e-04,  1.2117e-04, -6.8993e-05, -1.5279e-05,  5.1239e-04,\n",
       "          3.3942e-05,  4.7820e-04,  5.0555e-04,  1.9585e-04, -4.8680e-04,\n",
       "          6.0126e-05, -2.4929e-04, -7.1499e-05, -7.5296e-05,  1.0678e-05,\n",
       "         -1.0278e-04, -1.4949e-05, -1.7548e-04,  2.2463e-05, -8.6981e-05,\n",
       "         -7.6321e-05,  1.0696e-04,  1.0519e-04, -2.6580e-06,  2.0127e-04,\n",
       "          3.5335e-04,  3.9970e-04,  3.2872e-04, -1.3237e-04,  6.3045e-05,\n",
       "         -1.8932e-04,  4.2808e-04, -4.0330e-04, -1.3179e-04, -2.6771e-04,\n",
       "          1.7946e-04, -7.0361e-05, -5.6189e-05, -1.2205e-04, -1.2434e-04,\n",
       "          1.0438e-04,  2.1610e-04, -1.8829e-04,  2.6125e-04,  3.8929e-06,\n",
       "          4.1657e-05, -4.7371e-04,  1.6288e-04, -3.8591e-05, -1.4927e-04,\n",
       "         -1.6151e-04, -1.8032e-04, -2.0325e-04, -7.2997e-06, -7.5292e-05,\n",
       "         -1.2598e-04,  2.8609e-04,  1.1280e-05,  7.7120e-05, -1.7081e-04,\n",
       "         -2.3510e-04,  1.7002e-05, -1.5665e-04, -2.0004e-04,  2.4666e-05,\n",
       "          4.4360e-04,  2.9569e-04, -3.0828e-04, -9.3922e-05, -7.8325e-04,\n",
       "          1.0500e-04, -5.9120e-06, -4.0056e-04,  2.2728e-04,  1.2284e-05,\n",
       "         -2.4550e-04,  4.6834e-04, -2.7227e-04, -1.8569e-04, -3.3810e-04,\n",
       "          3.9958e-04,  1.1828e-04, -3.9977e-04, -6.0091e-05,  1.4435e-05,\n",
       "         -3.7902e-05,  8.4174e-05,  1.2135e-04, -5.7009e-05,  2.1621e-04,\n",
       "          2.2126e-04,  1.1035e-04, -1.0995e-04,  9.0990e-06,  1.7436e-04,\n",
       "         -7.2832e-05,  4.3390e-05, -1.4441e-04,  6.8554e-05,  9.9886e-05,\n",
       "         -1.5799e-04,  2.3300e-04,  4.8445e-04,  2.7526e-05, -9.8068e-07,\n",
       "          1.8462e-04, -5.0750e-05, -3.0063e-04, -2.5146e-04,  1.8958e-04,\n",
       "          2.4091e-04, -4.9124e-04, -1.6988e-04,  1.9296e-04,  4.2090e-04,\n",
       "         -4.4902e-04, -2.5424e-04,  2.5649e-04,  6.9782e-05,  2.1080e-04,\n",
       "         -4.0468e-05,  2.0940e-04,  3.9242e-05,  3.8893e-05,  2.4768e-04,\n",
       "         -1.0854e-04,  1.3649e-04, -2.7146e-04,  5.2507e-05, -6.6627e-05,\n",
       "          5.8603e-04, -4.7805e-06, -3.1206e-04,  2.4661e-04, -6.6947e-05,\n",
       "         -1.2607e-04, -5.9959e-04]),\n",
       " tensor([-4.4716e-03, -2.8157e-03, -3.9735e-03, -3.4284e-03, -1.6282e-03,\n",
       "         -5.2792e-04,  1.2279e-04, -2.1245e-03, -1.5659e-03, -1.7500e-04,\n",
       "         -2.4579e-03, -4.7885e-03, -1.3006e-03, -1.1449e-03, -2.0570e-03,\n",
       "         -2.5712e-03, -1.6569e-03, -3.1888e-03, -4.4599e-03, -2.1901e-03,\n",
       "         -1.0848e-04, -8.9490e-04,  1.2779e-04, -2.3251e-03, -2.7322e-03,\n",
       "         -2.5686e-03, -1.6469e-04, -3.6089e-03, -2.6367e-03, -2.7884e-03,\n",
       "         -1.1906e-03, -1.5239e-03, -2.3767e-03, -5.0698e-03, -1.3577e-03,\n",
       "         -1.9125e-03, -2.1750e-03, -2.5046e-04,  8.1420e-05, -2.9600e-03,\n",
       "         -2.3586e-04, -3.2946e-03, -4.1224e-03, -2.6719e-03, -1.0448e-03,\n",
       "         -1.0946e-03,  6.7496e-04, -1.9077e-03,  1.3524e-04, -2.4980e-03,\n",
       "         -1.7655e-03, -1.0518e-03, -2.3482e-03, -1.6597e-03, -1.2535e-04,\n",
       "         -1.7529e-03, -3.0160e-05, -1.0105e-03, -2.5626e-03, -2.6734e-03,\n",
       "         -4.7946e-03, -2.8589e-03, -4.0619e-03, -9.1076e-05, -1.2741e-03,\n",
       "         -8.3804e-04, -1.1529e-03, -1.5734e-03, -2.3950e-03, -3.4709e-03,\n",
       "         -2.6037e-03,  1.0604e-04, -1.2228e-03, -1.9125e-03, -2.0623e-03,\n",
       "         -2.3110e-03, -2.5559e-03,  1.0201e-03, -1.4614e-03, -1.5911e-03,\n",
       "         -3.5409e-03, -1.4296e-03, -3.1596e-03, -3.0386e-04, -3.7522e-03,\n",
       "          3.1173e-04, -8.4752e-04, -2.3071e-03, -4.3892e-03, -3.2989e-03,\n",
       "         -1.3829e-03, -1.6680e-03, -3.5036e-04,  4.7618e-04, -2.7981e-03,\n",
       "          1.4561e-04, -9.4271e-04, -1.1243e-03, -3.7342e-03, -2.1881e-03,\n",
       "         -2.5821e-03, -2.5127e-03, -1.4520e-03, -1.5652e-03, -3.1984e-04,\n",
       "         -3.8468e-03, -1.1692e-03, -2.7202e-03, -1.2261e-03, -2.7431e-03,\n",
       "         -3.2799e-03, -3.3402e-03, -2.4043e-03, -1.8411e-03, -5.0889e-03,\n",
       "         -5.6905e-04, -1.0181e-03, -1.2887e-03, -2.1889e-03, -4.2224e-03,\n",
       "         -4.6551e-03, -3.9503e-03, -3.9867e-03, -5.3409e-03, -9.8914e-04,\n",
       "         -9.9266e-04, -2.3389e-03, -3.1726e-03, -2.5777e-03,  9.9897e-05,\n",
       "         -1.0846e-03, -2.3186e-03, -1.9892e-03, -3.7211e-04, -2.3745e-03,\n",
       "         -2.2143e-03, -2.8182e-03,  6.9141e-06, -1.1660e-03, -2.3257e-03,\n",
       "         -3.1168e-03, -2.1730e-03, -3.6302e-03, -9.9063e-05, -2.2023e-03,\n",
       "          3.3212e-04, -1.8296e-03, -1.0538e-04, -2.0224e-03, -1.3252e-03,\n",
       "         -1.4518e-03,  2.5558e-04, -4.9663e-03, -1.5942e-03,  4.3470e-04,\n",
       "         -1.6433e-03, -1.9140e-03,  1.1615e-03, -1.2359e-03, -7.1466e-04,\n",
       "         -1.5551e-03, -1.8719e-03, -2.5600e-03, -2.3260e-03, -9.9349e-04,\n",
       "          1.9751e-03,  2.3127e-05, -2.0372e-03, -2.2138e-03, -4.0585e-04,\n",
       "          1.5736e-05, -1.3849e-03, -7.5698e-06, -4.0890e-03, -9.6989e-04,\n",
       "          1.6339e-03, -1.4149e-03, -8.3685e-05, -2.1647e-03, -5.8812e-04,\n",
       "         -1.2577e-04, -1.9670e-03, -2.1646e-03, -2.4177e-03, -5.9962e-04,\n",
       "         -5.4961e-04, -2.7852e-03, -1.0709e-03, -2.0117e-03, -1.4248e-03,\n",
       "         -5.3895e-03, -8.9669e-04, -2.1672e-03, -8.4758e-04, -4.1936e-03,\n",
       "         -1.4687e-03,  1.5378e-04, -4.4409e-03, -4.7719e-04, -6.6924e-04,\n",
       "         -4.3572e-03, -3.2691e-03, -3.8304e-03, -1.4357e-03, -3.8887e-03,\n",
       "         -6.1889e-03, -1.5680e-03, -1.0176e-03, -3.0841e-03, -4.0019e-03,\n",
       "         -3.3256e-03, -2.6445e-03, -1.4059e-03, -7.7879e-04, -9.0238e-04,\n",
       "          4.3535e-04,  9.7537e-04, -3.7931e-03, -1.6229e-03,  2.5910e-04,\n",
       "         -1.3456e-03, -2.2124e-03, -2.9307e-03, -1.5912e-03, -4.4771e-03,\n",
       "         -2.5384e-03, -5.7065e-04, -1.1990e-03, -1.1581e-03, -2.4924e-03,\n",
       "         -1.7344e-03, -6.8319e-04, -2.0042e-03, -9.7680e-04, -1.0656e-03,\n",
       "         -8.1456e-04, -2.1329e-03, -1.3528e-03, -1.0079e-03, -2.7363e-03,\n",
       "          8.3828e-04, -3.4910e-03, -7.0533e-04, -3.9656e-03, -1.7110e-03,\n",
       "         -2.2095e-04, -1.1937e-03, -3.8110e-03,  7.0608e-04, -2.2534e-03,\n",
       "         -5.7745e-04, -3.4897e-03, -1.2614e-03, -2.9855e-03, -2.1826e-03,\n",
       "          2.6762e-04, -2.1014e-03, -1.0701e-03, -2.8133e-05, -1.0946e-03,\n",
       "         -1.6010e-03, -5.9889e-03, -4.4966e-04, -1.8333e-03, -1.9242e-03,\n",
       "         -2.7865e-03,  1.8560e-03, -2.2870e-04, -3.8121e-03, -1.6814e-04,\n",
       "         -2.6664e-03,  6.9153e-04, -3.4333e-03, -1.9903e-03, -2.3249e-03,\n",
       "         -6.0296e-04,  6.0540e-04, -2.2460e-03,  1.6212e-05, -6.6173e-04,\n",
       "         -1.6556e-03, -2.9536e-03, -2.0748e-04, -4.4824e-03, -1.1644e-03,\n",
       "         -4.0485e-03, -3.1412e-04, -6.6167e-04, -2.0576e-03, -5.1916e-05,\n",
       "         -1.8035e-03, -4.1260e-03, -5.4598e-03, -3.2670e-03, -2.8444e-03,\n",
       "         -3.7776e-03, -3.4008e-03, -1.0285e-03, -1.3257e-03,  9.7036e-05,\n",
       "         -1.8477e-03,  2.7537e-04,  4.8900e-04, -1.8550e-03, -2.8594e-03,\n",
       "         -6.0052e-04, -2.9802e-06, -4.7267e-03, -2.5283e-03, -1.9292e-03,\n",
       "          1.6877e-03, -5.4497e-04, -3.1829e-05, -8.0943e-04, -1.6923e-03,\n",
       "         -1.0955e-04, -2.8187e-03, -1.4247e-03, -2.0651e-03, -2.1197e-03,\n",
       "         -2.0187e-03, -1.5274e-03, -3.4761e-04, -1.7216e-03,  4.4191e-04,\n",
       "         -1.6243e-03, -3.3381e-03, -4.0162e-03, -1.7292e-03,  1.1706e-04,\n",
       "         -4.8429e-04, -2.4723e-03, -1.9208e-03, -2.0301e-04, -4.1461e-03,\n",
       "         -2.4732e-03, -1.7456e-03, -5.2180e-03, -2.1583e-03,  1.6186e-03,\n",
       "         -7.4500e-04, -2.1821e-03, -4.0293e-04, -4.4486e-03, -7.4446e-05,\n",
       "         -1.4881e-03, -2.3360e-03, -3.4412e-03, -3.4604e-03, -9.1165e-04,\n",
       "         -1.7963e-03, -3.2895e-03, -6.3145e-04, -1.4047e-03,  4.5419e-05,\n",
       "         -8.2570e-04, -3.7742e-03, -5.7960e-04, -4.0693e-03, -1.0811e-03,\n",
       "         -2.2683e-03,  2.3879e-03, -3.7006e-03,  2.3997e-04, -4.1494e-03,\n",
       "         -1.2194e-03, -3.3720e-03,  1.8311e-04, -2.6510e-03, -3.4018e-03,\n",
       "         -3.1568e-03, -4.9484e-04,  2.4879e-04,  3.4171e-04, -2.2987e-03,\n",
       "         -2.1551e-03, -1.1845e-03, -2.6760e-03, -8.3864e-05, -2.4002e-03,\n",
       "         -3.6754e-03, -4.1276e-03, -2.5128e-03, -3.2468e-03, -1.2529e-03,\n",
       "         -2.0028e-03,  1.3317e-03, -3.4370e-03, -1.1090e-03,  9.1976e-04,\n",
       "         -3.0558e-03, -1.8777e-03, -3.0191e-03, -3.4946e-03, -4.5413e-03,\n",
       "         -4.0227e-03, -1.0490e-03, -5.6392e-04, -1.0328e-03,  9.2518e-04,\n",
       "         -2.1262e-03, -2.9179e-03, -1.9067e-03,  6.7770e-05, -1.1038e-03,\n",
       "          7.7623e-04, -2.6476e-03, -3.0512e-04, -2.1172e-03, -1.1295e-03,\n",
       "         -3.3329e-03, -3.0711e-03, -2.1611e-03, -2.9091e-03, -6.0487e-04,\n",
       "         -9.9981e-04,  1.1042e-03, -9.5469e-04, -7.6270e-04, -3.7094e-03,\n",
       "         -2.3518e-03, -4.0955e-03, -1.7060e-03, -1.1041e-03, -8.3560e-04,\n",
       "         -4.2616e-03, -3.3029e-03,  5.8192e-04, -7.1889e-04, -1.4483e-03,\n",
       "         -4.4249e-03, -2.8187e-03,  3.4457e-04, -4.1595e-03, -3.7185e-03,\n",
       "         -2.8253e-03, -3.7533e-04, -3.2545e-03, -1.4852e-03,  8.1718e-05,\n",
       "         -3.2674e-03, -3.9232e-03, -2.4505e-03, -2.2691e-04, -3.3144e-03,\n",
       "         -1.5291e-03, -1.1739e-03, -3.4785e-03, -2.1555e-03, -1.8434e-03,\n",
       "          8.2159e-04,  7.1865e-04, -3.2302e-03,  1.1057e-04, -2.7476e-03,\n",
       "          7.7486e-05, -4.4310e-04,  7.0858e-04, -1.2891e-03, -6.3097e-04,\n",
       "         -9.4736e-04, -1.3924e-03, -2.0576e-03, -1.3701e-03, -2.2784e-03,\n",
       "         -8.0544e-04, -1.5647e-03, -1.2652e-03, -6.0076e-04, -2.6894e-04,\n",
       "         -1.8309e-03, -1.4130e-03, -1.1871e-03, -2.3100e-03, -1.2308e-04,\n",
       "         -1.0875e-03, -4.0028e-03, -9.5433e-04, -2.2116e-03, -8.9723e-04,\n",
       "         -7.7313e-04, -2.2002e-03, -4.8518e-04, -1.8746e-03, -8.4597e-04,\n",
       "         -3.4155e-03,  7.4250e-04, -8.2225e-04, -2.2851e-03, -4.5812e-04,\n",
       "         -4.5738e-04, -1.1920e-03, -1.3792e-03, -2.1358e-03, -3.3603e-03,\n",
       "         -1.2498e-03, -8.3327e-04, -2.5056e-03,  5.6356e-04, -3.4267e-03,\n",
       "         -3.6455e-03, -5.2357e-04, -3.2215e-03, -1.8319e-03,  3.1340e-04,\n",
       "          1.6086e-03, -1.4544e-03, -2.2774e-03, -1.4039e-03, -2.0877e-03,\n",
       "         -1.9370e-03, -3.5781e-03]),\n",
       " tensor([ 1.4333e-04,  5.8818e-05,  3.8363e-05, -6.5755e-05,  2.5687e-04,\n",
       "         -7.3895e-05,  1.5015e-04,  1.5993e-05,  4.4887e-04,  6.4420e-04,\n",
       "         -1.2038e-04,  2.5182e-04, -1.5242e-04, -5.4018e-04, -3.5118e-05,\n",
       "         -4.4542e-04,  4.4277e-04, -2.4961e-04, -6.2566e-05,  1.1143e-04,\n",
       "         -2.1178e-04, -2.2522e-04,  2.5976e-05, -3.5789e-04,  9.2186e-05,\n",
       "          4.8175e-04, -3.8308e-04, -1.3841e-04, -6.1521e-04,  2.7318e-04,\n",
       "          8.5175e-04, -1.2231e-04, -2.6778e-04, -9.3658e-05,  1.3050e-04,\n",
       "          4.1315e-04, -2.3102e-04, -1.4320e-04,  5.5384e-04, -6.0458e-05,\n",
       "         -6.1013e-05,  3.8080e-05, -2.5344e-04,  7.1239e-04,  3.4351e-05,\n",
       "         -1.6879e-04, -1.2051e-04,  7.6673e-04, -3.9682e-04, -3.8382e-05,\n",
       "          3.1966e-04, -1.9104e-05, -2.3836e-04,  3.0207e-04,  4.6222e-04,\n",
       "          1.2667e-04,  3.8399e-04, -6.1471e-05, -3.1846e-04,  2.9228e-04,\n",
       "          1.8760e-04, -3.1747e-04, -1.0628e-05,  9.4127e-05, -1.6946e-04,\n",
       "          1.1519e-04, -2.2390e-04,  2.4966e-04, -3.5127e-04,  5.3945e-04,\n",
       "          1.2440e-04, -9.0636e-05, -3.3232e-04, -1.7107e-05,  1.3087e-04,\n",
       "          6.0800e-04,  2.3173e-04,  3.4397e-05, -3.6289e-04,  3.8008e-04,\n",
       "         -7.3083e-05,  1.3268e-04,  6.5826e-06,  5.7068e-05,  1.9869e-04,\n",
       "          1.8448e-05,  2.0438e-04,  3.7042e-04,  2.4067e-04, -4.5874e-04,\n",
       "         -1.5838e-04,  1.2063e-04,  2.2255e-05,  7.2051e-05, -2.7798e-04,\n",
       "          1.4291e-04, -1.2070e-06, -1.0690e-04, -7.8071e-05, -3.2505e-04,\n",
       "         -3.1742e-04, -1.3358e-04,  2.6251e-04, -9.4514e-05,  6.9644e-06,\n",
       "          9.6199e-05,  1.7431e-04, -9.3598e-05, -9.0098e-04, -1.4519e-04,\n",
       "          3.8730e-04,  2.6524e-04, -3.2629e-04, -1.3306e-04, -8.7591e-05,\n",
       "         -4.8161e-05,  1.2412e-03,  1.5010e-04, -9.3553e-05,  3.2649e-05,\n",
       "         -9.6075e-05, -4.6707e-04,  4.7566e-04,  3.6898e-04,  2.6427e-04,\n",
       "         -2.2573e-05, -1.8937e-04,  5.0005e-05,  6.2947e-05,  1.3594e-04,\n",
       "          1.8131e-05,  7.5594e-05,  1.9209e-04, -1.7053e-04,  1.3594e-04,\n",
       "         -3.0235e-04,  3.0840e-04,  2.5174e-06,  1.2403e-05,  5.2463e-05,\n",
       "         -1.8120e-05,  1.9480e-04,  2.9244e-04, -3.7527e-04, -4.0866e-05,\n",
       "         -7.2353e-05,  2.1670e-04, -6.7019e-05, -6.6284e-05,  4.0718e-04,\n",
       "          3.4811e-04, -8.6974e-05, -1.6811e-04,  6.9758e-04,  6.1710e-04,\n",
       "          9.8644e-05,  4.3145e-04, -3.6776e-04,  4.7631e-04,  2.9787e-04,\n",
       "          7.6112e-05,  3.9637e-04, -7.1368e-04, -9.3404e-05, -1.5652e-05,\n",
       "         -3.1688e-06, -5.7358e-04, -5.3120e-04, -2.0422e-04,  1.3502e-05,\n",
       "          1.0418e-04,  4.8948e-05, -6.5402e-04, -9.9037e-05,  1.1373e-04,\n",
       "          6.1428e-04, -8.1051e-05,  5.0710e-04,  1.9327e-04, -2.0426e-05,\n",
       "         -1.5101e-05, -2.8637e-04,  7.9192e-05,  1.9358e-04, -4.1332e-06,\n",
       "         -1.6235e-04, -8.0483e-05, -9.1636e-05,  4.6987e-05,  1.0539e-04,\n",
       "          2.9609e-04, -3.3835e-04,  3.0120e-04, -2.5165e-04, -3.7909e-04,\n",
       "          2.5364e-04, -6.4114e-04, -5.0908e-04, -3.9246e-04,  2.8871e-04,\n",
       "          5.8502e-05,  2.8012e-04,  2.5438e-05, -6.0941e-05,  5.7615e-04,\n",
       "          5.3462e-05,  4.4433e-05, -6.9197e-04,  9.2632e-04, -4.4928e-04,\n",
       "         -3.6963e-04,  6.0117e-04,  2.7869e-05,  1.1977e-06, -2.6307e-04,\n",
       "          8.1204e-04,  1.3779e-04,  3.5842e-04,  3.0558e-04,  3.9873e-04,\n",
       "         -8.9947e-06, -4.6055e-04, -1.1661e-04, -6.5931e-05,  1.1954e-06,\n",
       "         -4.6965e-04,  2.3579e-04,  1.7367e-04,  1.7651e-04, -4.7840e-05,\n",
       "          5.9675e-04, -1.1891e-04, -1.9680e-04, -2.8474e-05,  4.2408e-04,\n",
       "          1.0014e-04,  2.4294e-04,  7.2600e-05,  5.6008e-04,  2.0005e-05,\n",
       "         -1.5218e-04, -5.0675e-04,  3.2819e-04,  6.2879e-05,  5.1552e-04,\n",
       "          5.7703e-04, -5.2240e-04, -8.7034e-05, -7.1099e-04,  3.3731e-04,\n",
       "          1.2146e-04,  3.1044e-04,  3.9101e-04,  2.1292e-04,  3.6100e-04,\n",
       "          1.8194e-04, -2.5429e-04,  3.0258e-04, -2.4235e-04, -7.1645e-04,\n",
       "          4.7166e-04,  2.3281e-04, -6.0728e-04, -8.5143e-05, -3.7999e-04,\n",
       "          1.3423e-04, -1.3711e-04,  4.3635e-05, -5.8014e-05,  3.7839e-04,\n",
       "          1.4585e-04,  1.2550e-04, -1.1905e-04, -2.8259e-04, -3.7222e-04,\n",
       "          2.3010e-04, -3.1352e-04,  7.4215e-05,  4.9097e-04,  4.6910e-04,\n",
       "          2.7635e-04, -6.3352e-05, -4.7782e-04, -6.5836e-05,  1.7233e-04,\n",
       "         -2.6916e-04, -3.3956e-04, -1.4296e-04,  2.2235e-04, -4.0330e-05,\n",
       "          1.4951e-04, -5.7027e-05, -1.8476e-04,  3.5048e-05, -1.0260e-04,\n",
       "         -1.9315e-04,  8.4816e-05,  7.5502e-05, -1.9388e-04,  3.9228e-04,\n",
       "          2.2626e-04, -4.4312e-04, -5.5909e-04, -3.7045e-04,  2.3164e-04,\n",
       "         -6.4945e-04,  9.3766e-06, -1.2780e-04,  1.2219e-05, -1.7084e-04,\n",
       "         -2.2161e-04, -3.6874e-04,  2.5227e-04, -2.5950e-04, -1.1516e-04,\n",
       "          1.9769e-04,  8.3093e-05, -1.0106e-04,  8.7038e-05,  1.7250e-04,\n",
       "          1.6360e-04,  2.1417e-04, -1.7504e-04,  5.3068e-04,  9.6061e-05,\n",
       "          1.1307e-04,  2.9864e-05, -2.4479e-04,  1.8694e-04,  3.1114e-05,\n",
       "         -6.5673e-05,  2.9931e-05, -6.5265e-06,  2.7593e-05, -2.0920e-04,\n",
       "         -6.3345e-05,  1.2350e-04, -1.7460e-04,  1.6038e-04,  4.1284e-05,\n",
       "         -4.5374e-06,  7.0736e-05, -1.3672e-04,  5.9094e-05,  6.7849e-04,\n",
       "          2.4024e-04,  1.2435e-04,  3.3795e-04,  9.5656e-06,  1.9435e-04,\n",
       "          1.9819e-06,  1.4988e-04, -4.4766e-04, -3.5650e-04,  1.7331e-04,\n",
       "         -2.0471e-04, -3.4033e-04,  7.5687e-05,  6.5465e-04, -8.9014e-05,\n",
       "         -4.9070e-05, -3.8947e-04,  1.2640e-04, -3.8609e-05,  1.7255e-04,\n",
       "          1.5884e-04,  1.8920e-04, -1.4164e-05,  3.3350e-04,  1.5643e-04,\n",
       "          4.3685e-04, -1.7027e-04, -1.5832e-07,  2.6640e-04, -5.4330e-05,\n",
       "         -4.0655e-05,  5.7159e-05, -3.4106e-04, -3.0351e-04,  2.6129e-04,\n",
       "         -5.5518e-04,  4.6748e-04,  3.3704e-04,  2.7675e-04, -3.7951e-07,\n",
       "         -6.1911e-05,  8.1094e-04, -1.4282e-04, -5.6468e-05,  3.4533e-05,\n",
       "          4.3897e-05,  3.1693e-04,  9.3628e-05,  3.0204e-04, -6.5451e-05,\n",
       "         -3.9198e-05, -5.9158e-06,  3.4960e-04,  2.0757e-05,  6.6943e-06,\n",
       "          9.2428e-05,  7.3943e-05,  2.1547e-04,  2.1874e-04,  1.6268e-04,\n",
       "          3.4621e-04,  1.3628e-04,  2.4501e-05, -1.4407e-04, -2.0433e-04,\n",
       "          3.5477e-04, -1.1565e-04,  1.6825e-04, -8.9773e-05, -1.5897e-04,\n",
       "          2.1226e-04,  8.4292e-05,  8.5104e-06,  1.3198e-04, -3.7089e-04,\n",
       "         -1.4282e-04, -3.0944e-04,  4.1983e-04, -5.5532e-04, -1.0201e-04,\n",
       "         -1.9923e-04,  3.7303e-04, -1.8008e-05,  9.4690e-04,  4.3416e-04,\n",
       "         -2.2205e-04, -5.9815e-05, -1.2079e-04, -2.6115e-04,  2.0769e-05,\n",
       "          2.1975e-04, -2.8007e-04,  3.2662e-04,  3.9705e-05, -1.7389e-04,\n",
       "          1.4671e-04,  1.0730e-05, -1.5016e-04, -7.8168e-05, -7.8291e-05,\n",
       "          5.0815e-05,  2.1115e-05,  8.8482e-05,  3.7083e-04, -7.3802e-04,\n",
       "         -6.1560e-04,  1.8613e-04, -1.6687e-04, -6.8564e-06,  4.0160e-05,\n",
       "         -9.1836e-05, -5.4961e-05, -2.0583e-04,  2.6822e-06,  2.3476e-04,\n",
       "         -2.9579e-06,  2.0018e-04,  4.9276e-05,  1.1568e-04,  2.8970e-04,\n",
       "          2.2943e-04,  2.8331e-04, -1.8387e-05,  3.3083e-04, -3.3429e-04,\n",
       "          2.2237e-04,  4.6224e-04,  2.5965e-05,  2.0891e-05,  7.5254e-05,\n",
       "         -4.3005e-05, -2.3729e-04, -1.9049e-04, -1.4025e-04, -2.3474e-04,\n",
       "         -3.7760e-04,  5.1742e-04, -5.1438e-04,  1.4537e-04,  1.3853e-04,\n",
       "         -2.8580e-05,  1.7014e-04, -6.5555e-04,  2.0913e-04,  1.8788e-04,\n",
       "          3.3148e-04, -1.7325e-04,  1.2763e-05,  2.2865e-04,  1.5166e-04,\n",
       "         -2.7510e-04, -2.3350e-04, -2.7516e-04,  5.3838e-05,  2.2147e-04,\n",
       "          4.8905e-05,  3.8479e-04,  4.5231e-05,  2.2009e-05,  4.4410e-04,\n",
       "         -2.2182e-04, -1.1728e-04, -2.0630e-04,  1.0550e-04,  2.1667e-04,\n",
       "          1.0836e-04, -1.5283e-04]),\n",
       " tensor([[-1.0590e-03, -3.2558e-03,  4.8277e-03,  ...,  2.9924e-03,\n",
       "          -4.5990e-03, -1.6479e-03],\n",
       "         [-1.1552e-03,  5.1382e-04,  2.5457e-03,  ..., -5.1259e-04,\n",
       "           3.9897e-03,  1.4853e-03],\n",
       "         [ 1.1443e-03,  8.1084e-04, -5.0298e-03,  ..., -3.2137e-03,\n",
       "           1.4894e-03, -2.6008e-03],\n",
       "         ...,\n",
       "         [ 1.6878e-04,  6.7338e-04, -2.6291e-03,  ..., -1.1950e-03,\n",
       "           1.5614e-03,  8.8538e-04],\n",
       "         [-1.0611e-03, -4.8727e-06,  6.8542e-04,  ...,  2.3838e-04,\n",
       "          -5.2612e-04, -7.5739e-04],\n",
       "         [ 1.2192e-03, -1.1857e-03,  4.5212e-04,  ..., -4.7085e-04,\n",
       "          -1.1476e-03,  1.6179e-03]]),\n",
       " tensor([-9.0347e-04,  1.0460e-03,  1.2201e-03,  ...,  1.6054e-04,\n",
       "          8.5426e-05,  1.4176e-04]),\n",
       " tensor([[ 1.1886e-03, -2.5961e-03,  2.8317e-03,  ..., -2.6772e-04,\n",
       "           2.9485e-03, -1.8921e-03],\n",
       "         [-2.5583e-03, -2.9985e-03,  4.7988e-03,  ..., -2.9266e-03,\n",
       "           1.5599e-03, -2.9037e-03],\n",
       "         [ 9.6321e-05, -1.1627e-03, -9.2252e-04,  ...,  3.3988e-03,\n",
       "          -1.1298e-03,  8.4493e-04],\n",
       "         ...,\n",
       "         [-9.9733e-05, -2.5954e-03,  1.7596e-03,  ...,  1.3799e-03,\n",
       "          -5.2825e-04,  1.7597e-03],\n",
       "         [-2.4754e-03,  2.9997e-03, -1.1491e-03,  ..., -1.3569e-03,\n",
       "          -3.1089e-03,  6.3296e-05],\n",
       "         [ 5.9078e-04,  2.8156e-03, -1.4048e-03,  ...,  8.2038e-05,\n",
       "           1.1537e-03,  2.4916e-04]]),\n",
       " tensor([-2.5351e-04,  2.5545e-05, -1.5618e-04,  2.7033e-04, -4.0270e-04,\n",
       "          4.4211e-04,  1.5275e-04, -7.3621e-05, -8.7361e-05, -2.7250e-04,\n",
       "          1.5173e-04, -1.0984e-04,  4.4468e-04, -4.2715e-04,  4.1824e-04,\n",
       "          3.0598e-05, -4.4392e-04, -2.5908e-04,  1.4880e-04,  1.5347e-04,\n",
       "         -5.1604e-04,  3.2331e-04,  1.8563e-04,  4.3085e-04, -5.2582e-05,\n",
       "         -2.5959e-04,  1.9193e-04,  3.5100e-04, -1.0987e-04,  1.6491e-04,\n",
       "         -2.4646e-04,  1.4215e-04, -2.9845e-05, -5.0874e-04, -3.7017e-04,\n",
       "          1.1599e-04,  1.5847e-04, -8.8488e-05,  1.0064e-05, -4.8786e-04,\n",
       "         -1.8894e-04, -1.2064e-04,  1.1981e-04, -1.2974e-04, -2.6857e-04,\n",
       "          2.1576e-04,  3.1444e-05, -2.2025e-04,  2.1140e-04,  2.1691e-04,\n",
       "         -1.8848e-04, -1.0181e-04, -9.1538e-05, -9.0218e-05,  2.0166e-05,\n",
       "         -5.1528e-04,  2.0069e-04, -5.2632e-04, -1.7254e-04, -6.9465e-04,\n",
       "          4.2524e-05,  5.9891e-04, -1.6810e-04, -2.0679e-04,  2.2885e-04,\n",
       "         -3.1513e-04, -1.4843e-05, -1.8593e-04,  1.9608e-04, -6.6547e-04,\n",
       "          3.0044e-05, -2.0133e-04,  5.9782e-04, -2.7336e-04,  3.4200e-05,\n",
       "         -3.8911e-04,  7.0058e-05,  3.1685e-04,  3.5817e-04, -1.7212e-04,\n",
       "          3.8559e-04, -1.5461e-04,  6.4506e-05, -2.3196e-04, -2.5924e-05,\n",
       "          3.6994e-05,  9.5742e-05,  5.6523e-05, -2.6970e-04, -2.0122e-04,\n",
       "          6.6819e-05, -2.8724e-04,  3.2108e-04, -3.1710e-04,  2.0894e-04,\n",
       "         -5.2308e-04, -4.3976e-05,  3.2384e-05, -5.5742e-04,  1.1641e-04,\n",
       "          3.9791e-04,  2.7560e-04, -2.6662e-04, -9.3048e-05, -4.4583e-04,\n",
       "         -2.2588e-04, -2.5173e-04,  3.5356e-04, -2.2701e-04, -3.4791e-04,\n",
       "         -4.3736e-04, -1.9371e-04, -3.5374e-05, -5.6482e-04, -1.9236e-04,\n",
       "          9.0996e-05, -5.6352e-05,  2.6134e-04,  7.6221e-05, -2.4896e-04,\n",
       "         -1.4384e-05,  1.1708e-05, -3.0546e-04, -6.7179e-04, -2.4952e-04,\n",
       "         -2.3937e-04,  2.8916e-04,  5.4368e-04, -4.8519e-05,  1.5034e-04,\n",
       "          8.8004e-05, -2.6574e-05, -3.6989e-04,  1.3935e-04, -2.8604e-04,\n",
       "          1.8858e-04,  3.4473e-04, -2.9897e-05,  1.5051e-05, -1.2595e-04,\n",
       "         -1.3144e-04, -4.0729e-04, -2.6490e-04, -1.8511e-05, -1.8593e-04,\n",
       "         -2.8468e-04,  2.9009e-04, -1.2890e-04, -3.7943e-05, -2.4378e-04,\n",
       "         -1.9077e-04,  1.4743e-06, -9.4962e-05, -5.6063e-04, -1.1720e-04,\n",
       "         -1.6102e-04, -1.7203e-04,  7.0775e-05,  1.8103e-05,  2.5908e-05,\n",
       "          5.6853e-05, -4.5259e-04,  6.8067e-04,  4.4601e-05, -3.6711e-04,\n",
       "         -6.3404e-05,  4.5997e-04, -1.4745e-04,  6.2555e-04,  7.7063e-04,\n",
       "          1.7460e-04, -3.9814e-04,  2.6589e-04,  2.5414e-04,  4.1256e-04,\n",
       "         -1.7315e-04,  2.3757e-04,  1.8225e-04, -3.7794e-04, -2.0703e-04,\n",
       "         -5.9121e-05,  3.6338e-04,  1.5548e-04,  4.2264e-04, -3.6769e-05,\n",
       "          6.6929e-04,  1.3328e-04,  3.2826e-04, -1.7893e-05, -8.2676e-05,\n",
       "         -3.6010e-04, -1.2760e-04,  1.3811e-04, -8.8273e-05,  1.1590e-04,\n",
       "          9.8067e-05,  7.5617e-04,  1.8434e-04,  5.7727e-04, -1.0186e-04,\n",
       "          1.7818e-04,  3.8663e-05,  5.7858e-04, -7.8683e-05, -2.7173e-04,\n",
       "         -4.2841e-04,  1.0268e-04,  5.3945e-04, -2.0051e-04, -4.3175e-04,\n",
       "          3.4412e-04, -1.7890e-04,  7.8667e-05, -3.2312e-04,  1.0844e-04,\n",
       "         -2.1642e-04, -3.1079e-04,  1.4348e-04, -2.6294e-04,  1.7597e-04,\n",
       "         -5.2915e-05, -2.5271e-04,  1.0961e-04,  3.4786e-04, -8.0526e-05,\n",
       "         -3.3182e-04, -4.4748e-04, -5.2846e-04, -4.5119e-04, -1.9031e-04,\n",
       "         -3.7273e-04,  7.6551e-05,  3.5645e-04,  1.8979e-04, -2.4304e-04,\n",
       "         -1.6335e-04,  9.8070e-05,  2.4643e-04, -3.8003e-05,  1.7659e-04,\n",
       "          8.1075e-04,  6.2282e-04,  1.5799e-04,  1.2270e-04, -6.3196e-04,\n",
       "         -1.8879e-04, -1.1303e-04, -8.8003e-05,  1.0181e-04, -6.5803e-05,\n",
       "          2.6752e-04, -1.4711e-04,  2.0499e-04, -1.1193e-05,  5.1497e-05,\n",
       "         -4.2118e-05, -1.6124e-04, -4.3830e-04,  4.9789e-05, -1.1108e-04,\n",
       "         -1.1309e-05, -1.4280e-04,  2.9267e-04, -2.8702e-04,  2.3630e-04,\n",
       "         -1.5716e-04, -3.8769e-04, -3.5555e-04,  8.8097e-05,  1.2099e-04,\n",
       "          4.6324e-05, -5.0617e-05, -4.5812e-04, -1.9264e-05,  3.3477e-04,\n",
       "         -3.5175e-04,  8.4096e-04, -2.0826e-05, -1.0624e-04, -4.2254e-04,\n",
       "         -1.1364e-04,  1.9661e-04,  6.7105e-04,  1.2632e-04,  3.4464e-04,\n",
       "          5.1706e-04,  1.0460e-04, -9.5734e-05, -1.8800e-05,  2.6769e-04,\n",
       "         -1.9606e-04,  1.8957e-04, -2.7374e-04,  2.3905e-04, -1.4653e-04,\n",
       "          1.0091e-04,  5.6383e-04,  5.8440e-04,  5.3370e-05, -1.6550e-04,\n",
       "         -5.1289e-05, -1.2403e-04,  1.4098e-05, -8.9700e-05, -5.1560e-04,\n",
       "          6.6995e-04, -2.9571e-04, -1.6249e-04,  1.7552e-05, -1.7139e-04,\n",
       "          2.6685e-04, -3.7395e-04, -1.6862e-04, -2.7834e-05,  6.8696e-05,\n",
       "          4.5465e-05,  8.5352e-05, -5.7511e-05,  2.8376e-05,  3.5387e-05,\n",
       "          1.1609e-04,  2.6816e-04,  7.6684e-05, -4.9105e-04,  2.4489e-04,\n",
       "         -1.1457e-04,  3.2307e-05, -2.2586e-04,  9.8846e-05,  3.4077e-05,\n",
       "          1.2016e-04, -2.0263e-04,  3.5174e-05, -7.6666e-06,  6.5487e-04,\n",
       "         -1.3575e-04, -3.6626e-04,  4.4419e-04, -3.2945e-04,  1.4434e-04,\n",
       "         -6.1064e-04,  2.4838e-04,  3.9007e-04, -3.8344e-04, -4.5951e-04,\n",
       "         -2.7109e-04,  4.9876e-05, -8.1393e-05,  1.9619e-04,  1.1411e-04,\n",
       "         -1.1991e-05, -2.0773e-04,  1.2180e-04, -8.3954e-05, -1.8044e-04,\n",
       "          3.3281e-04,  1.5646e-04, -9.1476e-05,  1.6006e-04,  2.3293e-04,\n",
       "         -2.4436e-04,  3.0706e-04, -1.4879e-04, -4.2272e-05, -4.1523e-04,\n",
       "          1.2962e-05,  2.8051e-04,  1.6559e-05, -1.0631e-04,  3.3423e-05,\n",
       "          3.7173e-04, -1.6516e-04, -4.9630e-04, -8.8869e-05,  5.1931e-05,\n",
       "         -2.9077e-04,  1.0116e-04,  5.2018e-05,  1.8351e-04,  3.8039e-04,\n",
       "          3.2357e-04,  3.0916e-04,  1.3069e-04,  8.6317e-05, -5.1126e-04,\n",
       "          1.9203e-04, -3.0174e-04, -2.7445e-05,  1.1046e-05, -2.5099e-05,\n",
       "         -1.3446e-04, -3.1367e-04, -1.6931e-04, -4.5875e-05, -7.2923e-07,\n",
       "         -5.3087e-05,  1.6087e-04, -1.2312e-04,  4.8833e-05,  1.7292e-04,\n",
       "          3.3091e-04,  3.2912e-04,  2.7911e-04, -3.0095e-04,  4.7207e-05,\n",
       "         -3.7871e-04,  2.7508e-04, -1.9101e-04,  1.1271e-05, -2.0765e-04,\n",
       "          1.0159e-04, -4.2013e-05, -3.2135e-04, -2.3935e-04, -1.4565e-05,\n",
       "         -6.2224e-05,  1.4544e-04, -1.8222e-04,  2.0495e-04,  1.5935e-04,\n",
       "          1.7727e-04, -3.3805e-04,  1.0478e-05,  1.8020e-04, -9.1106e-05,\n",
       "          7.4325e-05, -2.8436e-04, -1.8417e-04, -4.0608e-04, -2.8891e-04,\n",
       "          6.2425e-05,  2.3562e-04, -1.3863e-05,  4.7420e-04, -1.9415e-04,\n",
       "         -4.7002e-04,  3.4535e-04, -4.5960e-04, -2.7630e-04,  1.4629e-04,\n",
       "          3.8549e-04,  2.5599e-04, -1.6563e-04, -5.8115e-05, -7.2262e-04,\n",
       "          8.8543e-05, -1.3209e-04, -5.7823e-04,  7.9010e-05,  2.4962e-04,\n",
       "          2.5271e-03,  2.5539e-04, -2.7820e-04, -2.2793e-04, -3.3616e-04,\n",
       "          5.2320e-04,  1.7449e-04, -2.3969e-04,  5.2266e-06, -1.6425e-04,\n",
       "          1.1344e-04, -9.1928e-05,  6.5982e-05, -2.1349e-04,  1.4965e-04,\n",
       "          2.4872e-04,  1.2830e-05, -1.1354e-04, -1.5471e-04,  3.0340e-04,\n",
       "         -2.4701e-04, -2.0066e-04, -1.8538e-04,  7.3839e-05,  7.8629e-05,\n",
       "         -8.4818e-05,  4.0750e-04,  7.4464e-04,  8.7636e-05,  1.0548e-04,\n",
       "          1.9003e-04, -1.8617e-04,  1.0773e-04, -2.8759e-04,  8.0565e-05,\n",
       "          2.0537e-04, -4.1345e-04,  2.9072e-05, -3.5414e-05,  4.2148e-04,\n",
       "         -3.7879e-04, -2.0209e-04,  1.9040e-04, -4.0940e-05,  1.6706e-04,\n",
       "          1.4458e-04,  4.0104e-04,  2.4245e-04,  4.3247e-05,  1.7244e-04,\n",
       "         -1.2629e-04, -2.0144e-04, -2.9560e-04,  7.4971e-05, -1.0458e-04,\n",
       "          3.8985e-04, -5.4532e-05, -1.2174e-04,  1.7623e-04, -2.0896e-04,\n",
       "         -2.4191e-04, -6.0177e-04]),\n",
       " tensor([ 3.7324e-04,  2.0242e-04,  2.7442e-04, -6.9308e-04, -2.0818e-03,\n",
       "          8.0359e-04,  2.7009e-03,  2.1687e-03, -1.7072e-03, -1.2168e-03,\n",
       "         -2.3919e-04, -8.7583e-04,  2.6668e-03, -6.2585e-04,  2.3246e-03,\n",
       "         -2.2885e-03,  8.3864e-04,  1.3947e-04,  9.0599e-06, -4.5919e-04,\n",
       "          2.7049e-03, -1.1719e-03,  2.1783e-03,  3.0959e-03, -2.3167e-03,\n",
       "          9.5165e-04,  1.1247e-03,  1.9395e-04,  9.4318e-04,  1.1619e-03,\n",
       "         -3.2476e-03,  7.2908e-04,  2.5868e-05, -7.4017e-04,  1.2431e-03,\n",
       "          9.4295e-04,  1.0737e-03,  2.7090e-04,  9.5356e-04, -8.9598e-04,\n",
       "         -9.8288e-04,  1.5846e-03, -5.8734e-04, -1.2372e-03, -4.2987e-04,\n",
       "          1.4267e-03,  9.7644e-04, -1.4732e-03,  1.5858e-03,  2.5803e-04,\n",
       "          1.7537e-03,  1.4756e-03,  1.9124e-03, -1.0103e-03,  1.0718e-03,\n",
       "          8.2839e-04,  7.5638e-04, -1.9479e-03,  1.1120e-03, -1.9630e-03,\n",
       "          1.1514e-03,  1.8331e-03, -1.8481e-03, -3.9154e-04,  2.4630e-03,\n",
       "          8.9288e-04,  3.2222e-04, -1.4621e-03,  2.6464e-05, -2.1732e-04,\n",
       "         -6.0284e-04,  1.0142e-03,  6.8533e-04,  1.0571e-03, -1.4517e-03,\n",
       "         -2.8668e-03,  8.6862e-04,  1.7580e-03,  6.2120e-04,  9.9754e-04,\n",
       "          3.0577e-04, -7.4327e-04,  1.2100e-04, -7.6115e-05,  1.2755e-03,\n",
       "         -2.2590e-05,  1.7352e-03, -7.0691e-05, -1.1241e-03, -5.2179e-03,\n",
       "          1.6044e-03, -5.6231e-04,  9.5391e-04,  1.6070e-03,  9.5463e-04,\n",
       "          3.3386e-03, -9.9301e-05,  3.4249e-04, -1.1739e-03, -4.6074e-05,\n",
       "          4.9543e-04,  6.1047e-04,  2.1405e-03,  1.4657e-03,  2.8354e-03,\n",
       "         -1.5173e-03,  8.0597e-04,  2.2204e-03, -2.6947e-04,  1.1384e-04,\n",
       "          2.3428e-03, -1.1416e-03, -1.3176e-03, -9.2149e-04, -5.4283e-03,\n",
       "          1.0338e-03,  3.2234e-04, -9.1851e-05, -1.6199e-03,  1.5157e-03,\n",
       "          2.1958e-03, -1.1147e-03,  2.4581e-04,  1.7239e-03, -1.0663e-03,\n",
       "         -1.0524e-03,  1.2891e-03, -5.6672e-04,  1.9331e-03,  1.6559e-03,\n",
       "          6.8450e-04,  9.9087e-04,  3.6418e-04, -3.7202e-03,  4.8089e-04,\n",
       "          8.2850e-05,  6.6960e-04, -6.9582e-04,  1.7397e-03,  1.2888e-03,\n",
       "          3.1178e-03,  2.1936e-03, -5.0479e-04,  8.3840e-04, -1.9414e-03,\n",
       "          4.5341e-04, -4.4595e-03,  8.1652e-04, -3.9717e-03, -7.7081e-04,\n",
       "          5.7787e-04, -2.4589e-03,  4.8417e-04,  4.1759e-04, -2.4377e-03,\n",
       "          2.4928e-03,  1.8021e-03,  2.9993e-04,  7.0691e-04,  7.0739e-04,\n",
       "         -1.4577e-03, -1.3418e-03,  1.9886e-03, -1.9197e-03, -1.0052e-03,\n",
       "          2.2280e-04, -1.3257e-03, -1.0622e-04, -1.4654e-03,  1.3001e-03,\n",
       "          2.0040e-03,  5.9187e-05,  1.6841e-03, -1.6475e-04,  1.1009e-03,\n",
       "         -2.7138e-04, -1.3890e-03,  1.8094e-03,  1.6868e-04,  4.6778e-04,\n",
       "         -2.4401e-03,  2.3749e-03,  1.5399e-03,  5.5885e-04, -1.2256e-03,\n",
       "          1.6652e-03, -6.0868e-04, -5.2440e-04,  5.3763e-04, -6.1035e-05,\n",
       "         -1.1301e-04,  1.9567e-03, -6.7091e-04, -2.4014e-03, -1.1476e-03,\n",
       "         -1.8625e-03,  8.6641e-04, -4.5342e-03, -9.2745e-05, -1.0753e-03,\n",
       "          3.4811e-03,  2.8065e-03,  1.6197e-03,  1.5845e-03, -3.9613e-04,\n",
       "         -2.1112e-03,  9.5510e-04, -5.4908e-04,  8.1962e-04, -1.8366e-03,\n",
       "         -2.1719e-03, -5.8979e-04,  4.7767e-04,  4.8769e-04,  2.0700e-03,\n",
       "          8.5580e-04,  2.3452e-03, -1.1683e-05, -6.4147e-04,  5.5087e-04,\n",
       "          2.3657e-03,  3.0125e-03,  1.7750e-03,  1.2636e-04,  2.1936e-03,\n",
       "          1.3257e-03,  9.5284e-04, -1.9554e-03,  6.9070e-04,  5.7042e-04,\n",
       "          1.5540e-03,  1.3214e-03,  2.4076e-03,  2.4056e-04,  1.5235e-04,\n",
       "          1.4079e-04, -2.5568e-03,  7.7605e-05,  9.6822e-04, -7.7260e-04,\n",
       "         -2.4605e-04, -2.3146e-03, -1.0808e-03,  1.9472e-03, -8.1539e-05,\n",
       "          7.7224e-04,  1.8311e-04, -1.3446e-03, -1.0618e-03,  1.7780e-03,\n",
       "          1.0438e-03,  3.6077e-03, -9.8300e-04,  2.0576e-03,  1.1414e-03,\n",
       "          7.5901e-04,  1.6273e-03,  3.4192e-03,  1.2717e-03,  1.4976e-03,\n",
       "          2.0913e-03, -3.9054e-03, -1.8124e-03,  1.4138e-03,  1.6061e-03,\n",
       "          1.8319e-03,  1.2081e-03, -4.9114e-04,  4.4262e-04, -1.3542e-04,\n",
       "         -9.8968e-04,  1.0003e-03, -1.7340e-03,  1.4835e-03,  4.9901e-04,\n",
       "         -1.9658e-04, -3.2926e-04,  1.0140e-03, -8.5342e-04, -1.6522e-04,\n",
       "         -2.6929e-04,  3.5417e-04, -5.3787e-04, -4.9436e-04, -1.3632e-03,\n",
       "          7.2896e-04,  1.7883e-03, -2.0564e-04, -2.0891e-04,  5.5814e-04,\n",
       "         -7.8070e-04, -1.2470e-03, -1.8954e-05,  2.6538e-03,  9.3126e-04,\n",
       "          1.7059e-03, -9.5117e-04, -1.5140e-04,  2.3000e-03,  2.3034e-03,\n",
       "          3.2158e-03,  3.0470e-03,  6.6656e-04,  2.0977e-03, -3.0560e-03,\n",
       "          2.6143e-03,  6.3610e-04,  4.6009e-03,  4.0056e-03,  6.5053e-04,\n",
       "          9.2387e-06,  1.7955e-03, -1.4472e-04,  3.6695e-03,  8.9765e-04,\n",
       "         -1.9646e-04,  3.9917e-03,  1.1849e-03,  1.3208e-04,  2.7006e-03,\n",
       "          8.1933e-04, -2.2840e-04,  1.7668e-03,  2.4784e-03,  1.2319e-03,\n",
       "          8.7392e-04,  1.0008e-03, -3.5614e-04,  1.4896e-03, -2.7587e-03,\n",
       "          1.5441e-03, -1.1927e-03,  5.6744e-05, -3.5095e-04,  1.0946e-03,\n",
       "          5.4646e-04, -1.4429e-03, -2.6664e-03, -1.3006e-04,  9.9826e-04,\n",
       "         -2.0211e-03,  1.3071e-03,  1.5581e-04,  1.7169e-03, -9.5010e-05,\n",
       "          4.9794e-04,  3.2747e-04,  2.3326e-03,  1.2310e-03,  2.9624e-04,\n",
       "         -1.5925e-03, -1.0445e-03, -2.6968e-03,  8.9765e-05,  1.0684e-03,\n",
       "         -8.6188e-05, -1.3578e-04,  1.6744e-03, -2.3201e-03, -6.0713e-04,\n",
       "          2.5328e-03,  2.3955e-04, -1.5276e-03,  3.7110e-04, -2.8127e-04,\n",
       "          2.3968e-03,  3.1614e-04,  6.5923e-05, -5.5313e-04,  2.1076e-03,\n",
       "          3.1657e-03, -1.1456e-04,  1.8144e-03,  1.5556e-03,  5.5575e-04,\n",
       "          8.4960e-04,  3.0288e-03,  2.2558e-03, -2.2521e-03, -1.2171e-03,\n",
       "          1.2635e-03, -1.7226e-03,  7.7057e-04,  2.0951e-03,  1.3607e-03,\n",
       "         -1.0598e-03, -1.1325e-05, -1.9374e-03, -1.8792e-03,  2.2737e-03,\n",
       "         -1.1322e-03,  8.6057e-04,  2.2434e-03,  1.5666e-03,  1.5296e-03,\n",
       "         -1.3896e-03, -4.2354e-03,  3.3162e-03,  3.4450e-03, -1.4473e-03,\n",
       "         -1.9060e-03,  2.6522e-03, -6.1679e-04,  1.7254e-03, -1.2658e-03,\n",
       "         -1.9929e-03,  2.3417e-03,  5.1785e-04,  3.0220e-03,  1.7725e-03,\n",
       "          3.4773e-03,  4.5586e-04, -5.6303e-04,  2.7943e-04, -5.3728e-04,\n",
       "         -8.7786e-04,  8.7631e-04, -8.6117e-04,  8.3220e-04,  2.1077e-03,\n",
       "          1.0205e-03, -4.5907e-04,  1.1979e-03, -1.0451e-03,  1.4965e-03,\n",
       "         -9.7084e-04, -1.7594e-03,  1.9510e-03, -2.0096e-03,  1.1399e-03,\n",
       "         -2.2381e-03,  7.1681e-04,  9.7430e-04, -1.2372e-03,  7.6032e-04,\n",
       "          1.4548e-03,  2.3981e-03, -1.2956e-03,  8.4507e-04,  2.8567e-03,\n",
       "          1.3375e-03,  2.0947e-03,  5.8413e-06, -3.6037e-04,  1.3176e-03,\n",
       "         -2.4109e-03,  8.8525e-04,  1.4970e-03,  5.4955e-05, -3.6854e-04,\n",
       "          1.0940e-03, -1.3912e-04,  3.3617e-03,  5.2238e-04,  5.2482e-04,\n",
       "          1.5441e-03, -3.3104e-04,  2.2386e-03,  4.1950e-04, -5.3561e-04,\n",
       "          5.2500e-04, -1.6729e-03, -1.1816e-03, -3.9654e-03,  2.9670e-03,\n",
       "         -4.5860e-04,  2.7623e-03, -7.8547e-04, -2.5898e-04,  1.0041e-03,\n",
       "         -1.1119e-03,  8.3137e-04, -4.7314e-04,  2.0647e-04,  2.9567e-03,\n",
       "          5.3656e-04, -1.7643e-05,  1.7452e-04, -2.0361e-04,  2.0659e-04,\n",
       "          1.5588e-03, -8.6099e-04, -2.9491e-03,  8.9872e-04, -4.8852e-04,\n",
       "          2.8113e-03,  1.9538e-04,  3.1483e-03,  1.4108e-03,  1.2943e-03,\n",
       "          9.8908e-04,  5.4574e-04,  2.9165e-04,  1.8717e-03,  1.7098e-03,\n",
       "         -5.5385e-04,  1.8246e-03,  8.2016e-04,  1.1177e-03,  2.9850e-04,\n",
       "          1.4246e-04, -8.8811e-04,  2.2335e-03, -1.9226e-03, -3.2192e-04,\n",
       "          1.1679e-03,  1.7622e-03,  6.4850e-04,  4.1970e-03,  1.3902e-03,\n",
       "          6.6483e-04, -3.2432e-03]),\n",
       " tensor([ 1.1508e-04,  6.1303e-04, -2.0349e-04, -5.4722e-04, -6.4754e-04,\n",
       "          5.8841e-05,  2.3611e-04, -1.3762e-04,  6.4332e-05, -8.4136e-05,\n",
       "         -3.4880e-04,  7.6123e-05, -6.7137e-05, -5.1851e-04, -9.6804e-04,\n",
       "          9.7614e-04, -3.4515e-04, -1.3057e-05, -1.4833e-04,  1.7721e-03,\n",
       "          2.1023e-04, -8.3843e-05,  5.5878e-04, -1.3991e-04, -8.2809e-04,\n",
       "         -4.5382e-04, -9.5171e-04,  1.1767e-04,  1.9174e-04, -2.3543e-04,\n",
       "          1.9061e-04, -7.1198e-05,  5.2526e-04,  8.8609e-04,  2.1877e-04,\n",
       "         -4.1106e-04,  2.6394e-05,  4.0094e-04,  5.7265e-05,  1.9625e-04,\n",
       "         -2.6774e-04,  1.4890e-04, -2.9868e-04,  6.1807e-04,  7.1693e-06,\n",
       "          2.8596e-04,  2.5805e-04, -4.3024e-04,  4.5771e-04,  3.0135e-04,\n",
       "         -2.8636e-04, -1.1748e-03,  4.8217e-04, -7.9428e-04, -5.9293e-04,\n",
       "         -4.5747e-04, -2.9226e-04, -2.4088e-04,  3.4810e-04, -2.3712e-04,\n",
       "         -3.6683e-04,  1.2091e-03,  3.6944e-04, -1.3370e-04,  6.7962e-04,\n",
       "          3.6662e-04, -9.4183e-04, -5.4086e-04, -6.2883e-06, -6.4449e-04,\n",
       "          3.1780e-04, -6.2249e-04,  4.5792e-04, -2.5243e-04, -4.6730e-04,\n",
       "          3.5963e-04, -5.0133e-04,  2.0321e-05, -5.4964e-05,  2.4498e-04,\n",
       "          3.7207e-04,  1.4070e-04, -5.5894e-05,  4.4525e-04,  4.4946e-06,\n",
       "          9.8087e-05, -2.2369e-04, -3.1352e-04,  1.9153e-04, -9.2856e-05,\n",
       "          3.5012e-04,  1.7859e-05,  6.2774e-04,  5.3582e-04, -1.6682e-05,\n",
       "         -4.0097e-04,  5.2020e-05,  2.3828e-04, -3.2279e-04,  4.7163e-04,\n",
       "          9.6178e-04, -2.5740e-04,  3.2631e-04,  9.0015e-04, -3.9011e-05,\n",
       "         -3.0929e-04, -7.8687e-04, -6.3550e-04, -5.8472e-04, -8.7097e-06,\n",
       "         -2.2652e-05, -2.1834e-04,  1.6122e-04, -4.6083e-04, -1.7943e-04,\n",
       "          2.1846e-04, -1.8364e-04, -3.0063e-04,  1.8766e-04,  4.9271e-04,\n",
       "         -3.3310e-04,  1.7665e-04, -2.7069e-04, -8.9003e-04,  2.1861e-04,\n",
       "          5.5711e-04,  6.6807e-04,  2.5266e-04, -2.2291e-04, -2.1385e-04,\n",
       "         -7.7904e-04,  8.6457e-04, -6.6856e-04,  1.2713e-04,  2.8631e-04,\n",
       "         -6.3656e-05, -1.7948e-04, -3.2757e-04,  4.6816e-05,  9.0498e-05,\n",
       "          2.5174e-04, -3.0902e-04,  3.8629e-04, -1.0129e-04, -6.2586e-04,\n",
       "          2.7902e-04,  4.6819e-04,  1.2754e-04, -5.6654e-04, -1.0149e-04,\n",
       "         -2.5120e-05, -6.6756e-04, -1.6231e-04,  3.3491e-04, -2.9602e-04,\n",
       "         -3.5037e-04, -5.0510e-04,  3.7564e-04,  3.1621e-04, -5.9862e-05,\n",
       "         -6.1461e-04,  6.5957e-04,  5.2127e-04, -2.4778e-04, -7.2645e-04,\n",
       "          3.7340e-04,  2.2922e-04,  2.4199e-04, -7.7507e-05,  7.7846e-04,\n",
       "         -4.2688e-05, -7.3513e-04,  5.4058e-04, -2.9339e-04, -1.0153e-04,\n",
       "         -1.3531e-04, -9.9605e-06,  4.1164e-04,  1.9444e-04,  2.0881e-04,\n",
       "          2.4429e-05,  4.8927e-04, -2.6229e-04,  2.6954e-04,  1.2675e-04,\n",
       "         -2.5351e-04,  3.5483e-04, -1.2854e-04,  1.1644e-04,  5.0873e-04,\n",
       "         -1.7067e-04,  1.2450e-03,  2.2143e-04,  1.6676e-04,  6.7105e-04,\n",
       "         -2.3448e-04,  4.3172e-04,  8.8046e-04,  8.6719e-04, -1.1694e-04,\n",
       "          6.5295e-04, -6.2982e-04,  1.4004e-04,  1.8577e-04, -6.1496e-04,\n",
       "          1.4442e-04, -2.3691e-04, -2.7530e-04, -2.0447e-04, -1.4539e-04,\n",
       "          3.4299e-04,  6.1715e-04,  9.8582e-05, -4.4638e-04,  1.0480e-04,\n",
       "          8.1163e-04, -4.5132e-05, -1.9034e-04, -7.1654e-04,  9.1015e-04,\n",
       "          2.6167e-04,  7.7115e-05,  2.3040e-04,  6.8817e-04, -3.6265e-04,\n",
       "         -1.9731e-04, -3.8886e-04, -1.0229e-04, -2.6218e-04,  1.8779e-04,\n",
       "         -4.6858e-04, -1.2197e-04,  2.2438e-04,  1.5159e-04, -1.2484e-03,\n",
       "         -6.0669e-04,  6.8322e-06,  4.8990e-04, -5.5379e-04, -1.5409e-04,\n",
       "          4.5304e-04, -7.0544e-04,  5.0773e-04, -3.4800e-04, -4.1083e-04,\n",
       "         -8.2052e-04,  6.5603e-04,  2.8236e-04, -4.5947e-04,  5.5278e-05,\n",
       "          3.7810e-04, -4.7026e-04, -2.1752e-04,  5.1459e-04,  4.1746e-05,\n",
       "          1.2891e-04, -4.8569e-04, -6.2276e-05,  3.4052e-04,  8.1683e-04,\n",
       "         -2.5967e-04, -8.7529e-05,  1.1208e-04, -4.7630e-04,  5.0045e-04,\n",
       "         -2.9343e-04,  3.6359e-04,  9.5300e-05, -1.3527e-04,  7.3909e-04,\n",
       "          7.5719e-04, -1.4940e-04,  2.7965e-04,  1.4035e-04, -3.0274e-04,\n",
       "          3.8165e-04, -4.0717e-05,  3.8729e-04,  2.4867e-05,  1.3774e-04,\n",
       "          1.7283e-04,  7.0655e-05, -2.4371e-04,  2.2612e-04, -2.8193e-04,\n",
       "          1.1494e-03,  4.8196e-04,  1.0192e-04,  7.1633e-04, -1.2812e-04,\n",
       "         -2.4046e-04, -3.6438e-04, -8.6498e-05,  3.5188e-04,  5.2267e-04,\n",
       "          4.1597e-05, -5.4074e-04, -1.6626e-04,  3.8542e-05, -2.1914e-04,\n",
       "         -5.9791e-04, -4.9312e-04,  6.1247e-04, -1.5225e-04, -1.7247e-04,\n",
       "          2.1857e-04,  4.2446e-04, -4.0671e-04,  1.8176e-04, -3.5090e-04,\n",
       "         -2.1115e-04,  1.8569e-04, -2.2318e-04, -5.2743e-04, -1.5133e-04,\n",
       "         -1.9871e-05,  5.5681e-04, -3.0227e-05,  4.3078e-04,  2.1097e-04,\n",
       "         -9.2742e-04, -2.7857e-04,  3.5626e-04, -1.5170e-04,  7.0213e-04,\n",
       "          2.0519e-05, -4.5158e-04,  1.6824e-05, -3.9332e-05,  7.2018e-04,\n",
       "         -2.3134e-06, -3.5608e-04, -1.5527e-04,  2.3305e-04, -2.3117e-05,\n",
       "         -4.7481e-04, -1.2478e-04, -7.5173e-05, -4.6733e-04,  1.3160e-04,\n",
       "         -3.3265e-04, -2.1944e-04, -1.6677e-04, -6.8645e-04, -3.8181e-04,\n",
       "          2.2807e-04, -5.7995e-05, -5.1381e-04,  5.3753e-04, -2.1923e-04,\n",
       "         -7.0396e-04, -3.3606e-04,  1.0440e-04, -1.6521e-04,  6.4857e-05,\n",
       "          3.7234e-04,  1.2314e-04, -9.1487e-04, -2.0129e-04, -8.0374e-04,\n",
       "          1.2634e-04, -1.9888e-04, -2.0208e-04, -1.5883e-04,  4.3377e-05,\n",
       "         -6.7830e-05, -1.8773e-04, -4.0383e-05, -3.3522e-04,  2.6105e-04,\n",
       "         -2.1134e-04,  2.3064e-04,  3.3850e-04,  8.3890e-05, -2.6773e-04,\n",
       "         -1.9473e-04, -5.5553e-04, -2.2948e-06,  7.3686e-06, -1.3475e-04,\n",
       "          2.7110e-04,  1.5160e-04, -4.5298e-04,  6.6422e-04,  2.4945e-04,\n",
       "         -3.7664e-04,  1.5959e-04,  7.5248e-04,  7.6563e-04, -2.8532e-04,\n",
       "         -1.3115e-04, -2.2393e-04,  7.6882e-04,  2.4097e-04,  3.4882e-04,\n",
       "          1.0251e-03, -3.4354e-04, -1.5119e-04,  3.4718e-04, -2.3559e-04,\n",
       "         -2.7621e-04,  2.3997e-04, -6.3568e-05,  4.4153e-04,  2.0514e-04,\n",
       "          9.9582e-05,  2.4094e-04, -8.7798e-05,  1.3718e-05,  4.5547e-04,\n",
       "          4.9391e-04,  2.4593e-04,  1.2761e-04, -1.4000e-04, -4.7607e-04,\n",
       "          1.2117e-05,  8.4877e-05,  1.4607e-04,  5.1670e-04,  8.9712e-04,\n",
       "         -2.4968e-04, -3.2618e-04,  1.0989e-04,  1.9162e-04, -4.7233e-04,\n",
       "          3.1349e-04,  1.4836e-04,  4.6251e-04,  4.7635e-05, -8.6235e-05,\n",
       "         -7.2092e-05,  1.7820e-04, -2.3863e-04,  4.6858e-04,  4.0682e-04,\n",
       "         -5.1335e-04, -2.5255e-04,  6.6023e-05, -8.1446e-04,  6.1286e-04,\n",
       "         -9.8093e-04,  2.1128e-05, -2.5088e-04, -2.5704e-04,  4.4121e-04,\n",
       "          3.4077e-04,  4.6391e-04, -1.0504e-04, -6.4619e-04,  5.6410e-05,\n",
       "         -9.2621e-04,  1.1522e-05, -7.8301e-05,  7.8268e-06,  1.9491e-04,\n",
       "         -4.3963e-05, -6.2620e-04,  6.0555e-04,  5.8665e-04,  4.0269e-04,\n",
       "          8.2086e-05, -8.0360e-05, -2.0794e-04, -2.2428e-05, -2.1702e-04,\n",
       "          2.1731e-04,  1.9560e-04,  4.3041e-04,  1.6407e-04,  5.3559e-04,\n",
       "         -5.7529e-04, -6.2366e-04, -7.2992e-07,  4.3624e-04,  1.0149e-04,\n",
       "         -6.5577e-04, -5.2757e-04,  2.7966e-04, -2.8156e-04, -1.7686e-04,\n",
       "          7.4324e-05, -2.0604e-04,  9.9204e-05, -1.8600e-04,  1.3666e-04,\n",
       "          2.7134e-04, -3.1143e-04,  4.5393e-04, -2.2819e-04, -5.7035e-04,\n",
       "         -6.0104e-05,  2.5372e-04, -4.1197e-04, -6.9327e-05,  5.1022e-05,\n",
       "         -1.1042e-04, -1.0858e-04,  3.6978e-04, -2.0086e-04,  3.8560e-04,\n",
       "         -4.3176e-04, -5.2022e-04, -1.1083e-03, -3.7878e-04,  2.6339e-04,\n",
       "          5.7491e-04,  1.2922e-04,  5.1703e-04, -2.2146e-04, -5.1275e-04,\n",
       "          1.0312e-04, -9.8901e-05]),\n",
       " tensor([[ 1.2833e-03, -2.9003e-03,  3.5585e-03,  ...,  4.4121e-03,\n",
       "           3.7640e-03, -1.9025e-03],\n",
       "         [-4.0236e-03,  1.4996e-03,  1.4280e-03,  ...,  2.8009e-03,\n",
       "           3.3667e-03,  4.5768e-04],\n",
       "         [-1.6229e-03,  2.0773e-03, -7.0544e-04,  ...,  1.4430e-03,\n",
       "           1.7550e-03, -2.3325e-04],\n",
       "         ...,\n",
       "         [ 5.0624e-04, -2.8329e-05, -9.0241e-05,  ...,  6.1985e-04,\n",
       "          -1.9675e-03,  6.5115e-04],\n",
       "         [ 2.2072e-03, -1.2051e-03,  3.0778e-03,  ...,  2.6810e-03,\n",
       "           8.1230e-04,  2.0305e-04],\n",
       "         [ 1.0959e-03, -1.3716e-03,  1.3005e-03,  ...,  2.1257e-03,\n",
       "           1.2847e-03,  3.3325e-03]]),\n",
       " tensor([ 0.0012,  0.0004, -0.0012,  ..., -0.0001, -0.0006, -0.0003]),\n",
       " tensor([[-3.3576e-04, -1.4954e-03,  1.9846e-03,  ...,  2.3101e-03,\n",
       "          -1.0361e-03,  3.8232e-03],\n",
       "         [-2.6866e-03,  8.9604e-04, -3.4076e-04,  ...,  9.7492e-04,\n",
       "           1.6755e-03, -1.8972e-04],\n",
       "         [ 7.6299e-03, -4.7268e-04,  1.0651e-03,  ...,  2.0117e-07,\n",
       "          -5.4229e-04, -1.2547e-03],\n",
       "         ...,\n",
       "         [ 5.0855e-03,  3.9427e-03, -1.0751e-03,  ...,  1.3329e-03,\n",
       "          -6.0766e-04, -1.4000e-03],\n",
       "         [ 7.8099e-04,  6.5302e-04,  2.0304e-03,  ...,  8.6802e-05,\n",
       "           5.9605e-05,  1.4653e-03],\n",
       "         [-2.6595e-04,  1.9469e-03,  4.5587e-03,  ..., -7.7552e-04,\n",
       "           4.6289e-04, -6.6536e-04]]),\n",
       " tensor([-3.4764e-04, -1.6212e-04,  1.7865e-04,  4.7329e-04, -1.4067e-04,\n",
       "          3.6629e-04,  9.6928e-05, -3.3448e-05, -1.4962e-04, -2.2321e-04,\n",
       "          2.5495e-04, -1.2816e-04,  4.0274e-04, -1.8068e-04,  6.4739e-04,\n",
       "         -5.3811e-04, -3.0328e-04, -2.1996e-04,  4.0553e-04, -5.2556e-04,\n",
       "         -7.1581e-04,  3.1534e-04, -1.9911e-04,  4.9946e-04,  3.2189e-04,\n",
       "         -1.0475e-04,  4.2683e-04,  4.2404e-04, -2.6812e-04,  4.4251e-04,\n",
       "         -2.9059e-04,  1.5696e-04, -2.8881e-04, -1.0522e-03, -4.2308e-04,\n",
       "          2.8092e-04,  8.5735e-05, -2.9330e-04,  3.6417e-05, -5.4018e-04,\n",
       "         -1.6838e-04, -1.8073e-04,  2.9697e-04, -2.6484e-04, -1.9617e-04,\n",
       "         -1.2962e-04, -1.7504e-04,  2.6178e-04,  7.5886e-05,  4.7265e-05,\n",
       "          1.3798e-04,  2.3435e-04, -3.0752e-04,  1.5135e-04,  1.5054e-04,\n",
       "         -3.7238e-04,  3.0347e-04, -4.4689e-04, -4.2060e-04, -5.3851e-04,\n",
       "          2.4282e-04,  3.4861e-05, -2.0796e-04, -1.5885e-04, -2.4639e-04,\n",
       "         -4.2295e-04,  4.2505e-04,  1.5006e-04,  1.6289e-04, -3.6608e-04,\n",
       "         -5.4552e-05,  6.5306e-06,  4.6666e-04, -1.1498e-04,  2.4570e-04,\n",
       "         -4.4677e-04,  2.3634e-04,  3.0554e-04,  3.6946e-04, -2.0897e-04,\n",
       "          2.0496e-04, -1.6047e-04,  7.8195e-05, -4.3330e-04,  3.1665e-07,\n",
       "          2.1257e-05,  1.2243e-04,  2.9985e-04, -2.8948e-04, -1.4768e-04,\n",
       "         -1.3031e-04, -3.2437e-04,  3.6399e-05, -5.4647e-04,  1.7221e-04,\n",
       "         -4.7580e-04, -3.7620e-05, -1.4290e-04, -3.2917e-04, -1.1732e-04,\n",
       "         -2.5614e-04,  3.7401e-04, -3.8678e-04, -5.1363e-04, -3.1717e-04,\n",
       "         -1.3376e-04,  1.9562e-04,  5.8926e-04, -3.9618e-05, -2.6449e-04,\n",
       "         -3.4423e-04, -9.9243e-05, -8.2300e-05, -3.6627e-04, -1.2793e-04,\n",
       "         -9.5604e-05,  3.4142e-05,  3.9548e-04, -9.1260e-05, -4.2019e-04,\n",
       "          1.6981e-04, -5.5460e-06, -5.1162e-05, -3.1167e-04, -3.1197e-04,\n",
       "         -4.5959e-04, -1.5159e-04,  4.7595e-04,  1.1306e-04,  3.6721e-04,\n",
       "          4.0344e-04, -2.5986e-04, -1.1787e-04,  1.5277e-04, -5.2744e-04,\n",
       "          2.2896e-04,  5.3054e-04,  1.4502e-04, -1.7360e-05, -1.5157e-04,\n",
       "         -3.3119e-04, -3.1709e-04, -3.9523e-04,  1.5892e-05,  2.2999e-04,\n",
       "         -3.8676e-04, -1.0942e-04, -1.7214e-04,  2.4369e-04, -2.4045e-04,\n",
       "         -2.1044e-04,  2.2328e-04, -4.4712e-05, -5.4659e-04, -1.0006e-04,\n",
       "         -2.8308e-05,  9.0860e-06, -8.3248e-05, -1.8234e-04,  4.8701e-05,\n",
       "          2.4632e-04, -7.7043e-04,  3.1492e-04,  3.4280e-04, -1.4510e-04,\n",
       "         -2.0092e-04,  4.0241e-04, -2.3045e-04,  6.4126e-04,  5.3264e-04,\n",
       "          2.1826e-04, -1.2331e-04,  1.6172e-04,  4.1005e-04,  4.1648e-04,\n",
       "         -1.7498e-04,  1.7990e-04,  8.5577e-05, -5.2464e-04, -3.5374e-04,\n",
       "         -1.4761e-05, -4.7896e-05,  3.8580e-04,  3.5084e-04, -3.7469e-05,\n",
       "          5.8584e-04,  7.0939e-05,  3.9124e-04, -4.4467e-05, -3.3093e-04,\n",
       "         -2.6872e-04, -5.8484e-04,  1.2157e-04, -2.1626e-04, -1.1423e-04,\n",
       "          1.4969e-04,  6.3259e-04, -3.4219e-04,  2.7411e-04, -4.8686e-05,\n",
       "         -1.5648e-04,  3.2053e-04,  5.0612e-04, -2.4181e-04,  1.2885e-04,\n",
       "         -4.8940e-04,  1.9914e-04,  6.2027e-04, -3.8236e-05, -3.1448e-04,\n",
       "          2.3410e-04, -3.3710e-04,  9.1949e-05, -1.0346e-04,  1.1981e-04,\n",
       "         -5.8066e-04, -2.1941e-04,  2.8891e-04,  8.5081e-05, -1.6753e-04,\n",
       "         -1.2000e-04, -3.1776e-04,  4.1320e-05,  2.9912e-05,  2.2270e-05,\n",
       "         -1.3814e-04, -1.4057e-04, -4.2780e-04, -3.2781e-04, -2.7846e-04,\n",
       "         -2.6468e-04,  1.0194e-04,  2.0201e-04,  1.8553e-04,  2.3444e-04,\n",
       "          1.5448e-04,  4.1749e-05,  7.6245e-05,  4.1515e-04,  1.7468e-04,\n",
       "          7.6520e-04,  9.2756e-04,  6.3950e-05,  2.5798e-04, -3.2773e-04,\n",
       "          2.2604e-04, -3.6049e-04, -1.9431e-04,  3.2247e-04, -1.4023e-04,\n",
       "          1.1668e-04,  3.1690e-05,  4.4302e-04, -1.9880e-04,  2.3536e-05,\n",
       "         -9.6692e-05,  1.7246e-05, -3.9808e-04,  1.3131e-05, -3.7948e-04,\n",
       "          2.5349e-04, -3.7942e-06,  2.5473e-04,  1.6002e-05, -1.6835e-04,\n",
       "          1.2371e-04, -4.8604e-04, -3.3247e-04,  8.4128e-05, -9.4483e-05,\n",
       "         -2.4495e-04,  3.4068e-06, -5.9144e-04, -3.7309e-05,  5.6528e-04,\n",
       "         -5.4661e-04,  8.3642e-04, -1.3854e-04, -6.6098e-05, -5.0812e-04,\n",
       "         -3.5817e-04,  1.4624e-04,  6.6385e-04, -6.8640e-05,  4.9661e-04,\n",
       "         -8.7643e-05, -2.2824e-04, -2.3798e-04, -2.4963e-04,  3.0415e-04,\n",
       "         -1.2921e-04,  3.0952e-04, -2.5013e-04, -2.6369e-05, -3.1027e-04,\n",
       "          4.6579e-05,  7.1102e-04,  7.1158e-04,  7.8497e-05,  5.0031e-06,\n",
       "          1.5078e-04,  5.4379e-05, -3.4418e-04, -3.7312e-05, -2.9105e-04,\n",
       "          5.6723e-04, -4.0852e-04,  1.5241e-04, -8.2223e-05,  5.0525e-05,\n",
       "          3.4552e-04, -6.2330e-04, -5.2264e-05,  1.8674e-04,  8.4775e-05,\n",
       "          3.5029e-05, -1.3689e-04,  7.8829e-06, -1.4948e-04, -6.6275e-05,\n",
       "          5.2455e-04,  3.7506e-04,  3.4738e-05, -3.3383e-04, -2.2518e-04,\n",
       "         -1.7576e-04,  2.9475e-04, -2.3613e-04,  8.5345e-05, -3.9268e-04,\n",
       "          6.5327e-05, -1.6700e-05,  1.0188e-04, -1.7161e-04,  6.4285e-04,\n",
       "          8.5128e-05, -2.5448e-04,  4.3105e-04,  1.2066e-04,  7.9893e-05,\n",
       "         -2.4910e-04,  3.8073e-04,  4.3981e-04, -1.1495e-04, -1.3843e-04,\n",
       "         -5.0089e-04,  2.0220e-04,  2.4662e-04, -3.2500e-05,  1.3957e-04,\n",
       "          3.0251e-04, -8.1852e-05,  4.6386e-05,  1.9672e-05, -1.9470e-04,\n",
       "          9.2903e-05,  2.1592e-05,  1.7430e-04,  3.0584e-04,  6.0472e-04,\n",
       "         -2.3689e-04,  3.2946e-04, -6.4825e-05,  4.2990e-06, -4.2500e-04,\n",
       "          3.9896e-05,  3.5512e-04,  9.5896e-05, -2.5649e-06, -4.5161e-05,\n",
       "          6.8795e-04, -2.6611e-04, -6.4130e-04, -1.5697e-04,  2.3808e-04,\n",
       "         -2.1587e-04,  3.3231e-04,  1.7943e-04,  1.6485e-04,  3.4299e-04,\n",
       "          1.8124e-04,  2.4154e-04,  5.1579e-04, -1.9271e-04, -6.3602e-04,\n",
       "          3.1510e-04, -3.4301e-04, -2.9764e-04, -2.9905e-04,  1.5947e-04,\n",
       "          3.4751e-05, -3.0175e-04, -4.3145e-04, -7.4295e-05, -1.7684e-04,\n",
       "         -4.0839e-04,  1.9335e-04, -9.0984e-05, -2.0898e-04,  3.0231e-04,\n",
       "          4.3108e-04,  2.5737e-04,  4.1666e-04, -4.7615e-04, -9.8316e-05,\n",
       "         -4.3463e-04,  2.3708e-04, -2.5658e-04, -5.4915e-05, -3.2547e-04,\n",
       "         -2.4986e-05, -1.5732e-04, -3.9935e-04, -1.2788e-04,  3.2261e-04,\n",
       "          3.6867e-05,  1.6214e-04, -2.6680e-04, -3.8624e-05, -2.2069e-04,\n",
       "          2.4825e-04, -2.4932e-04, -7.7462e-05,  1.7712e-04,  6.5604e-05,\n",
       "         -2.2790e-04, -3.8216e-04, -4.7013e-04, -4.3262e-04, -2.0573e-04,\n",
       "          4.4039e-05,  2.5102e-04,  6.6746e-05,  1.4302e-04, -4.4719e-04,\n",
       "         -2.8512e-04,  4.8356e-04, -4.4205e-04,  9.7048e-05, -2.0406e-04,\n",
       "          6.4883e-04,  2.5745e-04, -1.1374e-04,  1.9392e-04, -1.1967e-03,\n",
       "         -5.7789e-05, -3.3744e-04, -4.6342e-04,  3.7706e-04,  1.8165e-04,\n",
       "          1.9147e-03,  2.4818e-04, -1.8591e-04, -3.0760e-04, -3.6848e-04,\n",
       "          5.1829e-04,  4.3232e-04, -4.7972e-04, -2.1178e-04, -2.9106e-04,\n",
       "          8.1702e-05, -9.8969e-05,  1.9705e-04, -3.1174e-04,  2.4170e-04,\n",
       "          2.1170e-04, -6.5724e-05, -2.5479e-04, -2.3106e-04,  2.5557e-05,\n",
       "          4.5281e-06,  7.3590e-05, -1.3305e-04, -7.9686e-05,  5.1635e-05,\n",
       "          2.7506e-04,  5.4789e-04,  6.5956e-04,  1.9375e-04,  1.9745e-04,\n",
       "          1.6435e-04, -1.5283e-04,  7.1881e-05, -2.1408e-04, -1.0479e-05,\n",
       "          4.9630e-05, -2.5077e-04, -1.9488e-04, -5.5424e-05,  5.3545e-04,\n",
       "         -3.9363e-04, -2.9948e-04,  2.6656e-04,  2.5662e-05,  1.0240e-04,\n",
       "          1.2097e-04,  4.2638e-04,  1.2958e-04,  1.3825e-04, -4.1791e-05,\n",
       "          1.2044e-04,  2.5161e-04,  8.5055e-05,  2.1362e-04, -1.5047e-04,\n",
       "          9.0632e-05, -7.6384e-05, -3.7581e-04,  3.8369e-04,  2.9847e-04,\n",
       "         -4.4109e-04, -4.7779e-04]),\n",
       " tensor([-2.3335e-03, -1.8253e-03, -2.2087e-03, -4.1645e-03, -1.9401e-03,\n",
       "          2.1517e-04, -3.3522e-04, -2.6383e-03,  4.8226e-04,  4.7642e-04,\n",
       "         -5.1779e-04, -1.8660e-03, -7.4798e-04, -9.8127e-04, -1.8187e-03,\n",
       "         -3.8740e-03, -5.6016e-04, -3.7951e-03,  5.6839e-04, -2.7961e-03,\n",
       "         -1.8294e-03,  1.5539e-04, -1.0553e-03, -1.1492e-03, -1.8714e-03,\n",
       "          9.1159e-04,  7.1609e-04, -8.8084e-04, -1.1026e-03, -1.0303e-03,\n",
       "          1.1663e-03,  3.9339e-04,  6.2019e-04, -1.9619e-03, -3.0279e-03,\n",
       "         -2.6256e-04,  2.2662e-04, -1.5625e-03, -7.3576e-04, -5.6577e-04,\n",
       "          2.1261e-04, -7.8708e-04, -1.2394e-03,  1.2906e-03, -2.6742e-03,\n",
       "         -1.1541e-03,  1.2547e-04, -2.4787e-03, -2.7797e-04, -5.7425e-03,\n",
       "         -2.6494e-04, -1.3668e-03, -1.8566e-03, -2.0652e-03, -6.6352e-04,\n",
       "         -3.5681e-03,  1.6596e-03, -4.7768e-03, -1.7637e-03, -2.2400e-03,\n",
       "         -2.5761e-04, -1.0608e-03,  7.0751e-05, -9.0599e-04, -9.2930e-04,\n",
       "         -1.5666e-03, -2.5403e-03, -1.7897e-03, -1.1017e-03, -1.6106e-03,\n",
       "         -4.2883e-03, -7.8857e-05,  7.1609e-04, -8.8841e-04, -1.2789e-03,\n",
       "         -4.2407e-03, -1.1412e-03, -6.3860e-04, -1.5413e-03, -8.0299e-04,\n",
       "         -1.9697e-03, -1.3261e-03,  1.2937e-03,  1.3781e-04, -5.3906e-04,\n",
       "         -2.7633e-04,  9.3457e-04, -2.0504e-03, -1.6344e-03, -2.4383e-03,\n",
       "         -1.9611e-03, -3.2592e-04,  4.4906e-04, -1.5276e-03, -2.0271e-03,\n",
       "         -8.3917e-04, -3.6895e-05,  7.7695e-04,  2.0754e-04, -6.6566e-04,\n",
       "         -3.6603e-03, -1.5483e-03, -2.0933e-04, -2.7012e-03, -2.6765e-03,\n",
       "         -1.2104e-03, -1.8137e-03, -1.4913e-03, -1.3084e-03, -1.6752e-03,\n",
       "         -9.9486e-04, -8.5473e-04, -3.6199e-03,  7.6151e-04, -2.7745e-03,\n",
       "          1.7291e-04,  9.8348e-06, -5.2956e-04, -1.7511e-03, -8.3691e-04,\n",
       "         -4.3690e-04,  8.7363e-04, -2.5523e-04, -4.0628e-03, -3.9339e-05,\n",
       "         -1.3081e-03, -2.6299e-03, -3.2370e-03,  1.6975e-03,  4.0191e-04,\n",
       "         -1.2608e-03, -1.3522e-03,  2.2554e-04, -5.9867e-04, -2.6495e-03,\n",
       "         -1.6502e-03, -7.2265e-04, -6.4552e-05, -9.7167e-04,  1.5515e-03,\n",
       "         -1.0011e-03, -9.5826e-04, -3.1742e-03, -2.2187e-03, -2.2767e-03,\n",
       "          1.7606e-03, -2.2237e-03, -8.6433e-04, -4.5882e-03, -7.2718e-06,\n",
       "         -1.1510e-03, -2.1051e-03, -3.0018e-03, -7.0387e-04,  1.2211e-03,\n",
       "         -3.4251e-03,  1.8674e-04, -5.4216e-04, -3.8413e-03, -3.6019e-04,\n",
       "         -4.1205e-03, -1.9775e-03, -3.1173e-05, -1.0548e-03, -1.4046e-03,\n",
       "         -8.8793e-04,  2.2680e-03, -1.6685e-03, -1.6575e-03, -1.5467e-04,\n",
       "         -3.9025e-03, -2.3600e-03,  1.7734e-03, -2.4119e-03,  1.0126e-03,\n",
       "          1.0427e-03, -2.2614e-04, -1.9165e-03, -1.8136e-03, -4.1108e-03,\n",
       "         -1.2072e-03, -2.6504e-03, -1.4344e-03,  3.9941e-04, -1.6439e-04,\n",
       "         -5.1093e-04, -2.8157e-03, -1.7321e-04, -2.2547e-03, -3.9810e-04,\n",
       "          9.3460e-05, -1.1538e-03, -1.5576e-03, -4.1012e-03, -3.5267e-03,\n",
       "         -3.6129e-03,  2.8193e-04, -5.2211e-03, -1.7140e-03, -6.0986e-03,\n",
       "         -1.6115e-03, -2.7058e-03, -1.7359e-03, -1.6902e-03, -9.2667e-04,\n",
       "         -1.3493e-03, -5.7995e-04, -1.6259e-03, -2.7307e-03, -1.4759e-03,\n",
       "          2.3013e-04, -2.1743e-03,  1.9367e-03, -2.1700e-03, -1.7780e-04,\n",
       "         -9.3639e-05, -1.2829e-03, -9.8515e-04, -2.1673e-03,  3.2407e-04,\n",
       "         -2.0183e-03, -2.9761e-03, -4.5633e-04, -5.1540e-04, -2.5526e-03,\n",
       "         -3.3665e-04, -7.6038e-04, -1.4384e-03, -2.4276e-03, -1.5754e-04,\n",
       "         -8.0049e-04,  1.1728e-03,  1.2207e-04, -1.3697e-03, -1.6155e-03,\n",
       "          1.2009e-03, -1.0586e-04, -3.6925e-04,  5.9330e-04, -1.7778e-03,\n",
       "          2.6309e-04, -5.3163e-03, -2.1356e-03,  3.6007e-04,  2.4217e-04,\n",
       "         -1.5863e-03, -1.4268e-03, -1.1955e-03,  6.6370e-04, -5.9140e-04,\n",
       "         -2.6072e-03, -1.3815e-03, -2.5159e-04, -1.6773e-03, -3.1656e-04,\n",
       "         -2.6017e-04, -1.8668e-03, -1.5797e-03, -3.4878e-04, -2.5583e-03,\n",
       "          2.3896e-04, -6.2144e-04,  5.2840e-04, -1.6993e-03, -8.7148e-04,\n",
       "         -9.7311e-04,  1.6264e-03, -1.2143e-03, -1.6681e-03, -4.8894e-04,\n",
       "         -2.8892e-03, -2.7007e-04, -5.5471e-03, -2.3780e-03, -6.8223e-04,\n",
       "         -1.4085e-03,  5.8782e-04, -1.2231e-03, -1.0192e-05, -1.0443e-04,\n",
       "         -1.2441e-03, -2.3245e-03, -2.3080e-03,  2.6041e-04,  7.2837e-05,\n",
       "         -4.0947e-03, -7.5591e-04, -3.3829e-03, -1.0374e-03,  2.5107e-03,\n",
       "         -6.5666e-04, -6.2746e-03, -1.7781e-03,  6.0916e-05, -2.1182e-03,\n",
       "         -8.8984e-04, -2.3711e-03,  4.2236e-04, -3.2580e-04,  8.0717e-04,\n",
       "          6.8784e-05,  1.2590e-03,  5.1492e-04, -1.0484e-04, -3.5000e-03,\n",
       "         -9.7388e-04, -1.3019e-03, -2.8099e-03, -2.7984e-03, -3.0081e-03,\n",
       "          2.1888e-03,  7.6908e-04, -2.9092e-03, -1.2255e-04, -3.8093e-04,\n",
       "         -2.3154e-03, -8.7947e-04, -2.0915e-04, -2.1052e-04, -2.3832e-03,\n",
       "         -6.3419e-04, -9.6619e-05, -4.8143e-04, -1.8138e-03, -2.8462e-03,\n",
       "         -3.5141e-03, -3.3988e-03, -4.7743e-04, -6.7407e-04, -2.7086e-03,\n",
       "          8.0240e-04, -5.9074e-04,  2.6757e-04,  8.0067e-04, -2.9309e-03,\n",
       "         -1.6564e-03, -2.0403e-03, -1.5560e-03, -1.7352e-03, -5.9307e-04,\n",
       "          1.8293e-03, -4.9257e-04, -6.6739e-04, -1.7377e-03, -1.1748e-03,\n",
       "         -1.4457e-03, -3.8698e-03, -2.6691e-04,  1.2240e-03, -1.6924e-03,\n",
       "          8.9675e-04, -2.0157e-03, -4.1223e-04, -1.6474e-03, -1.8287e-03,\n",
       "          2.2658e-03,  6.8820e-04, -4.7988e-04, -2.7903e-03, -1.3146e-03,\n",
       "         -1.3987e-03,  1.8841e-03, -3.7229e-04,  5.2512e-05, -1.3677e-03,\n",
       "         -7.3051e-04, -2.3973e-03,  9.8568e-04, -1.8436e-03, -1.7610e-03,\n",
       "         -2.0920e-03, -4.0257e-04, -1.3047e-03,  2.5725e-04, -2.0149e-03,\n",
       "         -9.1255e-04, -3.8517e-04, -1.9194e-03, -7.4250e-04, -3.4143e-03,\n",
       "         -1.3936e-04, -1.8167e-04, -7.9721e-04,  3.8981e-05, -2.1526e-03,\n",
       "         -4.2412e-03, -1.4125e-03, -8.3089e-04, -2.3552e-03,  9.4509e-04,\n",
       "          1.2152e-03, -1.8188e-03, -2.8740e-03, -5.3376e-04, -3.1036e-04,\n",
       "         -2.0674e-03, -2.1062e-03,  1.1154e-03, -1.0228e-03, -4.4048e-04,\n",
       "         -1.7540e-03, -2.1513e-03, -5.7554e-04, -7.6979e-04,  1.7405e-04,\n",
       "         -2.7354e-03, -1.2934e-03, -2.9600e-04, -1.4769e-03, -1.5435e-03,\n",
       "         -1.5861e-04, -8.3524e-04, -2.0406e-03, -1.3143e-03, -2.2134e-03,\n",
       "         -2.7440e-03,  9.1934e-04, -1.8157e-03,  2.6393e-04, -1.3199e-03,\n",
       "         -9.2149e-04, -9.7620e-04, -4.6015e-05, -6.6936e-04, -1.5925e-03,\n",
       "         -9.8592e-04, -2.5625e-03,  1.9026e-04, -3.7205e-04, -1.2085e-03,\n",
       "         -3.7457e-03, -5.2738e-04, -1.8210e-03, -3.6744e-03, -4.3190e-03,\n",
       "         -8.9687e-04, -9.4908e-04, -3.0766e-03, -2.2484e-03, -1.4985e-04,\n",
       "         -1.6736e-03, -8.9681e-04, -2.9930e-03, -1.5916e-03, -2.1296e-03,\n",
       "         -2.4952e-03, -4.6515e-04, -1.1288e-03, -2.4295e-04, -1.2821e-03,\n",
       "          1.6891e-03, -2.6649e-03,  1.9002e-04, -9.0128e-04, -2.8829e-03,\n",
       "          4.7362e-04,  2.6941e-05,  6.0785e-04, -2.3107e-03, -1.2814e-03,\n",
       "          1.1215e-03, -2.2076e-03, -2.2864e-04, -2.6596e-03, -4.4417e-04,\n",
       "          5.5313e-05, -8.1050e-04, -1.4139e-03, -2.0409e-03, -1.1724e-03,\n",
       "         -1.7363e-03, -1.8945e-03, -1.3822e-03,  3.2431e-04, -2.7198e-04,\n",
       "         -2.8038e-04,  2.8938e-04,  3.1728e-04, -1.4055e-03, -7.5656e-04,\n",
       "          1.6737e-04, -5.7471e-04, -8.0526e-05, -2.3887e-03, -7.4148e-04,\n",
       "          7.1669e-04,  1.5294e-03, -1.7805e-03, -4.0298e-03, -6.9481e-04,\n",
       "         -5.8562e-05, -1.5154e-03,  9.8082e-04, -1.7029e-04, -1.8314e-03,\n",
       "         -2.2495e-03, -2.1775e-03, -1.9807e-03,  8.9943e-05,  6.3741e-04,\n",
       "         -2.3069e-03, -1.1874e-03, -3.2109e-04,  4.9138e-04,  3.0875e-05,\n",
       "          3.9756e-05, -1.0840e-03, -1.7750e-04,  2.4647e-04, -7.8130e-04,\n",
       "         -6.0564e-04,  1.5694e-04]),\n",
       " tensor([ 3.3685e-04, -2.1337e-04, -5.1837e-05,  1.2282e-04, -2.2425e-04,\n",
       "          1.3439e-04,  8.9876e-05,  6.6133e-05, -8.8535e-05, -9.6775e-05,\n",
       "          2.6461e-04, -3.5080e-04,  2.3758e-04,  7.3381e-04,  1.9555e-04,\n",
       "         -2.0250e-04, -2.1682e-04, -3.7706e-04,  6.0711e-05,  1.4176e-04,\n",
       "         -2.3525e-04, -1.3731e-04, -2.1273e-04,  2.1905e-04,  4.9715e-04,\n",
       "          6.1784e-04,  4.8596e-04,  4.6179e-04, -4.3612e-05,  3.8528e-04,\n",
       "          1.6670e-04,  6.1678e-05, -5.2191e-04, -5.2201e-04, -4.7788e-05,\n",
       "          1.0033e-04, -3.4678e-04, -6.3017e-05,  2.8993e-04, -4.0445e-04,\n",
       "         -5.3622e-05, -5.2529e-04,  3.5363e-04,  3.2898e-04, -5.0146e-05,\n",
       "         -1.7431e-05, -1.4323e-03,  1.8118e-04, -3.7020e-04, -8.4001e-04,\n",
       "          6.9738e-04,  1.4281e-04,  5.9795e-05,  7.4642e-04,  5.1820e-04,\n",
       "         -4.2862e-04,  1.8027e-04, -6.7320e-05, -1.2779e-04, -1.2142e-04,\n",
       "          1.5519e-04,  8.4978e-05,  2.8153e-04,  2.7217e-04, -4.1438e-04,\n",
       "         -5.0443e-04,  4.4520e-04,  1.3022e-04, -1.3473e-04, -3.2940e-04,\n",
       "         -7.4560e-05, -1.5056e-04,  9.1298e-05,  6.1640e-04, -8.7202e-05,\n",
       "         -4.2121e-04,  3.6327e-04,  1.1634e-04,  1.0576e-04, -2.9059e-04,\n",
       "          4.4401e-04, -1.2199e-04,  1.8914e-05, -3.1383e-04,  4.3120e-04,\n",
       "         -5.9050e-04,  1.6578e-04, -2.1550e-04, -3.2159e-04, -1.7418e-04,\n",
       "         -3.8694e-04, -7.5830e-05,  5.9056e-04, -4.8783e-06,  8.5231e-05,\n",
       "          2.1848e-04,  4.0001e-04, -1.1528e-04, -1.2377e-04, -4.8220e-04,\n",
       "         -3.8796e-04, -4.8518e-05,  8.0846e-05,  3.1336e-04,  2.2706e-04,\n",
       "          2.8049e-04,  6.2681e-04,  1.3501e-04, -9.4563e-05,  3.0508e-04,\n",
       "         -3.4533e-04, -3.0328e-05, -4.0095e-05, -2.1106e-04,  8.0606e-04,\n",
       "          1.7220e-04,  3.8339e-04,  4.9722e-04, -1.9744e-05, -2.3689e-05,\n",
       "          1.6382e-04, -3.8170e-04,  4.8643e-04, -2.2824e-04,  1.7624e-04,\n",
       "         -2.2996e-04, -3.2516e-04,  3.6623e-04,  2.0993e-04,  3.0325e-04,\n",
       "          6.5327e-04, -1.7053e-04, -4.2587e-04, -2.1451e-04, -1.8755e-04,\n",
       "          4.7022e-04,  1.4342e-04,  1.5381e-04,  1.4246e-04,  4.8404e-04,\n",
       "         -3.5500e-04, -8.3921e-05, -3.7281e-05,  2.5742e-04,  9.2890e-05,\n",
       "          3.7321e-04,  7.3723e-06, -2.5502e-04, -1.4430e-05, -3.8206e-04,\n",
       "         -1.0507e-04,  2.4296e-04, -6.0520e-05, -2.3009e-04,  4.7207e-04,\n",
       "          4.9901e-04,  1.2460e-04, -5.0036e-04, -1.5893e-04, -1.3238e-04,\n",
       "         -7.3095e-05, -3.4531e-04, -1.9438e-04,  1.6047e-04,  2.4112e-04,\n",
       "         -6.5693e-04, -2.7841e-04,  1.4083e-04,  6.0478e-05, -8.4393e-05,\n",
       "          3.9711e-04, -3.0947e-04, -1.0409e-03,  2.4858e-04,  5.2883e-04,\n",
       "         -3.8861e-04,  3.3043e-04,  2.2342e-04, -1.7764e-04, -2.0388e-04,\n",
       "          2.5472e-04, -2.7493e-04,  2.6761e-04, -1.5488e-04,  4.3408e-04,\n",
       "          4.5769e-04, -3.6293e-05,  1.7987e-04,  3.3339e-04,  3.6128e-04,\n",
       "          6.1564e-05, -1.9243e-04, -2.8597e-04, -6.1053e-04, -5.0921e-04,\n",
       "          4.0430e-04, -1.5721e-04, -9.2893e-04,  9.2357e-05,  5.6476e-04,\n",
       "         -3.8005e-05, -1.5328e-05, -2.5219e-04,  1.2618e-05, -2.7581e-04,\n",
       "          1.7756e-04,  4.2167e-04,  2.3203e-04,  1.2391e-03, -1.3972e-04,\n",
       "          3.4023e-04,  2.0470e-04, -2.7910e-04, -5.5544e-04,  2.1853e-04,\n",
       "          2.0131e-05,  5.2579e-04,  3.6595e-04,  1.3712e-04, -4.2111e-04,\n",
       "         -3.7033e-04, -2.1975e-04,  1.5131e-04, -6.2503e-04,  1.5004e-04,\n",
       "          7.1425e-05,  6.4673e-05,  7.8208e-04,  8.8112e-06, -1.7047e-04,\n",
       "          2.1520e-04, -1.9103e-05, -3.9665e-05, -3.4350e-04,  2.5088e-04,\n",
       "          1.7485e-04,  7.8832e-04,  4.4993e-04,  4.4662e-04,  4.1749e-04,\n",
       "          3.3224e-04,  1.9933e-04, -1.7811e-04,  1.4332e-04, -3.7604e-04,\n",
       "         -4.7566e-04,  1.3282e-04,  3.9869e-04,  7.1281e-04,  2.6690e-04,\n",
       "          4.9507e-05,  1.8303e-04,  7.4327e-05, -3.4026e-04,  2.1438e-04,\n",
       "         -1.2928e-04, -5.7898e-04, -2.9023e-04,  2.3581e-06, -6.8080e-04,\n",
       "          3.6473e-04,  3.7286e-05,  1.6289e-04,  8.1275e-05, -5.5800e-04,\n",
       "          5.2935e-04, -1.8929e-04, -1.1091e-04, -2.1974e-04,  2.7369e-04,\n",
       "         -8.9429e-05, -2.6667e-04, -2.3588e-04,  5.5831e-04,  1.8921e-04,\n",
       "         -1.2440e-04,  4.5235e-04,  4.3474e-06,  1.7132e-04, -4.8206e-04,\n",
       "         -4.0548e-04,  7.3630e-05,  2.1059e-04, -1.7995e-04,  1.2645e-05,\n",
       "          2.3551e-04, -3.7212e-04, -4.5155e-04, -2.8652e-04,  2.9331e-04,\n",
       "         -1.6262e-04, -3.4494e-04, -2.3326e-04,  1.3834e-05,  8.0233e-05,\n",
       "          3.4125e-04,  6.4529e-04,  1.8866e-04,  3.3690e-04,  3.8472e-04,\n",
       "          2.3816e-04,  3.2562e-04, -1.9103e-05, -1.2774e-04, -3.7867e-04,\n",
       "         -2.6271e-04,  2.1815e-05,  5.8240e-04, -4.7281e-05, -7.1686e-05,\n",
       "          1.3380e-03, -5.9072e-04,  2.9320e-04,  7.8934e-05, -4.5537e-04,\n",
       "          1.1100e-05, -2.6069e-04, -7.5273e-04, -2.1420e-06, -1.2586e-04,\n",
       "          4.0565e-04,  4.9702e-04,  1.1995e-05, -2.4730e-04, -4.0366e-04,\n",
       "         -2.0387e-04,  5.3130e-04,  5.0574e-04, -1.6898e-04, -2.8419e-04,\n",
       "          3.0231e-04, -1.9880e-04,  1.9000e-04, -1.7742e-04,  3.1344e-04,\n",
       "          3.8782e-04, -2.0861e-04,  3.1598e-04,  4.5125e-04, -1.2573e-05,\n",
       "          1.7448e-04, -3.6448e-05, -6.3489e-04,  2.4006e-04,  6.0797e-06,\n",
       "         -5.8055e-04,  3.1245e-04,  7.2262e-04, -6.6813e-05,  3.5093e-04,\n",
       "         -1.0131e-04,  4.2730e-04,  1.0138e-04,  2.1873e-04, -1.0543e-05,\n",
       "          2.2132e-04, -1.3411e-04, -1.4830e-04, -1.0664e-04, -6.9577e-05,\n",
       "         -4.0260e-04,  6.8094e-04,  2.9515e-04,  1.1355e-04,  1.2646e-04,\n",
       "         -1.7547e-04,  2.0954e-04,  1.1887e-04,  1.9757e-04, -4.9393e-05,\n",
       "          4.5477e-04, -3.9797e-04, -3.5835e-04,  1.4255e-04,  2.7061e-04,\n",
       "         -5.8554e-05,  4.8636e-04,  4.9532e-04, -2.5468e-04, -4.6579e-05,\n",
       "          4.1053e-06,  7.6871e-04,  5.1894e-04,  9.3773e-05, -1.0799e-03,\n",
       "         -1.1668e-05, -5.0436e-04, -2.5341e-04, -1.7850e-04,  2.1441e-04,\n",
       "          7.3018e-04, -8.7911e-04, -4.5798e-04, -2.5623e-05,  1.4499e-04,\n",
       "         -4.5397e-04,  1.5782e-04, -7.4101e-06,  4.6566e-06,  2.9641e-04,\n",
       "          2.9886e-04,  9.5481e-05, -4.1439e-04,  1.2391e-04,  1.9427e-04,\n",
       "         -2.8929e-04, -1.3107e-05, -5.0891e-05,  3.7332e-04,  2.0552e-04,\n",
       "          3.3569e-04, -1.7010e-04, -5.0984e-05,  1.7719e-04,  2.0940e-04,\n",
       "          3.2476e-04,  3.6269e-04,  2.7334e-04,  6.5030e-05, -6.7886e-05,\n",
       "          9.6422e-05, -2.0169e-04,  1.0019e-04, -1.8147e-04, -1.5442e-04,\n",
       "         -1.6671e-04, -1.6590e-04, -2.6989e-04,  2.2269e-04, -3.5862e-04,\n",
       "          2.4141e-04, -3.3793e-04, -5.8778e-05, -3.9041e-05, -1.4360e-04,\n",
       "          2.1029e-04,  3.7843e-04,  6.3907e-05,  3.0773e-05, -2.7543e-04,\n",
       "          5.6918e-04, -6.6571e-04,  4.3373e-05,  3.9458e-04, -4.7832e-04,\n",
       "          2.1876e-05, -3.7441e-05, -2.9131e-04,  7.6994e-04,  3.5493e-04,\n",
       "         -1.3525e-03,  4.3544e-04,  2.9132e-06,  1.0565e-04, -2.0094e-04,\n",
       "          1.5359e-04,  1.7763e-04, -1.5787e-04,  9.6239e-05,  1.4976e-04,\n",
       "          3.8563e-04, -1.7171e-04, -5.5417e-05, -3.2767e-04,  1.9309e-05,\n",
       "          3.9912e-04, -1.2478e-05,  4.6238e-04,  5.7325e-05,  5.4046e-04,\n",
       "          3.2606e-04, -8.7980e-05, -3.1947e-04, -7.6845e-05,  2.2433e-04,\n",
       "          3.2902e-04,  4.6670e-04,  1.1326e-04,  1.2062e-04, -3.5957e-04,\n",
       "         -3.9456e-04, -1.3847e-04, -4.0054e-04, -2.0008e-04,  1.3369e-04,\n",
       "          3.8108e-04,  8.4320e-04, -7.8768e-05, -1.5983e-04,  3.8051e-04,\n",
       "          1.4788e-04, -4.5173e-05, -1.5589e-04, -2.1358e-04, -2.1905e-06,\n",
       "          4.5476e-04,  1.2815e-05, -8.0776e-05, -2.3690e-04,  1.7601e-04,\n",
       "          3.6416e-04,  1.6778e-04, -1.2837e-04, -1.4421e-04, -7.7130e-05,\n",
       "         -1.1109e-03, -2.2592e-05, -6.1930e-04, -1.8937e-04,  1.8732e-04,\n",
       "         -6.3713e-04, -3.8871e-04]),\n",
       " tensor([[ 3.9280e-04, -1.7857e-03, -1.2876e-03,  ...,  3.3778e-03,\n",
       "           3.8772e-03,  5.9586e-03],\n",
       "         [ 1.3968e-03,  1.5043e-03, -1.1904e-03,  ...,  3.9326e-03,\n",
       "          -2.6323e-03, -1.1882e-03],\n",
       "         [ 1.7243e-03,  9.5000e-04,  2.7904e-03,  ..., -2.5122e-03,\n",
       "          -5.5784e-04,  2.9335e-03],\n",
       "         ...,\n",
       "         [ 1.7038e-04,  5.5348e-04,  1.8033e-03,  ..., -3.4728e-03,\n",
       "           1.1890e-03, -5.6631e-04],\n",
       "         [ 1.7977e-03,  1.0102e-04,  1.9865e-03,  ...,  1.2879e-03,\n",
       "           1.0483e-03,  9.1769e-04],\n",
       "         [-1.3622e-03,  2.2959e-03,  6.5163e-05,  ...,  5.2787e-04,\n",
       "           1.7113e-03, -1.2935e-03]]),\n",
       " tensor([ 2.4497e-04,  1.5441e-03, -5.1137e-04,  ..., -4.2770e-05,\n",
       "          8.8524e-05,  1.3977e-04]),\n",
       " tensor([[-3.0159e-04,  2.1224e-04, -2.0609e-03,  ..., -4.5225e-04,\n",
       "           4.2295e-04,  2.7664e-04],\n",
       "         [-3.6156e-04,  2.8368e-03,  2.0405e-03,  ..., -2.3848e-04,\n",
       "          -1.0744e-03,  2.3798e-03],\n",
       "         [ 2.0452e-03,  1.6345e-03, -1.9830e-03,  ..., -2.1884e-03,\n",
       "          -1.8553e-03, -8.7354e-04],\n",
       "         ...,\n",
       "         [-1.4435e-03,  2.1129e-03,  1.0529e-03,  ..., -7.8231e-04,\n",
       "          -1.7636e-05,  2.8283e-03],\n",
       "         [ 3.6141e-03,  1.7923e-03, -1.8633e-03,  ...,  7.7525e-04,\n",
       "          -3.8390e-03,  1.1804e-03],\n",
       "         [ 1.9049e-03,  7.9065e-04,  2.2509e-03,  ...,  1.2020e-03,\n",
       "          -2.4877e-03, -8.9579e-04]]),\n",
       " tensor([-5.2939e-04, -5.8147e-05,  2.5308e-04,  4.5983e-04, -8.9239e-06,\n",
       "          4.5337e-04, -2.1458e-06, -9.7204e-05, -1.9650e-04, -1.3725e-04,\n",
       "          9.1600e-05,  1.8242e-04,  3.8900e-04, -3.9337e-04,  5.8446e-04,\n",
       "         -5.0069e-04, -2.4676e-04, -5.2107e-05,  3.3421e-04, -6.7191e-04,\n",
       "         -5.4003e-04,  5.7597e-04, -9.0506e-05,  4.5844e-04,  5.7725e-05,\n",
       "         -5.3376e-04,  1.0198e-04,  1.8463e-04, -2.4880e-04,  2.4322e-04,\n",
       "         -2.3461e-04,  1.9786e-04,  8.7950e-05, -6.8997e-04, -3.6156e-04,\n",
       "          3.2836e-04,  2.3912e-04, -3.0498e-04, -7.6151e-05, -3.8048e-04,\n",
       "         -1.2591e-04,  1.6867e-04,  2.5142e-05, -4.0836e-04, -2.5246e-04,\n",
       "         -9.9991e-05,  5.1490e-04,  2.2184e-04,  1.0726e-04,  3.6425e-04,\n",
       "         -3.9111e-04,  1.0749e-04, -4.5495e-04, -2.1556e-04, -2.5689e-05,\n",
       "         -8.2539e-05,  6.4673e-05, -4.2176e-04, -4.3644e-04, -4.5448e-04,\n",
       "          2.9122e-04, -2.6451e-05, -4.6427e-04, -3.2397e-04,  1.6183e-04,\n",
       "         -1.8612e-04,  1.7932e-04,  6.7388e-05,  3.4415e-04, -2.8928e-04,\n",
       "         -2.2842e-05,  1.4570e-04,  4.3057e-04, -5.1752e-04,  2.0508e-04,\n",
       "         -2.4297e-04,  4.8398e-05,  3.1997e-04,  3.4933e-04, -1.7211e-06,\n",
       "         -5.9252e-05, -5.9185e-05, -6.7521e-06, -2.5497e-04, -4.4224e-04,\n",
       "          5.6286e-05,  8.9841e-05,  4.2937e-04, -8.1167e-05, -2.0938e-05,\n",
       "          8.4924e-05, -2.2867e-04, -2.6707e-04, -5.8724e-04,  1.2085e-04,\n",
       "         -5.5761e-04, -1.4848e-04, -7.2889e-05, -2.5877e-04,  5.9588e-05,\n",
       "          1.7969e-04,  4.4784e-04, -4.5497e-04, -6.7831e-04, -6.1798e-04,\n",
       "         -2.2383e-04, -2.4930e-04,  6.4404e-04, -9.4293e-05, -4.0552e-04,\n",
       "         -1.0905e-04,  2.9688e-05, -7.6713e-05, -2.6328e-04, -5.1332e-04,\n",
       "         -2.4086e-04, -4.2215e-05,  3.3887e-04,  3.5004e-05, -5.0033e-04,\n",
       "          1.5363e-04,  1.1732e-04, -3.1970e-04, -3.0246e-04, -5.2509e-04,\n",
       "         -4.5954e-04,  5.8874e-05,  3.9435e-04,  1.7795e-05,  2.2506e-04,\n",
       "          6.3281e-05, -1.7974e-04,  1.9034e-04,  2.3864e-04, -4.6064e-04,\n",
       "         -1.2684e-04,  5.0182e-04,  1.6457e-04, -2.8020e-04, -4.4423e-04,\n",
       "         -8.2429e-05, -2.7705e-04, -4.1265e-04, -2.0176e-05,  1.5874e-04,\n",
       "         -4.5047e-04, -1.1691e-04,  1.5659e-05,  3.2566e-04,  1.7253e-05,\n",
       "         -1.8836e-04,  1.2006e-04, -2.2108e-05, -5.9360e-04, -7.2537e-04,\n",
       "         -3.3207e-04, -9.7765e-06,  2.0661e-04, -4.6296e-05,  1.7999e-04,\n",
       "          3.3257e-04, -6.9257e-04,  5.1168e-04,  2.6576e-04, -3.8425e-04,\n",
       "          1.5890e-04,  5.7472e-04, -2.8169e-04,  5.5193e-04,  7.2098e-04,\n",
       "          1.5747e-05,  5.1328e-05,  3.3988e-04,  2.5280e-04,  2.9893e-04,\n",
       "         -1.5377e-04,  1.2039e-05,  7.5784e-05, -3.1529e-04, -2.2858e-04,\n",
       "         -1.8983e-04,  1.2935e-04,  2.4795e-04,  4.2159e-04, -1.6868e-04,\n",
       "          4.7825e-04,  9.8724e-05,  4.1442e-04, -1.4167e-04, -5.9928e-04,\n",
       "         -2.7167e-04, -6.2239e-04,  3.1151e-04,  1.2299e-04,  1.2404e-04,\n",
       "         -8.8660e-05,  8.1350e-04,  2.6651e-04,  2.3886e-04, -2.7101e-04,\n",
       "         -1.4824e-04,  3.2237e-04,  1.0169e-03, -2.9593e-04,  2.6342e-04,\n",
       "         -7.8889e-04, -2.5784e-04,  7.7070e-04, -3.6618e-04, -4.5279e-04,\n",
       "          1.4138e-04, -2.9689e-04,  2.9369e-04,  9.7024e-05,  4.8450e-05,\n",
       "         -5.1793e-04, -7.0391e-04,  1.0722e-04,  2.9241e-05,  2.8096e-05,\n",
       "          1.8456e-04, -2.7555e-04,  5.4700e-05,  4.3737e-04,  2.4816e-05,\n",
       "         -1.8344e-04, -3.7062e-04, -9.1455e-04, -3.4374e-04, -2.0685e-04,\n",
       "         -4.4729e-04,  1.4523e-04,  2.4227e-04,  5.9243e-04,  8.8655e-05,\n",
       "          4.5746e-05, -1.2719e-04, -1.7464e-04,  5.5040e-04, -8.6911e-05,\n",
       "          7.1731e-04,  8.3113e-04,  1.9796e-04,  4.6491e-05,  2.0040e-05,\n",
       "          4.3869e-04, -5.6116e-04, -3.3588e-04,  2.4560e-05, -2.9303e-04,\n",
       "          3.3847e-05, -1.0912e-05,  3.6282e-04,  1.0478e-04, -1.1858e-04,\n",
       "         -1.2302e-04,  2.8831e-04, -3.1455e-04, -1.1944e-05, -1.9358e-04,\n",
       "          9.3325e-05,  1.1437e-05,  2.3712e-04, -1.2308e-05,  2.0413e-04,\n",
       "         -2.4607e-04, -3.5778e-04, -2.0115e-04,  3.3601e-04,  1.2373e-05,\n",
       "         -2.8331e-04,  3.8404e-04, -4.8755e-04, -3.5213e-04,  4.4822e-04,\n",
       "         -5.3905e-04,  5.6191e-04, -1.9646e-04, -1.6600e-04, -2.6840e-04,\n",
       "         -1.6292e-04,  2.1511e-04,  6.9951e-04,  1.8125e-04,  6.9404e-04,\n",
       "         -2.8782e-04,  1.2207e-04, -6.4826e-05, -1.5588e-04,  1.4403e-04,\n",
       "         -8.4255e-05,  5.7305e-04,  4.0354e-06, -2.5052e-05, -2.9549e-04,\n",
       "         -1.4967e-04,  4.1921e-04,  6.1565e-04, -3.6110e-04, -2.5257e-04,\n",
       "          4.2816e-05, -1.2629e-04, -3.7387e-04,  1.6481e-05, -6.4219e-05,\n",
       "          7.6872e-04, -5.4895e-04, -4.7078e-04, -5.4942e-05,  2.3283e-04,\n",
       "         -4.3070e-05, -2.4560e-04, -1.8100e-04,  1.5253e-04,  3.7644e-04,\n",
       "          2.2272e-05,  1.2985e-05,  4.0340e-04, -2.0199e-04,  8.3845e-05,\n",
       "          3.0468e-04, -1.4886e-04,  7.4618e-06, -1.9319e-04,  8.1087e-05,\n",
       "         -2.3997e-05, -2.9795e-04, -3.6324e-04,  1.5208e-04, -2.9417e-04,\n",
       "         -9.6031e-05,  1.1310e-04, -4.8164e-05,  2.0754e-04,  4.5147e-04,\n",
       "         -8.8062e-05, -1.6205e-04,  2.9488e-04, -2.9575e-04,  1.0061e-04,\n",
       "         -4.0620e-04,  3.2572e-04,  6.2362e-04, -3.0883e-04, -1.4069e-04,\n",
       "         -2.0560e-04,  5.4995e-05, -5.0666e-05,  1.2698e-04,  1.9954e-05,\n",
       "          4.1053e-04, -3.5763e-04, -4.4416e-05, -8.7977e-05, -2.0850e-04,\n",
       "         -6.4261e-07,  3.1640e-04,  4.5521e-04,  4.4092e-04,  8.3364e-04,\n",
       "         -4.0707e-05,  2.6781e-04, -3.0787e-04, -1.5975e-04, -5.8944e-04,\n",
       "          9.4340e-05,  2.1891e-04,  1.2626e-04, -1.9779e-04, -7.0911e-06,\n",
       "          2.9714e-04,  3.4577e-05, -6.0976e-04, -2.6306e-04,  8.4561e-05,\n",
       "         -2.0889e-04, -3.8184e-06, -4.0481e-05,  3.9994e-04,  6.3595e-04,\n",
       "          1.6778e-04, -1.6081e-04,  1.8644e-04, -3.8334e-04, -1.3057e-05,\n",
       "          3.3086e-04, -3.1526e-04, -8.7846e-05, -2.3315e-04,  1.0303e-04,\n",
       "         -4.8744e-04,  1.2774e-04, -1.3037e-04,  3.5132e-05, -2.1063e-04,\n",
       "         -2.3134e-04,  1.8694e-04, -4.2341e-05, -1.6145e-04,  1.7645e-04,\n",
       "          2.3891e-04,  1.4704e-04,  6.1927e-04, -6.7782e-04, -1.9914e-04,\n",
       "         -3.6012e-04,  2.3147e-04, -6.5573e-05, -2.3576e-04, -5.3077e-04,\n",
       "         -1.9774e-04, -8.3928e-05, -5.1372e-04, -3.5931e-04,  2.9484e-04,\n",
       "         -1.4409e-04,  6.5213e-05, -5.1431e-04, -2.6206e-05, -1.7116e-04,\n",
       "          1.5439e-04, -4.4261e-05, -1.6855e-04,  2.1786e-04,  1.0895e-04,\n",
       "         -6.6843e-05, -3.8474e-04, -2.9917e-04, -5.1419e-04,  1.2699e-04,\n",
       "         -8.7139e-05,  5.0466e-04,  9.8043e-05,  1.5801e-04, -4.1917e-04,\n",
       "         -4.4458e-04,  4.0149e-04, -6.2062e-04,  5.5034e-05, -6.4628e-05,\n",
       "          5.2855e-04,  5.8628e-04, -9.9706e-05, -8.4342e-05, -9.0266e-04,\n",
       "          3.3125e-05, -3.9731e-04, -2.7248e-04, -2.2742e-04, -1.3231e-04,\n",
       "          3.4099e-03, -1.6227e-04, -2.0862e-04, -3.9984e-04, -2.7400e-04,\n",
       "          5.3184e-04,  4.8828e-04, -4.3347e-04, -2.4126e-04, -4.4910e-04,\n",
       "         -2.1768e-04,  8.1467e-06,  2.6776e-04, -8.2074e-05,  3.1478e-04,\n",
       "         -3.8065e-05, -7.9352e-05, -4.9051e-04, -2.6866e-04, -2.7621e-04,\n",
       "         -2.6980e-04,  1.6881e-04,  8.9543e-05, -4.8527e-05, -1.1562e-04,\n",
       "          2.6386e-05,  3.3635e-04,  6.7515e-04,  9.1862e-05,  4.2269e-04,\n",
       "          3.3991e-04, -1.5038e-04,  3.6894e-04, -4.7984e-05, -7.2307e-05,\n",
       "         -6.9723e-05, -4.4813e-04, -2.3480e-04, -8.1787e-05,  4.2410e-04,\n",
       "         -3.6831e-04, -3.6684e-04,  2.9612e-04,  1.5316e-04,  1.2183e-04,\n",
       "         -1.7799e-04,  5.5611e-04,  1.1532e-04,  3.1929e-04, -3.3005e-04,\n",
       "         -1.7876e-04,  7.2568e-05,  6.9885e-05,  3.3830e-04, -7.2159e-05,\n",
       "          3.8071e-04, -1.4742e-04, -1.5580e-04,  5.8065e-04,  3.2007e-04,\n",
       "          9.4799e-05, -2.9458e-04]),\n",
       " tensor([ 3.2969e-03, -2.5134e-03, -1.1733e-03, -2.2811e-03,  1.0386e-03,\n",
       "         -2.4998e-04,  8.8394e-04,  1.7346e-03,  7.8356e-04, -1.1322e-03,\n",
       "         -2.3381e-03, -1.0196e-03,  2.1240e-03, -2.4568e-03,  2.1297e-03,\n",
       "          1.0304e-03, -2.3735e-04, -1.8430e-03,  2.5508e-03, -1.5519e-03,\n",
       "         -1.8764e-04, -6.6102e-05,  7.4410e-04,  1.6978e-03, -6.6543e-04,\n",
       "         -1.9389e-03,  1.9276e-04,  6.8378e-04, -1.1314e-03,  1.4049e-03,\n",
       "         -3.2905e-03,  3.1078e-03,  2.0781e-03, -4.4334e-04,  2.2197e-03,\n",
       "         -6.3723e-04, -3.0124e-04,  1.3826e-03,  9.8681e-04,  1.6882e-03,\n",
       "         -8.1313e-04,  2.7082e-03, -2.6886e-03, -1.5793e-03,  1.2397e-03,\n",
       "          2.2459e-04, -2.7847e-04, -2.2123e-03,  8.8429e-04,  9.1147e-04,\n",
       "          2.7657e-03,  1.0562e-03,  3.0280e-03, -6.6626e-04, -3.1424e-04,\n",
       "          2.5915e-03,  8.7249e-04, -1.0317e-03,  6.8462e-04,  1.4462e-03,\n",
       "         -9.5081e-04, -4.6039e-04, -2.5160e-03, -1.1253e-03,  1.4970e-03,\n",
       "         -7.1979e-04, -1.2469e-04,  1.0954e-03,  7.3612e-04, -1.3912e-04,\n",
       "         -3.5012e-04,  2.2268e-04,  1.6840e-03,  9.0241e-05, -3.5298e-04,\n",
       "         -6.2305e-04, -1.7166e-04, -2.5123e-03,  3.6913e-04,  2.3601e-03,\n",
       "          6.4802e-04, -7.2801e-04,  6.8009e-04, -1.1666e-03,  2.9055e-03,\n",
       "         -2.9755e-04,  3.3623e-04, -7.9751e-05,  1.6533e-03,  1.2026e-03,\n",
       "          2.5535e-03,  4.7421e-04,  1.7680e-03, -3.0917e-04,  3.3456e-03,\n",
       "          1.5686e-03,  1.3334e-03,  2.2495e-04, -1.6010e-04,  2.8610e-04,\n",
       "          7.0333e-06,  3.2635e-03,  6.0296e-04, -5.9319e-04,  2.3570e-03,\n",
       "         -1.4198e-04,  8.4460e-04,  1.9264e-03, -1.1625e-03,  7.2801e-04,\n",
       "          3.4046e-03, -1.5134e-03,  8.0585e-05,  2.5743e-03, -3.5476e-03,\n",
       "         -5.0461e-04, -4.2820e-04,  3.2443e-04,  3.2663e-04,  2.0379e-03,\n",
       "          1.5768e-03,  6.5851e-04,  3.1527e-03,  2.9266e-04, -8.6010e-04,\n",
       "          8.4400e-05,  2.5846e-03, -1.0352e-03,  8.8334e-05,  1.3715e-03,\n",
       "          4.3344e-04, -1.2100e-04, -9.5963e-04,  2.1688e-03, -2.3341e-04,\n",
       "         -5.9605e-04, -7.7307e-04, -6.6817e-04,  1.8653e-03, -2.9896e-03,\n",
       "          5.8031e-04,  2.9521e-03, -1.6654e-04,  6.8420e-04,  8.2135e-05,\n",
       "          4.6682e-04, -2.4101e-03, -1.9062e-04,  1.0436e-03, -9.1732e-04,\n",
       "         -4.7910e-04, -1.3912e-04, -2.8437e-04,  1.4377e-03, -2.1874e-03,\n",
       "          2.2836e-03,  1.0037e-03,  7.8940e-04,  3.4285e-04,  1.4270e-03,\n",
       "          9.2113e-04, -2.2560e-03,  1.6135e-03, -6.7687e-04, -5.3811e-04,\n",
       "          9.6405e-04, -1.8193e-03,  5.4806e-04, -6.1202e-04, -6.9141e-04,\n",
       "          2.6715e-03, -2.0760e-03,  1.4762e-03,  4.9496e-04,  3.4070e-04,\n",
       "          1.0823e-03, -4.9984e-04,  3.7754e-04,  1.6689e-04,  6.5756e-04,\n",
       "          7.4720e-04,  1.4567e-04,  1.2978e-03,  1.1798e-03, -6.6817e-04,\n",
       "          4.6664e-03,  6.7687e-04, -1.2134e-03,  1.7138e-03,  1.3386e-03,\n",
       "         -2.5117e-04,  6.4969e-05,  1.6611e-03, -9.0808e-04, -2.5695e-03,\n",
       "         -1.3590e-04,  1.2218e-03, -2.1193e-03, -7.9286e-04, -4.1032e-04,\n",
       "          1.8998e-03,  9.1553e-04,  8.8358e-04,  7.7212e-04,  7.4148e-04,\n",
       "         -1.2351e-03,  1.1512e-03, -1.3412e-03,  1.0626e-03, -2.7531e-04,\n",
       "         -9.0009e-04,  1.3726e-03,  2.5820e-03, -6.2323e-04,  7.4542e-04,\n",
       "          2.7822e-03, -9.5952e-04,  1.2647e-03, -3.8409e-04,  8.5914e-04,\n",
       "          4.6372e-05,  5.1689e-04,  2.4939e-04,  2.0353e-03,  1.2847e-03,\n",
       "          9.1290e-04, -1.4197e-03,  7.5102e-04,  2.2604e-03,  5.3167e-04,\n",
       "         -3.1298e-04,  1.5073e-03, -1.0483e-03,  2.5780e-03,  2.1893e-03,\n",
       "         -7.1526e-06, -2.6612e-03,  8.4019e-04,  1.0453e-03, -7.5471e-04,\n",
       "         -1.3534e-03, -8.5115e-05,  1.3685e-04,  1.4431e-03,  1.8435e-03,\n",
       "          1.8857e-03,  2.0115e-03,  4.1020e-04,  3.2544e-04,  3.3332e-03,\n",
       "          3.2835e-03,  2.4282e-03,  2.1839e-03,  1.2004e-04,  1.3791e-03,\n",
       "          4.8739e-04, -7.4470e-04,  1.1169e-03, -3.8797e-04,  3.0833e-03,\n",
       "         -1.5254e-03, -9.4771e-04, -6.0797e-05,  3.1545e-03,  4.3714e-04,\n",
       "         -1.5262e-03, -1.8480e-03,  6.9976e-04, -2.7061e-04, -3.2818e-03,\n",
       "         -1.1306e-03, -1.1451e-03,  1.2647e-03, -1.6118e-03,  2.1589e-04,\n",
       "          9.4634e-04,  3.3617e-05, -4.2450e-04,  1.0127e-03,  2.5926e-03,\n",
       "          4.5931e-04,  1.4275e-03, -9.9051e-04, -2.0862e-05, -3.0440e-03,\n",
       "         -1.1509e-03,  3.1180e-03,  2.8580e-03, -2.4487e-03,  1.5407e-03,\n",
       "         -8.1623e-04, -1.3777e-03,  4.4918e-04, -3.4225e-04, -9.7537e-04,\n",
       "          2.1032e-03, -3.5505e-03, -5.9712e-04,  1.6370e-03, -2.6345e-04,\n",
       "          1.1671e-04,  9.2232e-04,  1.6097e-03,  2.9597e-03, -3.0239e-03,\n",
       "          3.9709e-04, -1.2574e-03,  3.9439e-03,  1.5503e-03, -9.4581e-04,\n",
       "         -1.3970e-03,  2.7816e-03,  2.0090e-03,  1.8679e-03, -5.5057e-04,\n",
       "         -2.5165e-03,  2.7370e-04,  3.6389e-04,  2.8396e-04,  1.6952e-03,\n",
       "         -1.8771e-03, -1.9013e-03,  1.5748e-03,  1.4080e-03, -1.9766e-03,\n",
       "          1.2870e-03, -1.5583e-03, -3.1084e-03, -8.5902e-04,  7.6413e-04,\n",
       "          2.3192e-03,  1.2666e-03,  2.7540e-03, -4.8065e-04,  1.5439e-03,\n",
       "          6.5202e-04, -1.0104e-03, -1.6297e-03,  5.9319e-04,  2.3460e-04,\n",
       "          1.6844e-04, -3.2425e-04, -1.6272e-04,  1.4006e-03,  1.2976e-03,\n",
       "         -1.5744e-03,  3.2532e-04,  2.9156e-03,  1.3740e-03,  1.7107e-03,\n",
       "          1.9276e-04, -7.7510e-04,  1.1791e-03,  1.2848e-03, -1.0347e-04,\n",
       "          1.3222e-03, -2.8336e-03, -1.3628e-03, -1.7037e-03,  1.2503e-03,\n",
       "          1.8216e-03,  1.0319e-03, -1.1483e-03,  2.2906e-04, -2.6041e-04,\n",
       "          1.8784e-03,  1.4383e-03,  9.4652e-04, -5.3525e-04,  7.4196e-04,\n",
       "         -1.2124e-03, -6.3324e-04,  2.3401e-04,  1.7067e-03, -2.4762e-03,\n",
       "          7.5483e-04,  1.7033e-03,  3.4894e-03, -1.0031e-03, -4.6062e-04,\n",
       "          2.3143e-03, -8.1086e-04, -8.2421e-04,  4.3504e-03, -1.7202e-04,\n",
       "          2.3383e-04,  1.7421e-03,  6.2346e-04, -1.6577e-03,  2.4184e-03,\n",
       "          6.2156e-04,  5.4997e-04,  8.1015e-04, -1.8215e-04,  1.1597e-03,\n",
       "          6.2686e-04, -2.8427e-03,  2.2461e-03,  1.9588e-03, -5.5611e-04,\n",
       "          4.8542e-04,  2.2089e-04, -2.1058e-03,  5.5850e-04, -5.4502e-04,\n",
       "         -3.1745e-04, -7.4142e-04, -8.9723e-04,  8.6343e-04,  3.5924e-04,\n",
       "          7.2145e-04,  9.8777e-04, -2.6798e-04, -7.6658e-04,  9.6679e-04,\n",
       "         -7.8976e-04,  3.8743e-05,  6.6161e-05, -1.2171e-04,  3.6225e-03,\n",
       "          1.3317e-03,  8.3566e-04, -3.4654e-04, -4.7404e-04,  3.9339e-05,\n",
       "         -1.8421e-03, -1.6926e-03,  1.6459e-03, -9.6470e-04,  2.4915e-03,\n",
       "         -2.5059e-03, -2.7120e-04,  1.3722e-03, -8.1766e-04,  3.1805e-04,\n",
       "          3.6951e-03,  1.1005e-03, -7.7021e-04,  3.2734e-03,  1.0557e-03,\n",
       "          5.7101e-04,  4.8875e-03, -8.7523e-04,  3.4553e-03, -2.7065e-03,\n",
       "         -1.0394e-03, -5.4610e-04,  2.2457e-03,  1.2922e-03, -1.1945e-04,\n",
       "          2.2988e-03,  2.7873e-03,  2.2501e-03,  4.6563e-04, -5.8848e-04,\n",
       "          1.3760e-03,  9.1755e-04,  9.4277e-04,  1.2293e-03, -1.5068e-03,\n",
       "         -7.2324e-04,  7.5191e-04,  4.6074e-05, -1.5945e-03,  2.1207e-04,\n",
       "          3.4809e-04,  1.6459e-03,  4.0722e-04, -5.1725e-04,  4.5753e-04,\n",
       "         -3.6364e-03, -1.7524e-04,  2.4582e-03,  9.1529e-04, -3.3939e-04,\n",
       "          7.8130e-04, -4.5073e-04,  2.9457e-04, -5.9664e-04, -2.6929e-04,\n",
       "         -4.5013e-04, -1.0244e-03, -2.1560e-03,  1.1895e-03, -1.1257e-03,\n",
       "          2.6267e-03, -1.9733e-03,  3.9240e-03,  2.4551e-03,  3.1292e-03,\n",
       "         -5.2464e-04,  1.9580e-03,  1.1533e-04,  3.5810e-04,  3.2729e-03,\n",
       "          1.9166e-03,  1.5991e-03,  2.1898e-03,  5.6493e-04,  5.9772e-04,\n",
       "          4.7636e-04,  3.0832e-03,  2.2105e-03,  2.5342e-03, -2.9355e-04,\n",
       "         -2.2816e-03,  3.1972e-04, -1.4800e-04,  2.5243e-03,  3.3727e-03,\n",
       "         -1.6820e-04, -8.1277e-04]),\n",
       " tensor([-9.5302e-05,  8.5998e-04,  4.5335e-04, -2.3177e-04,  5.5788e-04,\n",
       "          3.2537e-04,  9.5637e-05,  5.4509e-04,  1.5087e-05,  9.0785e-05,\n",
       "         -3.3045e-04, -7.4476e-05, -6.2098e-04,  4.9553e-05, -6.1100e-04,\n",
       "          2.3751e-04, -4.0354e-04, -1.7287e-04, -8.1599e-05,  1.3539e-04,\n",
       "          1.5637e-04,  5.9025e-04, -2.7763e-05,  3.5271e-05,  6.1633e-04,\n",
       "          2.0542e-04,  2.3161e-04,  3.8591e-04,  4.9789e-04, -3.5482e-04,\n",
       "         -1.1126e-04,  1.6288e-04, -8.2469e-06, -3.3289e-04,  7.3238e-04,\n",
       "          8.0297e-04, -2.2172e-04,  8.0296e-04, -1.2429e-04,  1.3439e-04,\n",
       "         -3.8677e-04,  2.5038e-05,  5.1789e-04, -2.2250e-04,  2.5738e-04,\n",
       "          8.4341e-06, -1.2176e-04, -2.5517e-04,  9.8533e-04,  7.3241e-04,\n",
       "          3.8328e-04, -7.9639e-04,  2.8103e-04, -4.6364e-04,  3.4269e-04,\n",
       "         -1.0590e-03,  9.1821e-05, -9.9344e-05, -1.9320e-04,  1.6335e-04,\n",
       "          1.9719e-04,  6.6841e-04, -1.2567e-03,  7.9729e-05,  4.1101e-04,\n",
       "          8.0392e-04,  4.2530e-04, -7.4470e-04, -3.3142e-05, -4.7990e-04,\n",
       "         -5.4514e-04, -2.2311e-04,  2.0663e-04,  5.5791e-04, -5.8498e-04,\n",
       "          4.0550e-04,  4.4064e-04,  4.7113e-04,  2.1609e-05,  4.9356e-04,\n",
       "          5.4894e-04,  7.0965e-04, -1.7615e-04,  9.7935e-04,  3.4649e-04,\n",
       "          8.2889e-04,  4.3722e-04, -1.2188e-04,  3.6858e-05,  3.1347e-04,\n",
       "          4.0887e-04, -6.9190e-04, -9.5499e-04, -3.2756e-04, -5.9217e-05,\n",
       "         -4.5101e-04,  7.7874e-04,  2.5686e-04, -1.2278e-04,  8.4496e-04,\n",
       "         -1.2538e-04,  2.9989e-04,  5.1280e-04, -4.0145e-04, -2.7193e-04,\n",
       "          3.8225e-05, -8.1167e-04,  2.4236e-04,  6.5416e-05,  5.9702e-05,\n",
       "          7.3483e-04, -2.1788e-04, -4.0847e-04,  4.8096e-04, -2.8411e-04,\n",
       "          2.1621e-04, -1.4255e-04,  1.3195e-04,  8.5056e-04,  7.7923e-04,\n",
       "         -2.2904e-04, -5.5814e-04, -1.0535e-04, -1.3883e-04,  7.5004e-04,\n",
       "          1.0268e-03,  7.4766e-04,  2.4311e-04, -7.7810e-04,  1.2835e-04,\n",
       "          2.1018e-04,  6.9856e-04,  3.9861e-04,  9.3590e-05, -9.7180e-06,\n",
       "         -6.0775e-04, -5.4157e-04,  4.5604e-04, -2.8008e-04, -7.8991e-05,\n",
       "          3.4057e-04, -2.9033e-04,  2.1497e-04,  5.6997e-04,  4.6996e-04,\n",
       "          7.2055e-05,  4.1686e-05, -1.4440e-04,  1.7565e-04, -5.7219e-04,\n",
       "         -5.5427e-04, -7.3142e-04,  1.2704e-04,  1.9313e-04,  6.6612e-05,\n",
       "         -3.0423e-04,  9.1968e-06,  5.8220e-05,  2.5720e-04,  3.8286e-04,\n",
       "          1.7541e-04, -1.8559e-04,  3.6756e-04, -1.4183e-03, -1.8999e-04,\n",
       "         -1.3368e-04, -3.0131e-04,  1.6030e-04,  6.8510e-04, -2.4974e-05,\n",
       "          2.4117e-04, -2.9474e-04,  1.7125e-04,  7.5235e-04, -4.0330e-04,\n",
       "          1.0262e-04,  2.4222e-04,  6.3969e-04, -4.4096e-04,  3.3836e-04,\n",
       "         -6.7602e-04,  7.5810e-06, -1.6206e-04,  4.2271e-05,  7.5693e-04,\n",
       "         -3.7496e-04, -3.3222e-05, -7.8645e-04, -1.7499e-04,  8.8183e-05,\n",
       "         -6.3136e-04, -3.8531e-04,  2.6377e-04, -2.8996e-04, -4.3667e-04,\n",
       "          8.2372e-04,  2.8852e-04,  4.1811e-04,  1.4624e-04,  6.0076e-04,\n",
       "          6.4596e-04, -4.3613e-04,  2.8072e-04, -3.4439e-04,  1.1727e-05,\n",
       "         -4.2950e-04,  1.4810e-04,  1.3566e-05, -1.3299e-03, -5.7101e-04,\n",
       "          2.4085e-04,  6.7815e-05, -7.9820e-04,  2.7111e-06,  7.8067e-05,\n",
       "         -6.3954e-04,  4.3625e-04,  8.5387e-04, -2.3456e-04,  4.1984e-05,\n",
       "         -1.8312e-04, -9.9419e-04,  3.7661e-04,  1.9257e-04,  8.9898e-04,\n",
       "         -4.0003e-04, -6.0477e-04, -2.7797e-04,  2.2281e-04, -2.5099e-04,\n",
       "         -3.5220e-04,  3.9703e-04,  9.1579e-05,  6.8567e-04, -9.2001e-04,\n",
       "         -3.7050e-04, -8.0533e-04, -3.7446e-04, -7.7157e-04, -4.7931e-04,\n",
       "          1.0628e-04, -1.1154e-03,  5.3115e-04, -8.8971e-05,  3.2328e-04,\n",
       "          3.3045e-04,  2.2943e-04,  1.0356e-04,  1.9815e-04,  1.6066e-04,\n",
       "         -2.7906e-05,  2.5026e-04,  3.6771e-04,  8.4302e-04, -4.9921e-04,\n",
       "          4.7527e-04, -2.4984e-04, -1.1108e-04,  1.7142e-04,  6.2255e-04,\n",
       "         -1.0808e-04, -1.4755e-04, -6.1753e-04,  4.2611e-05,  7.3755e-04,\n",
       "         -8.8965e-04,  3.1333e-04,  5.1781e-04,  8.4106e-04, -4.9095e-04,\n",
       "          5.3908e-04,  2.0843e-05, -4.0516e-04, -2.5900e-04,  3.7267e-04,\n",
       "          7.8185e-04,  5.5755e-04, -8.5242e-04,  1.6155e-05,  1.0835e-03,\n",
       "          4.0920e-04, -3.0181e-04,  7.8379e-04, -3.0048e-04,  7.7206e-04,\n",
       "          9.0468e-04,  2.8202e-04,  5.8739e-04, -6.2568e-04,  1.6081e-04,\n",
       "         -4.2239e-04, -1.5737e-04, -4.1189e-05, -2.4656e-04, -2.0311e-04,\n",
       "          1.0175e-04, -6.3035e-04, -5.6711e-04,  6.4716e-05,  1.5854e-04,\n",
       "         -9.7401e-05,  6.6295e-04,  2.9473e-04, -6.9843e-04,  3.5649e-04,\n",
       "          1.2862e-05,  2.3657e-05, -3.3017e-05, -4.1796e-04, -1.2359e-04,\n",
       "         -1.7037e-04,  5.7573e-04, -9.1422e-04, -5.6251e-04,  9.2455e-04,\n",
       "          4.3229e-04,  5.0896e-05,  7.6977e-04, -5.4035e-05, -3.0104e-04,\n",
       "          3.5919e-04, -4.6083e-04,  3.6654e-04,  4.5435e-04,  5.9083e-04,\n",
       "          5.4790e-04,  1.3043e-04, -3.4396e-04,  2.3755e-04, -3.9175e-04,\n",
       "         -8.1327e-05, -3.2166e-04, -1.7799e-04,  1.2019e-03,  9.9460e-04,\n",
       "          8.2007e-04,  3.1677e-04, -6.1089e-04, -6.1665e-04,  3.1053e-04,\n",
       "         -9.0976e-04, -2.2108e-04,  1.0375e-06,  3.5962e-05, -7.6152e-04,\n",
       "          4.5092e-04, -7.3849e-04, -7.7020e-04, -6.1228e-04,  2.3932e-04,\n",
       "          3.0566e-05, -5.6295e-04, -3.9421e-05, -9.2159e-04,  6.9430e-04,\n",
       "         -2.2458e-04, -3.6478e-04,  7.7133e-04,  5.1370e-04,  4.4676e-04,\n",
       "          2.1285e-05, -1.9577e-04,  1.3859e-04,  6.0860e-05,  1.6991e-04,\n",
       "          3.0754e-04,  7.4511e-05, -8.2666e-04,  6.3952e-05,  4.1433e-04,\n",
       "          4.9012e-05,  5.9896e-04, -2.1552e-04,  3.2345e-04,  4.9968e-04,\n",
       "         -8.8128e-04, -9.0364e-04, -4.3964e-04,  1.4184e-03, -3.0527e-04,\n",
       "          5.2412e-04,  3.1478e-04, -5.2070e-04, -3.8410e-04,  7.6650e-04,\n",
       "         -1.1253e-04, -9.8657e-05,  5.3512e-04,  5.1338e-04, -3.4612e-04,\n",
       "          8.9340e-04, -8.4569e-04,  3.9829e-04,  8.9139e-04,  2.5533e-05,\n",
       "          7.3413e-04, -1.7629e-04,  1.9806e-04,  6.1237e-04, -3.9629e-04,\n",
       "          1.8781e-04, -2.9847e-04,  3.3832e-04, -7.6084e-04, -8.7163e-04,\n",
       "         -2.6405e-05,  1.3003e-04,  3.5490e-04, -2.8697e-04,  3.2369e-04,\n",
       "         -1.7550e-04, -4.6650e-05, -5.5708e-04, -1.9627e-04, -1.1506e-04,\n",
       "          5.4390e-04,  1.8214e-04, -9.7077e-04,  1.1056e-03,  3.2028e-05,\n",
       "         -2.8318e-04, -4.4510e-05, -8.7767e-04,  2.5477e-04,  1.4196e-04,\n",
       "          3.9591e-04, -2.4855e-04,  3.2865e-04,  4.7217e-04,  5.5281e-04,\n",
       "         -8.4062e-04,  1.5376e-04,  6.9482e-04,  4.4291e-04, -2.5341e-04,\n",
       "         -9.6465e-04,  6.7623e-04, -2.9547e-04,  1.4503e-04, -5.1286e-05,\n",
       "         -4.0433e-04,  3.8550e-04, -8.9951e-04,  1.1711e-03, -7.4822e-04,\n",
       "          9.2862e-04,  4.5121e-05, -4.1538e-04, -1.1071e-03,  5.5790e-04,\n",
       "         -2.1656e-03, -2.5620e-04,  2.3266e-04, -2.7334e-04,  4.1462e-04,\n",
       "         -4.0271e-04, -6.6876e-04,  1.8265e-04,  2.6106e-04, -3.4413e-04,\n",
       "         -3.0997e-04,  3.9008e-04,  2.9869e-04,  8.7108e-04,  6.2356e-04,\n",
       "         -1.0644e-03, -7.7529e-05, -2.1909e-04,  2.1793e-04,  1.3925e-04,\n",
       "         -3.3192e-04,  6.1056e-04,  3.2143e-04,  1.0391e-04,  4.6458e-04,\n",
       "         -4.9575e-04, -4.8867e-05,  5.2361e-04,  3.2373e-06,  2.5975e-04,\n",
       "         -5.4574e-04, -2.4818e-04, -2.1473e-04,  3.5617e-04, -2.8592e-06,\n",
       "         -4.3480e-05, -8.6288e-04,  7.9715e-04,  1.4045e-04,  3.4965e-04,\n",
       "         -4.4601e-04, -1.0453e-04,  7.7826e-04,  2.6671e-04, -3.0337e-04,\n",
       "         -4.7950e-04,  1.9266e-04, -1.8295e-04, -1.3608e-05, -7.8274e-04,\n",
       "         -3.8234e-04,  2.9282e-04, -3.2029e-04,  5.5901e-04,  1.7391e-04,\n",
       "          4.2556e-04, -4.8542e-04,  9.4505e-04,  2.5786e-04,  1.1619e-05,\n",
       "         -2.7929e-05,  7.7362e-04]),\n",
       " tensor([[-2.8662e-03,  1.5740e-04,  1.8351e-03,  ...,  1.1058e-03,\n",
       "           1.0813e-03, -1.0220e-03],\n",
       "         [ 8.7262e-04, -2.5024e-03,  1.1035e-03,  ...,  6.1234e-04,\n",
       "          -7.1748e-04, -2.4837e-03],\n",
       "         [-5.1563e-04,  2.5451e-04, -1.3826e-03,  ...,  2.7585e-04,\n",
       "           1.7307e-03, -3.6686e-03],\n",
       "         ...,\n",
       "         [ 2.4919e-04, -1.5696e-03,  1.5810e-03,  ..., -2.8506e-03,\n",
       "           4.3626e-04,  3.6581e-04],\n",
       "         [-1.4112e-03, -2.2383e-03,  2.2346e-03,  ..., -1.3595e-03,\n",
       "           1.9553e-03, -6.5468e-05],\n",
       "         [ 1.5097e-03,  8.9968e-04, -8.4142e-04,  ..., -2.6919e-03,\n",
       "           1.7438e-03, -4.2669e-04]]),\n",
       " tensor([-6.2974e-04, -1.1156e-04, -4.8966e-04,  ..., -3.6917e-04,\n",
       "         -7.0989e-05, -1.0904e-04]),\n",
       " tensor([[ 0.0021, -0.0008,  0.0022,  ..., -0.0023, -0.0044, -0.0006],\n",
       "         [-0.0006,  0.0020, -0.0027,  ..., -0.0012, -0.0037, -0.0004],\n",
       "         [ 0.0004, -0.0017,  0.0028,  ..., -0.0004,  0.0044, -0.0009],\n",
       "         ...,\n",
       "         [-0.0014, -0.0013,  0.0026,  ...,  0.0014,  0.0020,  0.0032],\n",
       "         [-0.0033,  0.0020,  0.0006,  ...,  0.0025, -0.0012,  0.0008],\n",
       "         [ 0.0029,  0.0016, -0.0015,  ...,  0.0008,  0.0014,  0.0018]]),\n",
       " tensor([-4.2128e-04, -3.2210e-04,  9.4458e-05,  5.1356e-04, -2.4914e-04,\n",
       "          3.7017e-04,  5.0142e-06, -2.4090e-04, -2.3133e-04, -1.9113e-04,\n",
       "          2.0420e-04,  2.5686e-04,  7.5719e-04, -2.8210e-04,  7.1974e-04,\n",
       "         -5.5273e-04, -9.9119e-05,  3.4081e-05,  4.7133e-04, -6.5345e-04,\n",
       "         -6.4412e-04,  3.2454e-04, -5.6166e-05,  5.2826e-04, -2.0440e-04,\n",
       "         -6.7640e-04,  5.1993e-05,  1.6226e-05, -4.8641e-04,  4.5784e-04,\n",
       "         -1.7070e-04,  1.2626e-04,  2.2555e-05, -5.4426e-04, -6.2931e-04,\n",
       "          1.3362e-04,  3.2667e-04, -6.3331e-04,  1.0362e-04, -3.9292e-04,\n",
       "         -8.3176e-05,  1.5146e-04, -1.4382e-04, -2.8527e-04, -3.0243e-04,\n",
       "         -1.4211e-04,  4.7852e-04,  4.4963e-04, -1.2294e-04,  1.7843e-04,\n",
       "         -5.2272e-04,  4.2548e-04, -6.2013e-04, -2.0709e-05, -8.7908e-05,\n",
       "          3.7068e-04, -4.6756e-05, -5.3063e-04, -3.7162e-04, -4.9631e-04,\n",
       "          1.8134e-04, -3.8265e-04,  3.2378e-04, -3.8515e-04, -1.2037e-04,\n",
       "         -3.9295e-04, -1.6633e-05,  5.2570e-04,  2.8339e-04, -7.3779e-05,\n",
       "          2.2655e-04,  2.1178e-04,  3.5429e-04, -7.5341e-04,  3.8898e-04,\n",
       "         -3.3687e-04, -1.2093e-04,  2.3878e-04,  3.7439e-04, -1.1450e-04,\n",
       "         -2.6627e-04, -2.1087e-04,  6.5619e-05, -6.6174e-04, -4.3758e-04,\n",
       "         -2.2172e-04,  6.3939e-05,  4.5548e-04, -7.5590e-05, -1.4589e-04,\n",
       "         -6.3848e-05,  4.9149e-05,  1.5917e-04, -4.4559e-04,  1.3484e-04,\n",
       "         -5.6888e-04, -4.6610e-04, -2.1295e-04, -1.8532e-04, -1.8941e-04,\n",
       "          2.5427e-04,  2.5036e-04, -5.9123e-04, -5.0845e-04, -4.6016e-04,\n",
       "         -2.0284e-04,  1.7285e-04,  4.8078e-04,  1.1147e-05, -3.3346e-04,\n",
       "         -3.2838e-04,  1.2228e-04,  4.8660e-05, -4.3351e-04, -3.8511e-04,\n",
       "         -3.7093e-04,  2.1473e-05,  3.0024e-04, -4.1706e-04, -7.3561e-04,\n",
       "          2.3645e-04,  4.0542e-04, -1.8304e-04, -2.6453e-04, -8.2114e-04,\n",
       "         -8.1107e-04, -1.9001e-04,  3.5181e-04,  3.1013e-04,  2.4328e-04,\n",
       "          2.9424e-05, -3.9171e-04,  4.2298e-05,  2.9341e-04, -4.2199e-04,\n",
       "          1.0904e-04,  7.2796e-04, -3.1773e-05, -8.5153e-05, -3.7860e-04,\n",
       "         -2.6183e-04, -2.3623e-04, -4.4983e-04, -2.7446e-04,  7.6164e-06,\n",
       "         -3.7302e-04, -1.3028e-04,  6.2365e-05,  2.2724e-04,  2.2330e-04,\n",
       "          4.7625e-05,  4.1696e-04, -5.4157e-05, -6.2210e-04, -8.3104e-04,\n",
       "         -2.5771e-04, -3.3063e-05,  1.7029e-04, -1.3788e-04,  1.8277e-04,\n",
       "          2.6318e-04, -6.4772e-04,  4.7576e-04,  9.9307e-04, -3.1431e-04,\n",
       "          2.4111e-04,  7.1134e-04, -3.0278e-04,  3.2544e-04,  8.4808e-04,\n",
       "         -4.0140e-05,  1.8632e-04,  2.7462e-04, -9.3316e-05,  3.8355e-04,\n",
       "         -1.7041e-04, -7.5285e-05, -1.1207e-04, -1.5747e-04, -3.4913e-04,\n",
       "          2.2962e-04,  1.3261e-04,  3.2694e-04,  4.5511e-04, -3.9687e-04,\n",
       "          5.9422e-04,  1.2718e-04,  6.4716e-04, -7.9907e-05, -5.5041e-04,\n",
       "          3.0246e-05, -5.0712e-04,  2.6049e-04,  2.0208e-04,  2.6123e-04,\n",
       "         -3.1674e-04,  7.4326e-04,  1.2638e-04,  1.6656e-04, -4.6689e-04,\n",
       "         -4.3082e-04,  4.3066e-04,  9.0547e-04, -3.1368e-04,  2.5833e-04,\n",
       "         -5.3959e-04, -3.3646e-04,  7.9814e-04,  6.8631e-05, -1.9001e-04,\n",
       "          7.7311e-05, -3.4419e-04,  6.6773e-04,  6.7262e-05,  4.7383e-05,\n",
       "         -1.3753e-04, -9.0670e-04, -1.8145e-04,  1.3516e-04,  7.1678e-05,\n",
       "          3.3726e-04,  1.9375e-04, -4.6567e-05,  3.2461e-04, -3.6211e-04,\n",
       "         -4.7455e-05, -1.8873e-04, -7.2931e-04, -3.6161e-04, -5.7148e-05,\n",
       "         -2.8455e-04,  3.8929e-06,  1.5759e-04,  4.0188e-04,  5.1190e-04,\n",
       "          2.6395e-04,  3.7237e-05,  9.7427e-05,  9.2626e-04,  1.6760e-05,\n",
       "          7.1066e-04,  1.3531e-03, -7.4166e-06,  5.5562e-05, -7.2186e-05,\n",
       "          2.9107e-04, -6.1344e-04, -3.4003e-04,  1.6511e-04, -4.0527e-04,\n",
       "          1.4574e-04, -1.2340e-04,  2.7200e-04, -9.3069e-05,  4.8149e-05,\n",
       "         -2.9150e-04,  4.1274e-04, -2.7352e-04, -7.0992e-05, -3.5781e-04,\n",
       "          1.8462e-04,  1.6519e-04,  4.1694e-04,  6.1055e-05, -5.8786e-05,\n",
       "          2.7081e-04, -4.1182e-04, -4.3114e-04, -1.8457e-04,  1.0599e-04,\n",
       "         -4.5467e-04,  3.6546e-04, -3.6034e-04, -2.5118e-04,  3.8505e-04,\n",
       "         -7.8194e-04,  3.9901e-04,  1.6332e-04, -1.9469e-04, -6.4469e-04,\n",
       "         -4.9771e-04,  3.3499e-04,  3.7381e-04,  2.7245e-04,  4.7817e-04,\n",
       "         -5.8945e-04,  4.2860e-05, -3.9836e-04,  4.3799e-05,  7.1889e-05,\n",
       "          1.0019e-04,  5.8871e-04, -1.5567e-05,  2.0491e-05, -2.9228e-04,\n",
       "         -1.9090e-04,  6.3879e-04,  1.0004e-03, -3.4642e-04, -2.9558e-04,\n",
       "          9.1722e-05, -3.9084e-04, -5.9602e-04,  3.4596e-04, -1.2087e-04,\n",
       "          7.5783e-04, -5.5271e-04, -4.6455e-04,  5.0023e-05,  3.0832e-04,\n",
       "          1.4791e-04, -5.1775e-04,  2.9579e-04,  3.2653e-04, -3.2190e-05,\n",
       "         -1.5677e-04,  4.2394e-05,  1.6753e-04, -1.5510e-04,  3.3981e-04,\n",
       "          2.3150e-04,  3.5344e-05, -7.7805e-05, -4.4865e-04, -2.2779e-04,\n",
       "         -2.7934e-04, -3.8321e-04, -1.6161e-04, -6.2352e-06, -1.8700e-04,\n",
       "         -6.1721e-05,  2.1043e-04,  1.5318e-05, -2.6264e-04,  1.0583e-04,\n",
       "         -3.7530e-04, -2.1669e-04,  4.4648e-04, -2.3419e-05,  8.7675e-06,\n",
       "          2.2277e-05,  4.2701e-04,  6.8925e-04, -2.8912e-04,  1.5432e-04,\n",
       "         -4.1463e-04,  3.7686e-04,  3.2552e-04,  3.3755e-04, -4.6264e-05,\n",
       "          3.4725e-04, -1.4781e-04, -3.3507e-05,  3.2429e-04, -5.3866e-04,\n",
       "          1.1237e-04,  4.0147e-04,  1.2210e-04,  2.7785e-04,  5.8739e-04,\n",
       "          8.1267e-06,  3.7618e-04, -3.8585e-04, -2.4040e-04, -6.4821e-04,\n",
       "          7.1562e-05,  2.1653e-04,  5.2114e-04, -2.0940e-04, -2.2200e-04,\n",
       "          3.3638e-04, -1.4528e-04, -6.3632e-04, -3.6533e-04, -1.9742e-05,\n",
       "          1.1444e-04,  4.1466e-04,  3.3590e-04, -8.1114e-05,  7.0415e-04,\n",
       "         -6.8716e-05, -2.3068e-04,  5.1974e-04, -2.0325e-04, -2.5640e-04,\n",
       "          3.7872e-04, -3.4268e-04, -2.8882e-04, -3.8743e-04,  2.3517e-04,\n",
       "         -7.8802e-04,  4.1514e-04, -2.1734e-04, -2.5042e-04, -1.9553e-04,\n",
       "         -4.3232e-04,  2.5379e-04, -1.1528e-04, -4.1874e-04,  3.9023e-04,\n",
       "          1.4592e-04,  3.0263e-04,  5.5997e-04, -3.0963e-04,  9.5030e-05,\n",
       "         -3.1822e-04,  1.5359e-04, -2.5928e-04, -8.0092e-05, -6.1537e-04,\n",
       "         -1.0924e-04, -8.7935e-05, -2.4230e-04, -1.9696e-04,  3.7901e-04,\n",
       "         -2.3407e-04,  3.3642e-05, -1.2141e-04, -5.8404e-04, -1.8942e-04,\n",
       "          2.1496e-04,  3.4087e-05,  1.8829e-04,  2.2018e-04,  3.8803e-05,\n",
       "         -2.5753e-04, -2.5898e-04, -3.4953e-04, -6.8383e-04, -3.9637e-06,\n",
       "          2.0993e-04,  6.0230e-04, -1.4656e-04, -1.1916e-06, -3.2263e-04,\n",
       "          2.6283e-05,  8.3689e-05, -5.4261e-04, -1.9260e-05, -7.0717e-05,\n",
       "          6.0472e-04,  4.5573e-04,  2.0113e-04, -5.0355e-04, -6.7927e-04,\n",
       "         -2.7653e-04, -3.8893e-04, -1.5686e-04,  2.2442e-04, -4.0964e-04,\n",
       "          2.7649e-03, -3.6711e-05, -2.6466e-04, -2.6563e-04, -3.7804e-04,\n",
       "          6.5965e-04,  6.8031e-04, -5.0331e-04, -2.4500e-04, -2.5315e-04,\n",
       "         -6.9325e-05, -6.0753e-05,  2.2745e-04, -3.8053e-04,  1.5395e-04,\n",
       "          5.0232e-04,  3.1905e-05, -5.0686e-04, -3.8534e-04, -3.1537e-04,\n",
       "         -1.5418e-04, -6.4787e-06,  2.8051e-06, -9.2672e-05, -2.3785e-04,\n",
       "          3.0264e-04,  3.1502e-04,  4.8848e-04,  1.4540e-05,  4.1192e-04,\n",
       "          5.4966e-04, -1.2246e-04,  3.8955e-04, -2.1654e-04, -1.0959e-04,\n",
       "         -6.5173e-05, -9.2033e-05, -5.7101e-04, -1.1798e-04,  3.3293e-04,\n",
       "         -3.1415e-04, -3.4675e-04,  7.7361e-05,  8.6688e-05,  2.7869e-04,\n",
       "          4.8686e-05,  3.6823e-04,  2.5203e-04,  3.3731e-04,  1.7934e-04,\n",
       "         -4.5691e-06,  4.1020e-05,  1.6501e-04,  2.0135e-04, -7.4670e-05,\n",
       "          2.5193e-04,  9.2378e-05, -5.1576e-04,  5.2127e-04,  3.1892e-04,\n",
       "          8.0872e-05, -6.2931e-04]),\n",
       " tensor([-5.5218e-04, -1.4244e-03,  4.3195e-04, -6.5690e-04,  1.8048e-04,\n",
       "          1.8196e-03,  2.1516e-03,  5.1749e-04,  1.4578e-03,  1.4055e-03,\n",
       "         -3.4380e-04, -1.2894e-03,  4.8286e-04,  1.1778e-04,  5.8258e-04,\n",
       "         -2.6045e-03, -5.3227e-05, -1.4227e-03, -8.3387e-05,  1.2447e-03,\n",
       "          1.0540e-03,  6.9499e-05,  8.0758e-04, -5.6148e-05, -1.9178e-03,\n",
       "         -1.5950e-04,  4.8345e-04, -6.1387e-04, -1.8841e-03, -7.5841e-04,\n",
       "          2.3472e-03,  1.7418e-03, -8.7285e-04, -1.1612e-03, -9.9838e-04,\n",
       "          1.2287e-03,  6.4266e-04,  1.1629e-03,  9.3365e-04, -1.1945e-03,\n",
       "         -1.3951e-03,  1.5783e-03,  2.2912e-04, -1.4656e-03, -1.6487e-04,\n",
       "          1.4903e-03,  1.0924e-03, -7.5167e-04,  2.8431e-05, -6.2913e-04,\n",
       "          1.0908e-03,  1.0715e-03, -1.0034e-03,  7.3099e-04,  6.5100e-04,\n",
       "         -1.0027e-03, -1.6093e-06,  7.5197e-04,  1.5364e-03,  1.5272e-03,\n",
       "         -1.5497e-04,  1.1237e-03,  1.2165e-03, -1.8054e-04, -3.7229e-04,\n",
       "         -8.1939e-04, -5.9777e-04, -1.1330e-03, -6.9320e-04,  1.7419e-03,\n",
       "          2.6554e-04,  3.4412e-03, -3.8463e-04,  4.2993e-04, -5.4640e-04,\n",
       "         -4.9412e-05, -8.1533e-04,  4.4924e-04,  3.8761e-04,  1.6115e-03,\n",
       "          1.1786e-03,  1.0697e-03,  5.6285e-04,  3.8886e-04,  3.5459e-04,\n",
       "          1.4635e-03,  1.7838e-03,  1.3067e-03,  2.5678e-04, -7.3975e-04,\n",
       "         -1.7990e-03,  1.2380e-04, -1.3691e-03,  1.3053e-04,  1.0074e-03,\n",
       "         -8.0884e-05,  6.6757e-04, -4.7165e-04,  5.6428e-04,  1.6025e-03,\n",
       "         -1.6621e-03, -1.1182e-03, -1.8299e-04, -6.7723e-04,  5.1564e-04,\n",
       "         -2.6882e-05,  6.8617e-04,  3.4714e-04, -1.3948e-03,  1.5666e-03,\n",
       "          4.4072e-04, -1.3494e-03, -8.2028e-04,  8.7798e-04,  7.3731e-05,\n",
       "          1.1476e-03, -1.2795e-03,  2.6292e-04, -2.2221e-04, -7.3606e-04,\n",
       "          2.7673e-03, -2.1194e-03, -3.6968e-03,  4.8101e-04, -6.6316e-04,\n",
       "          3.3629e-04,  1.0133e-05, -9.3180e-04,  8.2350e-04, -4.5061e-05,\n",
       "         -2.0123e-03,  5.6738e-04,  1.8466e-03, -7.4565e-04, -4.2826e-04,\n",
       "         -1.6254e-04, -4.9949e-04,  1.3047e-03,  2.7678e-03,  2.4438e-05,\n",
       "          2.5351e-03,  2.3705e-04, -2.8648e-03, -3.0118e-04, -2.2466e-03,\n",
       "          5.2661e-04, -2.7825e-03,  6.0713e-04, -2.0192e-03, -4.9233e-04,\n",
       "         -1.5044e-04,  1.4687e-03,  6.1774e-04,  1.2707e-03,  1.4648e-03,\n",
       "         -5.7685e-04,  6.9463e-04, -4.4084e-04, -1.8808e-03,  2.2030e-04,\n",
       "         -5.6142e-04, -1.5722e-03,  6.4224e-04, -7.5728e-04,  2.4824e-03,\n",
       "          1.2791e-03,  1.4173e-03,  2.4378e-05,  9.3585e-04,  7.7200e-04,\n",
       "          1.1192e-03,  1.7063e-03,  7.0995e-04, -6.3658e-04, -1.3914e-03,\n",
       "          1.8384e-03,  1.6354e-03,  1.9786e-03,  2.5624e-04, -1.6921e-03,\n",
       "         -6.3080e-04,  5.6016e-04,  5.3072e-04,  1.3053e-04,  1.3022e-03,\n",
       "         -1.7365e-03, -1.6209e-03, -1.3295e-03, -6.3252e-04, -8.3756e-04,\n",
       "          9.5022e-04,  1.6783e-03, -3.0088e-04, -2.9578e-03, -3.8075e-04,\n",
       "         -5.0926e-04,  3.9023e-04, -6.0391e-04,  9.8735e-04, -2.0251e-03,\n",
       "          7.3814e-04,  1.5304e-03, -3.3271e-04,  1.5882e-03,  2.5050e-03,\n",
       "          5.7441e-04, -1.5271e-03,  1.0529e-03, -3.5477e-04,  1.3456e-03,\n",
       "          1.5036e-03, -2.1982e-03,  1.0494e-03,  2.5509e-03, -1.0028e-03,\n",
       "          8.3357e-04,  7.3975e-04,  1.1775e-03,  3.7014e-04,  1.2871e-03,\n",
       "          5.8734e-04,  6.4808e-04,  4.7374e-04,  5.7340e-05, -1.7464e-05,\n",
       "         -3.4750e-04,  1.1041e-03,  6.8575e-04, -1.1374e-03,  2.3067e-04,\n",
       "          8.9252e-04,  1.3741e-03,  8.5145e-04,  5.4002e-05, -4.3571e-05,\n",
       "          9.5063e-04, -9.4950e-04,  1.1296e-03,  1.6323e-03, -2.0492e-04,\n",
       "          1.3835e-03, -3.9113e-04, -1.5113e-03, -2.9874e-04, -2.6017e-04,\n",
       "          7.3242e-04, -9.5010e-04, -8.8894e-04,  2.6633e-03,  3.9303e-04,\n",
       "         -1.1541e-03, -3.9178e-04,  1.7951e-03, -3.6043e-04, -1.1675e-03,\n",
       "         -7.6646e-04,  1.0393e-03,  6.0278e-04,  1.6841e-03, -9.4962e-04,\n",
       "          9.5451e-04,  1.2595e-03,  7.9095e-05, -6.7437e-04,  8.2254e-05,\n",
       "         -6.7514e-04,  2.3482e-03,  8.2433e-04,  7.1621e-04, -7.3081e-04,\n",
       "         -7.8756e-04,  3.8505e-05, -8.1325e-04, -5.8919e-04,  6.8009e-05,\n",
       "          6.2120e-04,  1.5994e-03, -2.2064e-03,  9.4426e-04, -3.8415e-04,\n",
       "          9.8282e-04, -2.0432e-04, -3.6500e-03, -1.3471e-03,  2.5128e-03,\n",
       "         -5.0092e-04,  1.5432e-04, -5.3549e-04, -2.0501e-03, -5.5742e-04,\n",
       "          6.3211e-04, -2.7224e-03, -1.4198e-03,  9.5344e-04, -2.5493e-04,\n",
       "          3.7295e-04, -1.4662e-03,  1.4673e-03,  2.2113e-03,  1.7101e-04,\n",
       "          7.0572e-05, -5.6195e-04,  2.7128e-03,  7.5233e-04,  4.3046e-04,\n",
       "         -1.7458e-03,  1.2338e-05, -4.4632e-04, -4.0251e-04,  3.5036e-04,\n",
       "          2.8300e-03, -1.2541e-03, -1.1104e-04,  1.7465e-03, -1.4526e-04,\n",
       "          6.7425e-04, -6.7115e-04,  1.0089e-03,  4.0483e-04,  2.7174e-04,\n",
       "          4.9520e-04,  1.0761e-03,  3.5369e-04, -9.3579e-05, -2.1422e-04,\n",
       "          3.2187e-04,  3.2276e-04,  7.6562e-04,  6.4552e-05,  1.1032e-03,\n",
       "          7.4977e-04,  1.1089e-03,  1.1323e-03,  1.3362e-03, -9.4104e-04,\n",
       "         -1.0928e-03,  2.2205e-03, -2.6059e-03, -2.2780e-03,  2.5030e-03,\n",
       "          4.7535e-04,  1.4950e-03, -1.9515e-04,  3.5799e-03, -4.6682e-04,\n",
       "          1.6248e-03,  7.9429e-04, -1.1179e-03,  7.3397e-04,  3.6955e-05,\n",
       "          1.7548e-04,  7.1812e-04, -6.2442e-04,  2.2107e-03, -8.5914e-04,\n",
       "          5.6803e-04, -1.1879e-03,  1.6226e-03, -1.9752e-03, -1.4460e-03,\n",
       "         -5.4538e-04,  1.8994e-03,  8.4990e-04,  1.1785e-03,  1.9739e-03,\n",
       "          5.0354e-04, -6.0976e-05,  7.1603e-04, -1.4424e-03, -4.3714e-04,\n",
       "          4.1759e-04,  1.5042e-03,  9.4354e-05,  2.9647e-04,  1.1827e-03,\n",
       "          1.8194e-03, -4.9841e-04, -9.7919e-04,  2.5324e-03,  1.0531e-03,\n",
       "          9.3490e-04,  9.6005e-04,  3.0810e-04,  1.2726e-04, -7.6246e-04,\n",
       "         -1.3891e-03,  9.8413e-04, -6.1733e-04, -1.4606e-03,  2.4736e-04,\n",
       "          2.4253e-04,  1.0787e-03,  3.1799e-04,  5.4413e-04, -4.7660e-04,\n",
       "          9.3800e-04, -6.8527e-04,  5.6869e-04,  4.0750e-03,  9.0283e-04,\n",
       "          1.1309e-03, -2.1122e-03, -1.0886e-03,  2.3800e-04,  8.7285e-04,\n",
       "         -9.3579e-05,  5.0151e-04,  6.0576e-04,  6.2162e-04,  1.0293e-03,\n",
       "         -9.7322e-04, -1.2525e-03, -1.2809e-04,  4.4763e-04,  3.5512e-04,\n",
       "          1.7345e-03,  1.2617e-03, -3.5416e-03,  1.1398e-03, -2.3073e-04,\n",
       "          8.5837e-04, -1.5191e-03,  1.7259e-03, -2.2695e-03,  6.6239e-04,\n",
       "         -2.8229e-04,  1.7527e-03,  4.8190e-04, -8.1325e-04, -4.4399e-04,\n",
       "          1.5122e-04, -1.3270e-03, -1.9759e-03, -1.0934e-03,  1.3692e-03,\n",
       "          8.1062e-04, -3.7497e-04, -1.6607e-03, -2.6220e-04, -1.0282e-03,\n",
       "          5.5915e-04,  2.0683e-05,  3.5328e-04, -1.8829e-04, -6.5464e-04,\n",
       "         -9.9421e-05,  1.7365e-03,  5.4282e-04,  4.2838e-04, -3.9119e-04,\n",
       "          1.7015e-03, -2.0772e-04,  1.0938e-03,  1.5675e-03, -1.1278e-03,\n",
       "          1.1911e-03, -1.4338e-03,  1.1815e-03,  1.6909e-03,  1.5537e-03,\n",
       "          2.5257e-03, -1.1276e-03,  1.2029e-03, -2.3276e-03,  9.2870e-04,\n",
       "         -2.5177e-04,  1.0893e-03,  1.9989e-03,  1.4213e-03,  8.0705e-04,\n",
       "          1.1852e-03, -7.4583e-04,  2.4202e-03,  2.2262e-04,  1.2803e-03,\n",
       "          1.3512e-04,  2.0509e-03,  2.4503e-04,  7.9477e-04, -1.0755e-03,\n",
       "          1.0431e-03, -1.9891e-03,  3.3003e-04,  2.0427e-04,  4.5216e-04,\n",
       "         -1.0499e-03,  2.1423e-03, -8.4698e-04,  1.4412e-03, -1.6981e-04,\n",
       "          3.8999e-04, -3.2198e-04,  4.4411e-04,  1.1476e-03, -1.4448e-03,\n",
       "         -1.0588e-03, -3.6240e-04, -1.1775e-03,  9.2554e-04,  1.4740e-04,\n",
       "          2.2072e-03, -1.1328e-03, -7.6383e-04, -2.2841e-03,  1.9338e-03,\n",
       "          1.6582e-03, -5.4723e-04,  1.0453e-03,  1.2625e-03, -8.4877e-05,\n",
       "          1.1206e-03, -2.2537e-04]),\n",
       " tensor([-2.4426e-04, -1.8027e-04, -2.8868e-04,  3.1877e-04, -5.9500e-04,\n",
       "          7.5404e-05,  6.0992e-05,  3.7391e-04, -8.1337e-04,  1.6219e-04,\n",
       "         -2.2133e-04,  5.9170e-05,  3.2974e-04, -7.1238e-04,  4.2598e-04,\n",
       "          3.3142e-05,  3.2608e-04, -3.9511e-04,  4.9151e-04,  4.2287e-04,\n",
       "         -5.7125e-04, -4.3672e-04,  4.3009e-04, -4.6685e-04,  4.4055e-04,\n",
       "          1.1042e-04,  3.0339e-04,  3.5569e-05, -7.4879e-04,  7.5175e-04,\n",
       "         -5.6075e-04, -1.6982e-04,  3.8054e-04,  2.4427e-04, -5.0115e-04,\n",
       "         -1.4600e-04, -3.8188e-04,  9.9465e-05,  3.4300e-04, -1.9706e-04,\n",
       "         -3.3636e-04, -3.6721e-04,  1.7618e-04,  3.8190e-04,  1.3709e-04,\n",
       "         -3.9268e-04, -4.4965e-04,  3.6756e-04, -7.2919e-05,  1.9983e-04,\n",
       "          4.7992e-04,  1.1798e-04, -6.7019e-04,  3.0291e-04,  4.5266e-04,\n",
       "          2.3018e-04, -5.3217e-04,  2.6884e-04, -3.4631e-04, -4.1698e-04,\n",
       "          1.0593e-04,  1.1513e-04,  3.8546e-04, -9.5446e-05,  9.9828e-05,\n",
       "         -2.5351e-04, -6.5006e-05,  5.6467e-04, -2.9670e-05, -4.8999e-04,\n",
       "         -7.7095e-06,  2.6227e-04, -1.5399e-04,  4.2450e-05, -5.5717e-05,\n",
       "         -1.3969e-04, -1.4365e-04,  1.1165e-04,  1.8532e-04, -1.8640e-04,\n",
       "         -3.0665e-04, -1.8118e-04, -4.2623e-04,  3.9356e-04,  5.4325e-04,\n",
       "         -5.3389e-04, -1.8874e-04,  5.2230e-04, -1.9343e-04, -3.2315e-04,\n",
       "         -5.6735e-04, -3.9237e-04,  2.0802e-05, -6.8545e-05,  4.4987e-05,\n",
       "         -1.3093e-04,  1.1429e-05, -4.1075e-05,  5.2188e-04, -1.3102e-04,\n",
       "         -2.2170e-04, -1.8151e-04,  3.5465e-04,  1.0582e-03, -1.5958e-04,\n",
       "          3.4248e-05,  4.5740e-04, -4.2688e-04,  2.8174e-04,  4.1358e-04,\n",
       "         -3.9952e-04,  1.8983e-04, -1.7685e-04, -3.5278e-05, -2.1257e-04,\n",
       "         -2.7657e-05,  4.6122e-04,  3.3296e-05,  2.6954e-04,  1.9455e-04,\n",
       "         -2.6818e-05,  6.4402e-04,  2.7206e-04,  3.1951e-05, -2.8538e-04,\n",
       "          1.4313e-05,  5.0457e-04, -9.5857e-05,  2.4912e-04,  4.5055e-04,\n",
       "          2.5034e-04, -2.2577e-04,  4.0925e-04,  7.8768e-05, -2.0495e-04,\n",
       "          9.6537e-05,  3.6711e-05,  2.4796e-04,  5.3465e-04,  1.5649e-04,\n",
       "         -2.6363e-04, -3.5483e-04,  2.6998e-04, -4.9368e-04,  3.4194e-05,\n",
       "          2.4077e-04, -3.9747e-04,  1.3666e-05,  2.0722e-04,  2.1042e-04,\n",
       "          1.9415e-04,  5.9915e-04, -1.1213e-04, -2.8761e-04, -6.8150e-05,\n",
       "          1.7738e-04,  9.8355e-05, -2.6945e-04, -2.8902e-04,  6.7836e-04,\n",
       "         -2.5123e-05,  1.0463e-04,  3.5959e-04,  5.1324e-04,  3.4665e-04,\n",
       "          5.3671e-04,  3.3118e-05, -2.8548e-04, -1.5371e-04,  5.7558e-04,\n",
       "          1.9861e-04,  3.4641e-04,  5.3324e-05, -8.1271e-04,  4.5370e-04,\n",
       "         -3.0669e-04, -1.3790e-04, -3.3995e-04,  2.0079e-04, -2.0065e-04,\n",
       "          1.2895e-04, -5.0679e-04,  1.0252e-04, -4.3013e-04, -4.0361e-04,\n",
       "         -7.4621e-05,  7.4044e-05,  6.4607e-04, -2.9485e-04,  3.6946e-05,\n",
       "         -7.0557e-05, -2.1417e-04,  4.3889e-04,  6.3498e-05, -8.1008e-04,\n",
       "         -3.9177e-04,  5.9046e-05,  1.1898e-04,  1.0160e-04, -6.3344e-04,\n",
       "          3.5096e-05,  3.2805e-04,  4.8143e-05, -5.7270e-04, -3.4320e-04,\n",
       "          1.3808e-04, -5.4950e-05, -1.0542e-04,  7.8276e-05, -2.6579e-04,\n",
       "         -3.7383e-04, -5.6580e-04,  1.7091e-04, -3.2742e-04, -2.6902e-04,\n",
       "          5.3199e-04, -2.1285e-04,  4.8858e-04,  4.5418e-04,  4.1631e-04,\n",
       "          7.5521e-04, -2.1861e-04, -3.7235e-04,  2.8061e-05, -5.6455e-04,\n",
       "         -1.7863e-04,  1.1092e-04,  2.2743e-04, -3.1793e-04,  2.4063e-04,\n",
       "          2.7914e-04,  3.2940e-04, -6.3632e-04, -1.2498e-04,  6.8266e-04,\n",
       "          4.1532e-04, -2.0859e-04,  7.5900e-04,  5.0354e-04, -3.2142e-04,\n",
       "         -2.4811e-04,  1.2487e-04, -3.4643e-04,  3.5889e-04,  4.2146e-04,\n",
       "         -5.4538e-06,  3.6161e-04, -3.4070e-04,  8.6710e-04,  1.3717e-04,\n",
       "          4.3092e-04, -1.8645e-06,  1.8441e-04, -1.3557e-04, -3.6572e-04,\n",
       "          1.5629e-04,  1.0241e-04,  1.6646e-04, -1.2780e-04, -2.3097e-05,\n",
       "          4.5745e-04,  2.0181e-04,  1.7233e-04,  2.3190e-04, -1.2498e-04,\n",
       "          9.2070e-04, -3.1868e-04, -7.2915e-05, -1.8915e-04, -2.9415e-04,\n",
       "         -6.1489e-04, -3.7937e-04,  4.8990e-04,  8.3964e-04,  4.3480e-04,\n",
       "         -6.8995e-04,  3.3817e-04, -2.4714e-05, -2.8058e-04, -4.7903e-04,\n",
       "         -3.0698e-04,  2.8549e-04,  1.7996e-04, -2.5767e-04, -2.0745e-05,\n",
       "         -3.4105e-04, -3.5926e-04,  8.1585e-05, -1.9593e-04, -2.9274e-04,\n",
       "          3.6683e-04, -2.0864e-04,  9.6697e-05,  6.3670e-05, -3.7084e-04,\n",
       "          1.1813e-04,  3.9577e-04,  1.7166e-04,  1.3509e-04,  5.4520e-04,\n",
       "          6.3799e-05,  1.7673e-05,  3.4146e-04,  6.6774e-04, -1.5981e-04,\n",
       "          3.5655e-04, -2.9364e-04,  8.7827e-05, -3.9874e-04, -2.5602e-04,\n",
       "          1.5714e-03, -2.4825e-04,  3.0335e-04,  6.9656e-04, -3.4525e-04,\n",
       "          2.4228e-04,  4.4101e-04, -5.9745e-04,  4.3273e-04,  7.1695e-04,\n",
       "          4.8549e-04,  6.2670e-04, -3.9676e-04,  6.2314e-04, -5.6932e-04,\n",
       "         -7.8280e-05,  3.7332e-04, -9.7041e-05,  6.7582e-04, -1.4331e-05,\n",
       "          4.1140e-04, -1.1475e-04,  4.6003e-04, -4.5610e-04, -2.8834e-04,\n",
       "         -3.9824e-04,  9.1726e-06, -3.4378e-04,  3.3271e-04, -4.4134e-04,\n",
       "          1.2501e-04,  1.2165e-04,  2.8076e-04,  2.3038e-04,  1.0088e-04,\n",
       "         -2.4513e-04, -1.5452e-04,  4.1569e-04, -1.2770e-04, -1.6345e-04,\n",
       "          9.3848e-05,  1.0147e-03, -2.9542e-04,  3.7110e-04, -4.2425e-04,\n",
       "         -3.8704e-04,  2.5410e-04, -4.8427e-04,  4.2733e-05,  8.0295e-05,\n",
       "          5.1579e-05,  4.5163e-04,  5.1402e-05,  2.0622e-04,  1.8787e-04,\n",
       "         -2.2811e-04, -2.4638e-04,  6.8008e-04,  1.1622e-04,  1.4737e-04,\n",
       "         -4.8302e-04, -3.7814e-04, -2.7556e-04, -1.6622e-04, -3.2391e-05,\n",
       "         -2.9599e-04,  3.6188e-04,  5.6907e-04, -5.7357e-04,  4.8945e-04,\n",
       "         -2.8307e-04, -4.2441e-05,  3.1094e-04,  2.0599e-04, -6.5646e-04,\n",
       "          4.4742e-04, -2.0421e-04, -4.5756e-04,  2.3427e-06,  1.0605e-04,\n",
       "          1.7492e-04,  2.3872e-04, -1.3805e-04,  1.4937e-04, -2.9350e-04,\n",
       "          6.3937e-04,  9.7483e-04, -3.0821e-04, -5.1794e-04,  1.8191e-04,\n",
       "          1.4862e-04,  5.2927e-04,  3.1028e-04, -1.5185e-04, -3.1478e-04,\n",
       "          6.7917e-04,  2.7677e-04, -5.0497e-04, -5.5840e-05,  2.2047e-04,\n",
       "         -1.3948e-04, -4.7210e-04,  4.9578e-04, -1.4982e-04, -9.7438e-05,\n",
       "         -4.0922e-05, -1.7988e-04,  1.7787e-04, -3.8853e-05, -1.0884e-04,\n",
       "          4.6916e-04, -4.6872e-05, -1.0828e-04,  9.7040e-05,  4.3068e-05,\n",
       "         -2.3900e-04, -5.6881e-05, -4.4190e-04,  9.3528e-05, -5.2199e-04,\n",
       "          4.9890e-05,  4.6920e-04, -4.3334e-04, -3.5614e-05, -1.2241e-04,\n",
       "          9.9350e-05, -4.7122e-04,  3.4921e-05, -1.4736e-04, -1.1250e-05,\n",
       "          1.1732e-04, -5.0112e-04, -5.1312e-05,  3.0328e-04, -4.8988e-04,\n",
       "         -1.2345e-04,  2.9512e-04, -4.6488e-04,  2.7064e-06,  7.1794e-04,\n",
       "         -1.2733e-03,  8.1105e-05, -1.7507e-04,  1.3103e-04, -2.1970e-04,\n",
       "          4.9301e-04,  3.4199e-04, -2.1463e-04,  2.8216e-04,  9.3017e-05,\n",
       "          4.6089e-04,  5.6566e-04,  4.2403e-04, -3.4214e-04,  5.9462e-04,\n",
       "          7.1844e-04, -3.7183e-04,  9.2730e-05, -4.6229e-04, -5.9600e-04,\n",
       "          5.3838e-04, -4.0719e-05, -2.9913e-04,  9.1823e-05,  3.1587e-04,\n",
       "         -1.3873e-04,  8.9841e-04,  7.8306e-06, -2.0204e-04,  4.2877e-04,\n",
       "          1.9375e-04,  3.2578e-04, -9.0158e-05, -5.8316e-05, -1.3456e-04,\n",
       "         -3.5111e-05,  3.4419e-04, -6.5731e-04, -3.5903e-04, -2.3262e-04,\n",
       "         -1.4605e-04, -1.6783e-04,  1.2699e-04, -8.4265e-05, -2.2158e-05,\n",
       "          1.9801e-04,  2.5814e-04,  2.9359e-04, -1.0614e-04,  2.5825e-04,\n",
       "          5.2425e-05, -2.1939e-04,  5.7275e-04,  3.0692e-04, -2.2482e-04,\n",
       "         -8.4087e-05,  1.8799e-04, -9.9548e-04, -1.9812e-04,  4.8809e-04,\n",
       "         -7.9968e-04, -6.0802e-04]),\n",
       " tensor([[-1.3002e-03, -5.0547e-04,  5.6051e-05,  ...,  1.2865e-03,\n",
       "          -5.6452e-04, -1.8591e-03],\n",
       "         [-3.1432e-04, -1.4881e-03,  5.6069e-04,  ..., -1.8684e-03,\n",
       "          -2.1153e-03,  3.8695e-04],\n",
       "         [ 1.8310e-03, -4.1593e-03,  1.0116e-03,  ..., -1.6389e-03,\n",
       "           1.4398e-03,  5.8317e-04],\n",
       "         ...,\n",
       "         [ 1.3425e-04, -5.1764e-04, -8.3136e-04,  ...,  2.2152e-03,\n",
       "          -6.3128e-04,  1.0429e-03],\n",
       "         [ 2.1538e-04,  4.0621e-04, -5.4345e-05,  ..., -4.9238e-04,\n",
       "          -1.2265e-03, -1.5762e-03],\n",
       "         [ 1.3672e-03,  3.8391e-04, -1.2133e-03,  ...,  2.4408e-04,\n",
       "          -1.2880e-03, -1.0951e-03]]),\n",
       " tensor([-0.0005,  0.0002,  0.0002,  ..., -0.0002,  0.0003, -0.0003]),\n",
       " tensor([[-1.3077e-03,  1.0160e-03, -1.5892e-04,  ..., -2.7090e-03,\n",
       "           3.1941e-05, -1.1978e-03],\n",
       "         [ 5.8869e-04, -2.7580e-03, -8.3060e-04,  ...,  5.9456e-05,\n",
       "           7.8487e-04,  1.1315e-03],\n",
       "         [-4.6754e-04,  4.1844e-04, -2.3673e-03,  ...,  5.5256e-04,\n",
       "           5.9132e-04,  5.7816e-04],\n",
       "         ...,\n",
       "         [ 1.3431e-04,  9.4781e-04, -1.5375e-03,  ...,  2.1255e-03,\n",
       "          -7.8325e-04, -1.1071e-04],\n",
       "         [-2.9244e-04,  1.8856e-03,  5.4404e-04,  ...,  1.9017e-03,\n",
       "          -7.1834e-04,  2.6514e-04],\n",
       "         [ 1.8198e-03,  2.6845e-04, -5.2077e-04,  ...,  1.2675e-03,\n",
       "           4.5285e-04, -4.1561e-04]]),\n",
       " tensor([-2.4978e-04, -2.0041e-04,  3.5283e-04,  4.1257e-04,  1.0543e-04,\n",
       "          2.9018e-04, -8.6576e-06, -3.6631e-04,  2.7055e-05, -2.7169e-04,\n",
       "          3.9650e-04,  2.1186e-04,  6.6346e-04, -1.9507e-04,  6.9053e-04,\n",
       "         -6.9086e-04, -1.9605e-04,  2.7087e-04,  2.8293e-04, -9.4789e-04,\n",
       "         -3.2556e-04,  6.7723e-04, -2.1258e-04,  8.4153e-04, -2.8422e-04,\n",
       "         -7.5069e-04, -6.5207e-05,  8.0448e-06, -2.0853e-04,  5.6606e-05,\n",
       "          1.6582e-04,  2.1495e-04, -1.6514e-04, -7.3289e-04, -4.3859e-04,\n",
       "          2.8463e-04,  4.4240e-04, -6.6651e-04, -2.3065e-04, -3.4184e-04,\n",
       "          1.1668e-04,  3.1677e-04, -2.7083e-04, -4.3335e-04, -3.7633e-04,\n",
       "          7.7087e-05,  8.5320e-04,  2.7013e-04, -1.1146e-04,  1.5951e-04,\n",
       "         -9.5459e-04,  3.4315e-04, -2.3399e-04, -1.2253e-04, -4.5148e-04,\n",
       "          2.9005e-04,  2.1464e-04, -6.4550e-04, -8.9876e-05, -3.1979e-04,\n",
       "          2.0533e-04, -4.0769e-04,  9.3864e-05, -2.8161e-04, -1.0875e-04,\n",
       "         -2.2206e-04,  7.8332e-05,  3.0575e-04,  3.3577e-04,  1.1343e-04,\n",
       "          2.6344e-04,  1.1902e-04,  4.2728e-04, -7.9669e-04,  4.6072e-04,\n",
       "         -2.9552e-04, -8.6415e-05,  1.5251e-04,  3.1828e-04, -1.0496e-05,\n",
       "         -1.7612e-04, -1.3722e-04,  3.1065e-04, -9.1249e-04, -8.5133e-04,\n",
       "         -6.5995e-05,  1.1332e-04,  3.4130e-04, -1.5154e-05,  5.4901e-05,\n",
       "          2.5782e-04,  2.7252e-04,  1.3404e-04, -4.4726e-04,  2.4424e-04,\n",
       "         -5.2489e-04, -5.0955e-04, -1.5694e-04, -3.1095e-04, -1.0750e-04,\n",
       "          3.9144e-04,  3.4250e-04, -8.0639e-04, -8.7145e-04, -4.1716e-04,\n",
       "         -1.6674e-04, -6.6988e-05,  7.6048e-04, -1.4746e-05, -4.8746e-04,\n",
       "         -8.9381e-05,  4.1772e-05,  1.7786e-04, -4.3280e-04, -3.1307e-04,\n",
       "         -3.2520e-04, -1.8191e-05,  4.0127e-04, -4.5052e-04, -8.4480e-04,\n",
       "          3.5286e-04,  7.3273e-05, -4.1301e-04, -2.4613e-04, -7.0495e-04,\n",
       "         -8.3444e-04, -4.0394e-04,  4.2136e-04,  2.1498e-04, -3.6662e-05,\n",
       "         -1.1708e-04, -3.4659e-04, -1.3916e-04,  2.7557e-04, -3.6891e-04,\n",
       "          1.0567e-04,  7.4597e-04, -2.0935e-04, -4.0986e-04, -4.8454e-04,\n",
       "         -2.0622e-04,  1.8812e-05, -5.8372e-04, -1.4130e-04, -2.7225e-05,\n",
       "         -6.1163e-04,  9.4090e-05,  7.4323e-05,  1.5926e-04,  2.4063e-04,\n",
       "         -6.9148e-05,  1.5579e-04,  3.4357e-05, -5.8264e-04, -8.6400e-04,\n",
       "         -2.7950e-04, -4.9371e-05,  2.5984e-04,  1.0236e-04, -1.2841e-04,\n",
       "          3.0487e-04, -6.9411e-04,  2.8340e-04,  8.1721e-04, -5.2840e-04,\n",
       "          7.3936e-05,  7.2824e-04, -1.1465e-04,  4.8216e-04,  6.1733e-04,\n",
       "         -1.6531e-04,  1.0079e-04,  3.6351e-04,  3.1273e-04,  3.6145e-04,\n",
       "         -1.0717e-04, -2.9031e-05,  4.7296e-05, -3.0059e-04, -2.7119e-04,\n",
       "          1.8949e-04,  3.5540e-04,  3.3043e-04,  7.6175e-04, -1.3897e-04,\n",
       "          7.2781e-04,  5.3935e-05,  3.2162e-04,  8.1573e-05, -5.5698e-04,\n",
       "          2.9711e-05, -3.2912e-04,  1.1033e-04,  2.3636e-04,  7.4166e-04,\n",
       "         -1.6309e-04,  7.9446e-04,  9.8254e-05,  1.8692e-04, -2.3369e-04,\n",
       "         -4.2965e-04,  2.8380e-04,  9.8526e-04, -3.2887e-05,  4.2686e-04,\n",
       "         -6.7614e-04, -3.2153e-04,  8.3434e-04,  4.9748e-05, -1.8998e-05,\n",
       "          2.4322e-04, -5.6715e-05,  5.8963e-04,  2.1170e-04,  1.8892e-04,\n",
       "         -3.0882e-04, -8.0065e-04, -3.4723e-04, -7.1179e-05, -1.9346e-04,\n",
       "          2.4440e-05,  3.6211e-04,  1.1375e-04,  3.9783e-04, -9.1149e-05,\n",
       "          3.4941e-05, -3.2525e-04, -9.8187e-04, -2.8282e-04, -2.4789e-04,\n",
       "         -4.8022e-04, -9.2765e-05,  4.6651e-04,  4.9184e-04,  2.4590e-04,\n",
       "          1.0759e-04,  1.8244e-04, -2.4416e-04,  7.2253e-04,  2.2308e-04,\n",
       "          8.1502e-04,  1.2676e-03,  2.2486e-04, -8.2646e-05, -3.9662e-04,\n",
       "          3.1877e-04, -7.7143e-04, -2.2976e-04,  6.9700e-05, -4.6194e-04,\n",
       "         -4.8975e-05, -1.3748e-04,  1.8918e-04, -6.4775e-05,  1.7023e-04,\n",
       "         -3.7808e-04,  4.5920e-04, -3.5091e-04,  3.5000e-05, -3.7297e-04,\n",
       "          4.8760e-05,  4.2500e-05,  5.0956e-04, -1.1563e-04,  1.8925e-05,\n",
       "         -1.3886e-04, -4.6498e-04, -4.9071e-04, -4.1335e-05,  2.1578e-04,\n",
       "         -2.5253e-04,  7.3611e-04, -6.1508e-04, -6.9608e-04,  2.2974e-04,\n",
       "         -3.7125e-04,  3.3991e-04,  3.0996e-04, -9.8748e-05, -4.4742e-04,\n",
       "         -3.3202e-04,  1.2387e-04,  3.3399e-04,  5.0076e-04,  5.7794e-04,\n",
       "         -4.7974e-04,  2.5799e-04, -3.9073e-04,  1.3237e-04,  2.7773e-04,\n",
       "         -2.2512e-05,  7.6463e-04,  6.3870e-06,  5.8277e-05, -7.6409e-05,\n",
       "         -3.1394e-04,  4.9057e-04,  9.4801e-04, -4.8853e-04, -7.0575e-04,\n",
       "          6.5443e-05, -3.7242e-04, -6.9293e-04,  8.5838e-05, -1.1926e-04,\n",
       "          7.2093e-04, -4.2293e-04, -4.9653e-04,  3.3065e-04,  5.5194e-04,\n",
       "          5.7997e-05, -3.8312e-04,  1.4607e-04,  1.0012e-05,  1.7820e-04,\n",
       "         -3.6636e-04, -2.6902e-04,  4.2533e-04, -4.0309e-04,  5.2225e-05,\n",
       "         -2.3413e-06, -2.8735e-04,  8.3420e-05, -6.6564e-04,  5.8241e-05,\n",
       "         -2.3034e-04, -6.2650e-04, -1.1091e-04, -2.5998e-04, -1.6134e-04,\n",
       "         -3.0048e-04,  3.6394e-04, -3.1223e-04,  2.5816e-05,  3.0820e-04,\n",
       "         -2.4013e-04, -2.0955e-04,  6.9345e-04, -2.0441e-04,  1.5798e-04,\n",
       "         -7.8117e-05,  3.7585e-04,  6.1501e-04, -3.6302e-04,  1.4995e-04,\n",
       "         -3.3318e-04,  5.2178e-04,  1.0263e-04,  5.2406e-04,  7.0676e-05,\n",
       "          3.6479e-04, -5.2449e-04,  1.1629e-04,  1.7622e-04, -3.6810e-04,\n",
       "          3.6351e-04,  3.3829e-04,  4.1827e-04,  3.2515e-04,  6.8847e-04,\n",
       "         -2.8572e-05,  3.6374e-04, -3.6166e-04, -3.1404e-04, -7.1657e-04,\n",
       "          1.7414e-04,  3.6734e-04,  2.6884e-04, -3.0472e-04, -3.0360e-04,\n",
       "          5.8495e-04, -2.5488e-05, -5.0895e-04, -2.6199e-04, -1.4408e-05,\n",
       "          4.2674e-04,  2.8059e-04,  2.1629e-05,  1.4191e-04,  6.2298e-04,\n",
       "          1.1213e-04, -2.4900e-04,  3.6732e-04, -3.4764e-04,  8.5697e-05,\n",
       "          1.7303e-04, -3.2593e-04, -4.4008e-05, -4.2519e-04,  2.0519e-04,\n",
       "         -1.0206e-03,  2.7855e-04, -1.1352e-04, -3.4061e-04, -5.7882e-05,\n",
       "         -6.9739e-04, -1.9326e-04,  5.5569e-05, -2.5695e-04,  2.9817e-04,\n",
       "          8.6733e-05, -8.7940e-05,  4.1523e-04, -2.3259e-04,  2.3615e-04,\n",
       "         -5.5142e-04,  4.1349e-05,  3.0134e-04, -9.1713e-05, -7.7395e-04,\n",
       "         -1.0327e-05,  1.4448e-04, -5.4285e-04, -1.3951e-04,  4.2038e-04,\n",
       "         -3.0895e-04,  1.0307e-04, -2.0497e-04, -4.2886e-04, -1.2060e-04,\n",
       "         -1.4832e-05, -3.5818e-06,  2.3048e-04,  2.0807e-04,  4.9476e-05,\n",
       "         -1.4105e-04, -1.5653e-04, -2.0173e-04, -7.3935e-04,  3.1477e-04,\n",
       "          2.6480e-04,  3.2341e-04, -1.9901e-05,  3.1946e-05, -3.0650e-04,\n",
       "         -4.2737e-05,  4.0405e-04, -6.6253e-04,  8.7027e-05, -1.5987e-05,\n",
       "          6.7924e-04,  6.5469e-04,  2.4628e-04, -6.0866e-04, -3.1632e-04,\n",
       "         -1.9830e-04, -4.4374e-04,  1.8843e-04,  2.6696e-04, -6.8814e-04,\n",
       "          2.9274e-03, -1.2794e-04, -1.3428e-04, -3.4099e-04, -3.1136e-04,\n",
       "          4.4700e-04,  6.4900e-04, -4.6021e-04, -3.4956e-04, -3.6116e-04,\n",
       "         -1.7780e-04, -3.5191e-04,  4.7332e-05, -2.8545e-04, -1.0251e-04,\n",
       "          2.2755e-04,  1.3200e-04, -4.5593e-04, -1.1252e-04, -4.2411e-05,\n",
       "         -3.8163e-04,  5.9586e-05,  1.3121e-04, -1.6537e-04, -4.0225e-04,\n",
       "          4.0604e-04,  4.3914e-05,  4.4934e-04,  4.7643e-05,  2.0526e-04,\n",
       "          5.2541e-04, -2.2755e-04,  4.5857e-04, -2.1270e-04, -8.5300e-05,\n",
       "         -2.1534e-05, -3.2591e-04, -2.9695e-04,  8.8406e-05,  4.8623e-04,\n",
       "         -3.1735e-04, -2.7156e-04,  1.1924e-04,  1.3502e-04,  3.4130e-04,\n",
       "         -7.9177e-05,  2.9976e-04,  1.3755e-04,  3.3144e-04,  6.0481e-05,\n",
       "         -1.9465e-05,  1.1223e-04, -1.5024e-04,  1.4018e-04,  1.1496e-05,\n",
       "          3.3825e-04,  4.1857e-05, -1.1306e-04,  6.3364e-04,  6.3691e-05,\n",
       "          4.4886e-04, -4.0575e-04]),\n",
       " tensor([-1.2416e-03,  3.0378e-03,  2.1695e-03, -1.7225e-03,  1.0434e-03,\n",
       "         -1.1286e-03,  2.2550e-03, -6.7759e-04, -1.0484e-04,  3.9595e-04,\n",
       "          2.0700e-03,  1.5625e-03, -2.8636e-03, -4.4243e-03, -7.5042e-04,\n",
       "         -1.3653e-03, -2.4444e-03, -2.3036e-03, -5.7030e-04, -3.7457e-03,\n",
       "          1.4092e-03, -1.7152e-03,  2.3239e-03,  4.6313e-04, -3.6470e-03,\n",
       "         -1.2352e-03,  1.7051e-03, -1.7334e-03, -2.6309e-04,  9.3782e-04,\n",
       "         -2.6000e-03,  2.3634e-03, -3.8242e-04,  1.2016e-03,  5.3167e-05,\n",
       "          6.1202e-04, -1.7148e-03,  3.5471e-04,  2.8824e-03, -5.7817e-04,\n",
       "         -2.7472e-04,  3.4506e-03,  2.2285e-03,  1.4363e-03,  1.1665e-03,\n",
       "         -5.8854e-04, -3.7744e-03,  2.8292e-03, -1.7080e-03, -2.5880e-04,\n",
       "          1.0456e-03,  7.6973e-04,  9.9158e-04,  7.5722e-04, -1.1578e-03,\n",
       "          2.9888e-03,  3.7885e-04, -4.3154e-03,  5.9247e-04,  7.7236e-04,\n",
       "          4.4156e-03,  8.3745e-04, -1.6354e-03, -1.7223e-03,  3.0336e-03,\n",
       "         -2.6079e-03,  6.1715e-04, -2.2531e-05, -2.4114e-03,  7.9763e-04,\n",
       "          9.3746e-04, -1.6116e-03,  9.6738e-04,  1.2348e-03,  1.3343e-03,\n",
       "         -1.3738e-03,  2.5276e-03, -1.2246e-03, -1.6415e-03,  1.6663e-03,\n",
       "          2.4951e-04, -1.6123e-03, -1.2016e-03, -4.1455e-04,  1.2263e-03,\n",
       "          2.0015e-04, -9.7197e-04, -9.3353e-04, -8.7667e-04, -1.2237e-03,\n",
       "          3.6741e-03, -8.9014e-04,  1.2854e-03,  1.5815e-03,  1.3428e-03,\n",
       "         -6.3848e-04, -1.1836e-03,  2.3794e-04,  6.4582e-04, -7.3111e-04,\n",
       "          5.1135e-04,  8.0884e-04,  2.3794e-04, -1.2808e-03,  6.6781e-04,\n",
       "          1.2014e-03,  4.0960e-04, -1.0872e-04, -4.1685e-03, -1.4508e-04,\n",
       "          3.9375e-04,  2.9027e-05,  2.1715e-03,  3.6955e-06, -5.1678e-03,\n",
       "         -3.5810e-04, -4.4409e-03,  4.6551e-04, -1.0972e-03,  2.1244e-03,\n",
       "          1.1714e-03, -6.5684e-05, -3.9983e-04, -1.7827e-03, -1.5846e-03,\n",
       "          1.4038e-03,  1.1096e-03, -4.0013e-03,  1.0553e-03,  1.8225e-03,\n",
       "          3.4785e-04, -1.2227e-03, -1.5323e-03, -7.3647e-04, -2.1160e-04,\n",
       "          1.4464e-03, -1.4859e-03,  1.8250e-03,  3.0267e-04, -3.0156e-03,\n",
       "         -3.5243e-03,  1.5235e-04, -1.5014e-03,  1.9157e-04,  6.2454e-04,\n",
       "         -3.7057e-03, -5.7542e-04, -1.1757e-03, -2.9737e-03, -2.0889e-03,\n",
       "          1.3018e-04,  9.1958e-04,  8.3959e-04, -9.4295e-04, -3.3463e-03,\n",
       "          2.1827e-04,  1.4178e-03,  6.2567e-04,  1.2904e-03,  2.5856e-04,\n",
       "          1.4179e-03, -3.9867e-03,  8.5008e-04, -1.2347e-03,  1.1294e-03,\n",
       "          2.1237e-03, -2.2112e-03, -3.9688e-03,  9.0259e-04, -4.5135e-03,\n",
       "         -3.3700e-04, -3.6926e-03, -9.2727e-04, -1.0766e-03,  2.4686e-03,\n",
       "         -1.1338e-03,  3.4001e-03, -6.0606e-04,  8.4889e-04, -9.3460e-04,\n",
       "         -3.3585e-03,  1.3413e-03,  1.6950e-03,  4.3082e-04, -1.9927e-03,\n",
       "          1.6171e-03, -3.1579e-04,  3.1317e-03,  2.7549e-04,  2.0516e-04,\n",
       "          2.2328e-04,  5.3298e-04,  1.5984e-03, -3.5105e-03, -2.0600e-03,\n",
       "         -4.3750e-05,  6.1154e-05, -1.8349e-03,  7.6330e-04, -2.9912e-03,\n",
       "          9.5487e-05,  1.3125e-03, -9.7591e-04,  1.4790e-03,  8.4931e-04,\n",
       "          1.9029e-03,  2.4482e-03, -2.9358e-03,  3.1626e-04, -2.2974e-03,\n",
       "          7.7063e-04, -2.5618e-04,  1.9047e-03,  4.3023e-04,  1.2321e-03,\n",
       "          9.2638e-04, -8.4472e-04,  8.9061e-04, -3.1168e-03,  1.4459e-03,\n",
       "          1.2373e-03,  5.2857e-04,  1.1057e-03,  9.9874e-04, -3.1847e-04,\n",
       "          9.1231e-04, -4.9686e-04, -2.6670e-03,  4.8423e-04, -5.2023e-04,\n",
       "         -1.5358e-03, -1.0736e-03,  4.4881e-03,  1.4861e-03, -1.6187e-03,\n",
       "          1.2162e-03, -3.6126e-03, -1.0748e-03,  1.0145e-04,  2.0413e-03,\n",
       "         -4.9056e-03, -1.0755e-03,  7.2813e-04, -6.7890e-04, -1.1166e-03,\n",
       "         -1.6272e-04,  1.4489e-03, -1.3518e-04, -5.3473e-03,  3.2485e-03,\n",
       "         -1.0036e-03, -9.9015e-04, -9.8217e-04,  5.0068e-04, -3.6943e-04,\n",
       "         -2.7269e-04, -5.6279e-04, -2.0003e-04, -5.9688e-04, -4.1008e-05,\n",
       "          3.1745e-03,  1.6957e-03, -3.8645e-03,  2.1102e-03,  1.4473e-03,\n",
       "         -2.5367e-03, -1.9213e-03,  1.7264e-03,  9.2840e-04,  1.8865e-04,\n",
       "         -7.9226e-04, -3.9855e-03, -1.5197e-03, -1.9276e-04,  1.2830e-03,\n",
       "          1.5914e-03, -4.3494e-03, -4.2510e-04, -1.2895e-03,  2.2762e-03,\n",
       "          8.6617e-04,  4.5229e-03, -2.2386e-03, -2.8912e-03, -1.5075e-03,\n",
       "          6.9737e-04,  1.9276e-03,  2.0757e-03, -1.8404e-03,  1.6067e-03,\n",
       "         -1.3864e-03, -2.9063e-04,  2.3780e-03,  1.7536e-03, -1.5730e-03,\n",
       "          1.7262e-04,  4.7731e-04, -5.3751e-04, -2.3894e-03,  1.1482e-03,\n",
       "         -7.1883e-05,  4.8995e-04, -6.8367e-04,  1.9847e-03,  7.8440e-05,\n",
       "          1.3787e-03,  2.3940e-03, -2.5477e-03,  1.7204e-03,  5.1129e-04,\n",
       "         -2.4247e-03,  1.3940e-03,  1.6038e-03,  6.0642e-04,  2.1236e-03,\n",
       "          3.9887e-04,  1.7651e-03, -4.4894e-04, -9.9850e-04,  4.5452e-03,\n",
       "         -1.2460e-03, -1.8581e-03,  1.1889e-03, -2.8443e-04,  1.4069e-03,\n",
       "          1.1132e-03, -6.6614e-04, -1.4620e-03, -2.8723e-03, -2.6337e-03,\n",
       "          1.1921e-03,  1.1833e-03,  5.0437e-04, -2.1719e-03, -1.5627e-03,\n",
       "         -1.3148e-03, -1.3063e-03, -7.2777e-05,  1.0753e-04, -4.1747e-04,\n",
       "          3.8624e-05, -3.3319e-04,  9.9337e-04,  1.7190e-04,  1.8817e-03,\n",
       "          8.0287e-04, -1.2648e-03, -1.5498e-03,  1.4377e-04,  2.4629e-04,\n",
       "          3.6218e-03, -1.7428e-04,  2.5953e-03,  2.2205e-03,  6.7914e-04,\n",
       "         -6.9785e-04,  1.0277e-03, -2.0003e-03, -1.7732e-04, -2.0921e-05,\n",
       "          1.3671e-03, -1.1993e-03,  7.2867e-04,  2.1907e-03,  2.5105e-04,\n",
       "         -4.0609e-04, -3.7110e-04,  1.8631e-03,  7.3981e-04,  9.4914e-04,\n",
       "         -1.5314e-03, -1.4164e-03,  3.5267e-03, -7.4828e-04, -7.5722e-04,\n",
       "          2.8908e-04,  3.3090e-03, -2.4831e-04,  4.2772e-04, -6.4850e-05,\n",
       "         -1.0427e-03, -2.5249e-03, -8.9717e-04,  2.0142e-03,  9.6667e-04,\n",
       "         -2.7319e-03, -1.2681e-03,  9.6697e-04, -3.9696e-03,  3.0375e-04,\n",
       "         -1.7163e-03, -8.5258e-04, -2.1040e-04,  4.5621e-04,  1.3300e-03,\n",
       "          9.7334e-05,  3.7570e-03,  1.8715e-03,  2.4045e-04, -2.0046e-03,\n",
       "          4.7922e-04,  3.6675e-03, -2.9438e-03,  1.8418e-03,  9.0182e-04,\n",
       "         -8.6427e-05, -1.6303e-03, -2.9182e-03,  2.2490e-03,  2.7851e-03,\n",
       "         -3.8517e-04, -7.9930e-05,  2.0658e-03,  1.7047e-05, -3.8267e-03,\n",
       "         -1.5087e-03, -1.7747e-03,  2.4872e-03, -2.7096e-04,  1.3630e-03,\n",
       "         -7.5388e-04, -3.7336e-04,  2.0117e-03, -2.0999e-03,  1.5330e-03,\n",
       "         -4.0711e-03,  1.5475e-03, -3.7026e-04, -3.6175e-03, -2.7001e-05,\n",
       "         -3.0229e-03,  2.1160e-03, -6.0105e-04, -5.7352e-04,  5.1546e-04,\n",
       "          2.6608e-04,  3.2482e-03, -1.8597e-03,  1.2623e-03, -4.2892e-04,\n",
       "         -2.0909e-04, -1.1270e-03, -1.8008e-03, -2.2151e-03,  4.4508e-03,\n",
       "          3.2830e-04,  5.5027e-04,  1.6046e-03, -1.0542e-03, -3.9625e-04,\n",
       "          2.1935e-03, -5.6481e-04,  1.3753e-03, -5.6779e-04, -1.4480e-03,\n",
       "          1.0624e-03,  1.7854e-03, -4.4286e-04,  6.0451e-04, -1.4265e-03,\n",
       "         -6.6459e-04, -4.5419e-05,  8.4102e-04, -1.6739e-03,  1.6497e-03,\n",
       "          2.5703e-03,  4.2713e-04,  1.4821e-03,  1.9022e-03, -9.4283e-04,\n",
       "         -5.3208e-03,  2.5184e-03, -3.7742e-04,  1.6656e-03, -3.2464e-03,\n",
       "         -1.4982e-03,  3.4857e-04, -2.7311e-04, -2.5630e-04, -4.4477e-04,\n",
       "         -1.8122e-03, -1.5807e-03, -4.7189e-04,  1.3618e-03, -6.5780e-04,\n",
       "          2.8297e-03, -4.1103e-04,  1.7045e-03,  1.4794e-03,  2.2563e-03,\n",
       "         -1.0670e-03, -1.4615e-04,  3.4988e-05,  3.7491e-05,  1.8103e-03,\n",
       "          5.0104e-04,  4.5992e-03,  3.0078e-03,  5.6171e-04, -6.9785e-04,\n",
       "          1.7844e-03, -1.4219e-03,  5.6219e-04,  1.7053e-03,  1.7155e-03,\n",
       "         -2.9683e-05,  1.0347e-04,  3.0661e-04, -1.1485e-03,  8.8429e-04,\n",
       "          4.6605e-04,  2.4018e-03]),\n",
       " tensor([-2.6353e-04,  6.7238e-05,  6.6229e-04,  2.5493e-04, -6.2214e-04,\n",
       "         -6.4215e-04, -4.6755e-04,  2.8890e-04,  6.0445e-04, -6.9706e-04,\n",
       "         -4.8527e-05, -7.3366e-05, -1.6032e-04,  5.3460e-04,  5.3956e-04,\n",
       "          7.9508e-04, -3.8684e-04, -2.2508e-04,  1.5379e-03,  2.1198e-04,\n",
       "          4.3857e-04,  7.6834e-04,  6.8920e-04, -6.4287e-04,  4.4767e-05,\n",
       "         -1.2451e-03, -1.3095e-04,  6.5481e-04,  9.9316e-04, -4.1476e-04,\n",
       "          1.4825e-04,  5.7448e-05,  3.4410e-04, -2.7231e-04, -4.6359e-05,\n",
       "          1.2993e-03,  1.3078e-03, -1.1011e-04, -8.5449e-04,  5.5268e-04,\n",
       "         -9.3974e-05, -4.0758e-04, -1.6689e-04, -4.0860e-04,  3.4330e-04,\n",
       "          2.9234e-04,  4.2752e-04, -1.9459e-03, -2.2833e-04,  7.4491e-05,\n",
       "         -2.6745e-04, -5.9781e-04,  2.9932e-04, -8.7015e-04,  1.1040e-03,\n",
       "         -5.2901e-04,  1.9333e-03, -8.5995e-04, -8.0386e-04, -7.0196e-04,\n",
       "          3.4781e-04,  1.2580e-03, -3.8464e-04,  3.0566e-04, -3.9988e-04,\n",
       "          1.2830e-03,  3.6637e-04, -2.4742e-04,  4.6524e-04, -1.3773e-03,\n",
       "         -3.3955e-04, -1.4085e-04,  2.5340e-04,  5.0358e-04, -6.5579e-05,\n",
       "         -1.3743e-04,  2.0147e-04, -6.2152e-04, -5.4583e-04,  2.8540e-04,\n",
       "          5.3504e-04,  7.4866e-04,  2.0444e-04,  9.6294e-04, -7.4781e-04,\n",
       "          3.0779e-04, -5.0401e-04, -5.7041e-04, -2.5893e-04,  3.3931e-04,\n",
       "          6.7338e-04,  7.1757e-04, -8.9593e-06,  5.2547e-05, -3.6449e-04,\n",
       "         -4.9349e-04,  7.7095e-04,  4.8902e-04, -4.1973e-04,  4.3185e-04,\n",
       "         -3.4965e-04,  6.6202e-04,  2.9568e-04, -5.4486e-04, -2.8674e-04,\n",
       "         -7.6315e-04, -9.4965e-04, -2.5009e-04, -4.6187e-04,  1.7180e-04,\n",
       "         -1.8543e-04, -2.0379e-03,  7.8296e-04, -3.2069e-04, -1.0653e-04,\n",
       "         -5.7349e-04, -9.0639e-04, -2.3359e-04,  3.5798e-04, -4.0658e-05,\n",
       "         -2.3284e-04, -4.4803e-04, -5.2710e-04, -3.3993e-04,  1.0714e-03,\n",
       "         -3.5025e-05, -1.5918e-04,  1.0638e-03, -6.6378e-04, -4.6529e-04,\n",
       "          1.2346e-04,  2.9768e-04,  8.8944e-04, -1.9849e-04, -3.7052e-04,\n",
       "         -5.4325e-04,  3.9469e-04, -1.2670e-03, -2.4131e-04, -1.3012e-03,\n",
       "          1.4658e-03, -2.3439e-04, -1.3350e-03,  1.4318e-03,  1.3479e-04,\n",
       "         -8.0113e-04, -3.4273e-06,  2.1243e-04,  1.3020e-03, -3.2984e-04,\n",
       "         -2.3823e-04, -6.5240e-04,  3.0879e-04, -2.6874e-05, -3.6363e-04,\n",
       "          1.2867e-03, -5.0011e-04,  1.6552e-03,  8.2655e-04, -1.1295e-03,\n",
       "          1.0401e-03, -9.7226e-05, -1.8353e-04, -6.0424e-04, -5.9588e-04,\n",
       "         -5.3414e-04, -7.2928e-04, -2.2963e-04,  8.3895e-04,  4.4583e-04,\n",
       "          1.7485e-04, -8.5313e-05, -3.5038e-04,  7.0637e-04, -1.0677e-03,\n",
       "         -8.6171e-04, -8.4069e-05,  1.1019e-04, -3.9169e-04,  3.1274e-04,\n",
       "         -1.3170e-03,  2.9999e-04,  1.1268e-04,  4.8866e-04,  2.1717e-03,\n",
       "          7.7430e-04,  6.0416e-04, -6.9736e-04,  8.5350e-04, -8.1982e-04,\n",
       "         -1.0449e-03,  7.1731e-04,  3.2133e-04, -1.2381e-03,  8.9983e-04,\n",
       "          3.4179e-04,  3.9990e-04,  9.7433e-04,  1.7547e-03, -1.3925e-05,\n",
       "          4.3634e-04, -9.3084e-04, -8.7332e-04,  1.2016e-03,  3.7818e-04,\n",
       "          1.4193e-05, -1.6539e-04,  1.4815e-03, -4.3841e-04,  1.6021e-04,\n",
       "          4.3402e-04,  1.1057e-03, -2.0079e-04,  6.0420e-04,  1.4437e-03,\n",
       "         -9.9925e-04, -5.1290e-04,  2.6238e-04, -2.1624e-04, -2.5250e-04,\n",
       "         -6.5290e-04, -6.4762e-04,  3.1389e-04,  3.0419e-04,  8.7428e-04,\n",
       "          6.6362e-04, -6.7442e-04, -2.0503e-04, -4.3292e-04, -9.6068e-05,\n",
       "         -3.9523e-04, -3.4568e-04,  9.1190e-04, -1.2745e-03,  1.1963e-04,\n",
       "         -8.4684e-04,  1.0371e-03,  1.0089e-04, -4.5026e-04, -1.0266e-04,\n",
       "          1.8918e-03, -1.1146e-03, -3.6803e-04, -9.6781e-04,  7.5315e-04,\n",
       "         -3.1602e-05, -6.5954e-05, -1.4606e-03, -1.2991e-04,  2.4575e-04,\n",
       "         -6.0750e-04, -1.4892e-04, -1.0422e-03,  4.9687e-04, -6.1004e-04,\n",
       "         -9.5833e-05, -2.0561e-04, -4.4897e-05, -3.6813e-04,  4.8130e-04,\n",
       "         -7.2553e-04,  4.2883e-04,  3.5689e-04, -9.8132e-05,  1.2283e-03,\n",
       "         -3.4086e-04,  6.2336e-04,  6.4086e-04,  1.2859e-03,  4.1751e-04,\n",
       "          1.2039e-04,  1.0131e-04, -1.8759e-03,  4.5322e-04, -7.2929e-04,\n",
       "          4.9035e-04, -1.3579e-03, -4.2710e-04,  1.3412e-03,  4.7971e-04,\n",
       "          6.0826e-04,  4.5142e-04,  8.9709e-04, -2.1908e-03,  4.2350e-04,\n",
       "         -4.7339e-04, -5.0791e-04,  1.1119e-03,  1.1731e-03,  1.2230e-03,\n",
       "         -1.6692e-04, -2.5675e-04, -1.7326e-04,  6.9725e-04, -1.4248e-03,\n",
       "          1.7820e-04, -3.7388e-04, -1.0739e-03,  6.2337e-04, -9.3486e-04,\n",
       "         -2.8990e-04,  5.9499e-04,  7.1882e-04, -7.1789e-04, -5.8929e-04,\n",
       "          1.4451e-04, -4.3434e-04,  4.5680e-04, -4.1439e-04,  1.5470e-03,\n",
       "         -8.5473e-05,  1.4474e-03, -1.0954e-04, -8.9719e-04,  1.0181e-03,\n",
       "          2.1427e-04, -1.9866e-04,  1.3784e-03, -1.0784e-03, -1.8610e-03,\n",
       "         -1.1339e-03, -4.3579e-04,  1.0281e-03, -8.3387e-05,  1.0443e-03,\n",
       "          2.8671e-04, -6.9465e-05, -6.4283e-04,  1.0810e-03,  3.2347e-05,\n",
       "         -2.1468e-04,  8.4549e-05, -1.1550e-03,  1.0319e-03, -4.2468e-04,\n",
       "          1.0745e-03,  9.4955e-04, -1.4288e-04, -1.4889e-03,  5.2432e-04,\n",
       "         -4.2133e-04,  4.5194e-04, -2.4972e-04, -5.7866e-04, -6.4800e-04,\n",
       "          5.8188e-04, -2.6210e-04, -4.0598e-04,  3.6329e-04,  3.2116e-05,\n",
       "         -6.4887e-05, -1.5955e-03,  1.3525e-03,  2.3526e-04, -1.3382e-04,\n",
       "          1.3723e-04, -5.9886e-04,  4.7245e-04, -2.0322e-04,  3.9028e-04,\n",
       "         -3.0252e-04,  5.4681e-04,  5.4456e-04, -3.8148e-04,  4.4782e-04,\n",
       "          2.0732e-04, -9.5509e-05, -1.8937e-03, -1.0939e-03,  8.4512e-04,\n",
       "          3.2895e-04, -1.2670e-03,  5.6260e-04,  4.0811e-05,  3.8687e-06,\n",
       "         -5.6061e-04,  1.9476e-04, -3.4772e-04,  4.9102e-04, -8.5806e-04,\n",
       "          5.4890e-04, -4.1408e-04, -6.7877e-04,  3.7818e-04,  8.0029e-04,\n",
       "          2.7899e-04, -6.4297e-04,  4.4134e-04, -1.9618e-04, -1.3952e-03,\n",
       "          6.4426e-04, -8.7619e-06, -5.0578e-04,  2.9734e-04,  7.0445e-06,\n",
       "          9.0282e-05,  1.1070e-04,  1.5497e-05,  9.8447e-04, -4.9294e-04,\n",
       "          7.3496e-05,  2.2374e-04, -4.7802e-04, -1.4232e-03,  4.3501e-04,\n",
       "         -1.6666e-03,  2.3639e-05,  9.1556e-04, -4.7292e-04, -3.6648e-04,\n",
       "         -3.2313e-05,  2.4104e-04,  5.4641e-04, -3.2794e-04, -8.0653e-04,\n",
       "         -1.8949e-04,  2.6239e-04,  5.7756e-04,  9.3587e-04,  7.9285e-04,\n",
       "          3.9955e-04, -4.9721e-04,  1.7291e-04,  2.2683e-05,  1.2775e-04,\n",
       "         -2.6152e-05,  2.5026e-04, -7.1627e-04, -6.1098e-05, -5.2954e-04,\n",
       "          1.3369e-03,  1.7033e-04,  9.5544e-04, -1.3693e-04, -2.5699e-05,\n",
       "         -7.3381e-04,  5.1753e-04,  6.5217e-04,  1.3444e-03,  7.5753e-04,\n",
       "         -6.1478e-04,  3.6079e-04,  4.0828e-04,  7.9050e-04,  8.8379e-04,\n",
       "          5.1473e-04, -9.3216e-04, -6.3017e-04, -9.9880e-04,  1.0387e-03,\n",
       "         -2.3618e-03,  6.9145e-05, -7.7879e-04, -1.3172e-03, -4.1399e-04,\n",
       "         -2.6530e-04, -2.6007e-04,  8.7631e-05,  3.9522e-04, -4.8500e-04,\n",
       "         -3.6272e-04,  1.9374e-04, -8.3484e-06,  8.1292e-05,  1.7447e-04,\n",
       "         -7.5656e-04, -1.1733e-03,  6.7383e-04,  4.3009e-04,  7.1790e-04,\n",
       "         -8.1432e-04, -5.6490e-04, -4.6854e-04, -7.1134e-06, -4.5110e-05,\n",
       "         -2.3841e-04,  4.2710e-04,  1.2533e-04,  4.7083e-04,  5.9911e-04,\n",
       "         -6.1911e-04,  1.0706e-03,  2.8204e-04, -1.9892e-04, -5.7717e-04,\n",
       "         -2.3590e-04, -1.4096e-03,  1.2124e-03, -1.0349e-05, -4.4314e-04,\n",
       "         -3.1742e-04,  2.3953e-04,  1.3456e-05, -1.0432e-03,  9.8781e-04,\n",
       "         -5.7392e-04,  3.5376e-04, -1.3093e-03, -3.3092e-04, -4.6032e-04,\n",
       "         -3.8092e-04,  1.4322e-04,  1.6070e-03, -2.6579e-04,  8.3272e-04,\n",
       "          1.0475e-03, -3.0097e-04,  1.0469e-03,  3.0597e-04, -1.1167e-03,\n",
       "         -2.6760e-04,  1.1570e-04]),\n",
       " tensor([[ 2.0221e-03, -2.3942e-03,  1.9590e-03,  ...,  4.0387e-04,\n",
       "          -2.8521e-03,  6.0679e-04],\n",
       "         [ 2.9136e-03,  1.1967e-03, -8.5812e-04,  ..., -1.5851e-03,\n",
       "           3.1096e-03,  1.4013e-03],\n",
       "         [ 6.2409e-04,  1.5508e-03,  1.6652e-03,  ..., -2.9405e-04,\n",
       "           1.1690e-03,  3.6504e-04],\n",
       "         ...,\n",
       "         [-5.2051e-04,  2.0274e-03, -1.1134e-03,  ...,  1.6340e-03,\n",
       "           1.1453e-03,  2.3502e-03],\n",
       "         [ 1.2948e-03, -3.4812e-03, -1.8030e-03,  ..., -6.7480e-04,\n",
       "           1.3042e-03,  8.3553e-04],\n",
       "         [ 3.4081e-03,  3.0427e-03, -9.8300e-04,  ...,  1.0484e-03,\n",
       "          -4.5069e-05,  1.2879e-03]]),\n",
       " tensor([ 0.0006, -0.0009, -0.0005,  ..., -0.0009,  0.0002, -0.0010]),\n",
       " tensor([[-2.1686e-03,  4.7789e-04,  4.5532e-04,  ...,  4.2738e-04,\n",
       "          -2.8290e-03, -9.6340e-04],\n",
       "         [-3.1763e-03,  2.6129e-03,  1.5053e-03,  ...,  6.3023e-04,\n",
       "           6.3518e-04, -2.9875e-04],\n",
       "         [-5.0887e-04, -6.3977e-04, -2.8769e-03,  ..., -2.0829e-03,\n",
       "           3.1267e-03,  4.4193e-04],\n",
       "         ...,\n",
       "         [ 8.9219e-04,  1.7755e-03, -1.7494e-03,  ..., -9.1583e-04,\n",
       "          -6.1780e-04,  1.1388e-03],\n",
       "         [ 5.7593e-06,  7.6108e-04,  6.6499e-04,  ..., -1.1896e-03,\n",
       "           2.6501e-03,  1.0885e-04],\n",
       "         [ 1.5315e-03,  1.2523e-03,  3.5182e-03,  ..., -1.3122e-03,\n",
       "          -2.7716e-03, -7.9972e-04]]),\n",
       " tensor([-2.2845e-04, -1.8695e-04, -5.0245e-05,  2.6974e-04,  2.8343e-04,\n",
       "          4.2504e-04,  1.1283e-04, -3.9804e-04, -5.4810e-05, -1.5490e-04,\n",
       "          4.2596e-04,  1.9338e-04,  6.9148e-04, -3.0152e-04,  5.7707e-04,\n",
       "         -7.4947e-04, -1.4363e-05,  3.3731e-04, -2.5549e-04, -1.0104e-03,\n",
       "         -4.4031e-04,  3.7745e-04, -4.2563e-04,  1.1435e-03, -3.0130e-04,\n",
       "         -3.9051e-04, -9.0335e-05, -1.7492e-04, -4.2815e-04,  1.4413e-04,\n",
       "          1.0007e-04,  1.6719e-04, -2.2414e-04, -6.4481e-04, -3.8825e-04,\n",
       "         -1.1089e-04,  1.4709e-04, -6.0776e-04, -6.2833e-05, -4.8414e-04,\n",
       "          2.1519e-04,  3.4542e-04, -1.1950e-04, -4.1797e-04, -4.7393e-04,\n",
       "          1.0949e-04,  7.6062e-04,  6.6987e-04,  1.2579e-05,  2.2839e-04,\n",
       "         -8.8049e-04,  4.1056e-04, -2.1804e-04,  4.0285e-05, -6.8984e-04,\n",
       "          2.9550e-04, -2.9890e-04, -3.9679e-04,  1.4098e-04, -1.1536e-04,\n",
       "          1.1920e-04, -7.2101e-04,  2.0917e-04, -3.1419e-04,  1.2174e-05,\n",
       "         -4.4033e-04, -2.6610e-05,  3.0677e-04,  2.3416e-04,  5.7720e-04,\n",
       "          3.9108e-04,  8.0976e-05,  4.3366e-04, -9.1184e-04,  4.9676e-04,\n",
       "         -2.1613e-04, -1.8137e-04,  3.0352e-04,  4.8369e-04, -1.3336e-04,\n",
       "         -2.4122e-04, -3.3367e-04,  2.4762e-04, -1.0515e-03, -5.5696e-04,\n",
       "         -3.8260e-05,  2.0922e-04,  4.7983e-04,  5.4661e-05, -2.5775e-05,\n",
       "          2.5628e-05,  3.7617e-05,  1.6175e-04, -3.8482e-04,  3.2148e-04,\n",
       "         -4.3619e-04, -7.0480e-04, -2.3656e-04, -1.9814e-04, -1.2738e-04,\n",
       "          4.4955e-04,  9.3058e-05, -7.1936e-04, -6.0955e-04, -4.0818e-04,\n",
       "          2.0383e-04,  2.6374e-04,  7.9842e-04,  1.3265e-04, -5.0089e-04,\n",
       "         -2.8050e-05,  4.0616e-04,  2.8066e-05, -3.4660e-04, -3.6538e-04,\n",
       "         -1.8008e-04,  1.6575e-04,  3.7696e-04, -4.7414e-04, -6.6528e-04,\n",
       "          3.5440e-04,  1.5028e-04, -1.9036e-04, -1.6492e-04, -1.0680e-03,\n",
       "         -8.2590e-04, -3.3285e-04,  6.6699e-05,  3.9896e-04,  1.0287e-04,\n",
       "         -1.6285e-04, -5.2757e-04, -3.8538e-04,  3.7304e-04, -2.5906e-04,\n",
       "          2.4367e-04,  6.3127e-04,  1.0741e-04, -2.7326e-04, -1.8656e-04,\n",
       "         -5.6070e-04,  2.2811e-05, -2.4220e-04, -4.3690e-04, -5.3292e-05,\n",
       "         -4.6891e-04,  1.4877e-04, -1.9427e-05, -5.7043e-05,  2.7583e-04,\n",
       "         -2.8803e-05,  3.5075e-04, -1.0998e-04, -5.9650e-04, -6.2501e-04,\n",
       "         -5.8121e-04,  1.1818e-04, -1.2037e-04, -9.5099e-05,  2.4212e-04,\n",
       "         -6.5366e-05, -5.6996e-04,  3.3355e-04,  9.0795e-04, -3.0440e-04,\n",
       "          2.0603e-04,  7.9588e-04, -1.1355e-04,  3.2264e-04,  4.8200e-04,\n",
       "         -2.4120e-04,  9.0775e-05,  4.2575e-04,  9.6454e-05,  4.3489e-04,\n",
       "         -5.3707e-05,  1.5482e-05,  7.4448e-05, -2.6160e-04, -3.3860e-04,\n",
       "          4.9378e-04,  2.7482e-04,  2.1706e-04,  6.9567e-04, -6.9499e-04,\n",
       "          5.0103e-04, -1.1863e-04,  6.1072e-04, -1.2300e-04, -2.3415e-04,\n",
       "          2.4902e-04, -5.6772e-04, -5.7779e-05,  5.8920e-04,  6.1390e-04,\n",
       "         -2.9982e-04,  8.2397e-04, -9.9661e-05, -3.1565e-04, -3.0291e-04,\n",
       "         -6.1247e-04,  4.9926e-04,  1.2594e-03, -4.6914e-04,  2.7818e-04,\n",
       "         -6.6108e-04, -2.8516e-04,  4.7113e-04,  6.3974e-05, -9.4060e-05,\n",
       "          1.8067e-04, -3.2087e-04,  6.0179e-04, -2.9579e-05, -7.8741e-05,\n",
       "         -5.9821e-05, -5.4336e-04, -4.5489e-04, -2.3857e-05, -4.3534e-05,\n",
       "          2.7165e-04,  5.6416e-04, -1.6512e-06,  2.4927e-04, -3.7767e-04,\n",
       "         -3.2177e-04, -1.4016e-04, -9.7026e-04, -1.2476e-04, -1.4310e-04,\n",
       "         -4.1111e-04, -5.2489e-05,  2.6880e-04,  7.8988e-04,  2.3890e-04,\n",
       "          4.1009e-04, -1.1743e-04, -2.3026e-04,  6.4147e-04,  3.3267e-04,\n",
       "          3.4654e-04,  1.5480e-03,  3.5446e-04,  2.4541e-04, -5.4775e-04,\n",
       "          4.5459e-04, -6.3574e-04,  1.5353e-04,  2.5813e-04, -5.7589e-04,\n",
       "          1.8692e-04, -1.0181e-04,  4.9582e-04, -2.9850e-04,  2.9141e-04,\n",
       "         -1.5057e-04,  6.6516e-04, -3.4257e-04,  9.7453e-05, -4.4420e-04,\n",
       "          3.2064e-04, -1.9915e-04,  4.9721e-04, -5.4851e-05, -2.6782e-04,\n",
       "         -2.3068e-05, -6.9986e-04, -6.5171e-04, -4.3704e-04, -3.4069e-05,\n",
       "         -3.1534e-04,  6.0701e-04, -1.7893e-04, -8.4381e-04,  4.5514e-04,\n",
       "         -5.2880e-04,  7.6691e-04,  3.0448e-04, -3.3711e-04, -5.1660e-04,\n",
       "         -4.6673e-04, -4.4686e-05,  2.2403e-04,  1.0115e-03,  4.7099e-04,\n",
       "         -3.0924e-04,  3.9160e-04, -6.2012e-04, -9.9697e-05, -7.4701e-06,\n",
       "          4.4661e-05,  7.4977e-04,  1.1158e-04, -6.4798e-05,  2.3039e-04,\n",
       "         -2.7228e-04,  4.9781e-04,  1.2713e-03, -6.3625e-04, -3.6246e-04,\n",
       "          1.5318e-04, -5.0191e-04, -8.6024e-04,  2.9747e-04,  2.3551e-05,\n",
       "          7.2287e-04, -3.2608e-04, -5.5760e-04,  3.9294e-04,  2.6839e-05,\n",
       "          1.8476e-04, -8.4030e-04,  1.4467e-04,  2.4282e-04, -4.9042e-05,\n",
       "         -4.0593e-04, -7.4744e-05,  1.8154e-04, -1.4384e-04,  5.2743e-04,\n",
       "          2.9565e-04, -1.2935e-04, -2.1789e-04, -6.6827e-04, -2.2911e-04,\n",
       "         -2.9986e-04, -5.7259e-04,  6.6472e-05, -4.7594e-04, -1.5495e-04,\n",
       "         -1.6542e-04,  3.4599e-04,  2.1214e-05, -2.6211e-04,  4.6262e-04,\n",
       "         -5.4947e-04, -4.9634e-04,  7.3738e-04,  1.6394e-04, -7.4934e-05,\n",
       "         -1.1008e-04,  2.8382e-04,  6.7198e-04, -1.9739e-04,  2.3806e-04,\n",
       "         -3.9952e-04,  5.9689e-04,  2.1682e-04,  4.2828e-04,  5.0128e-06,\n",
       "          4.2963e-04, -1.5071e-04, -2.3748e-04,  1.2372e-04, -3.4189e-04,\n",
       "          3.0059e-04,  4.2266e-04,  2.4996e-04,  3.7302e-04,  6.1421e-04,\n",
       "          8.8900e-05,  3.1594e-04, -4.8180e-04, -2.7619e-04, -8.4382e-04,\n",
       "          1.4868e-04,  3.9167e-04,  7.6368e-04,  3.3444e-06, -6.1331e-04,\n",
       "          5.2001e-04,  3.0756e-04, -6.5499e-04, -3.1454e-04,  6.7520e-06,\n",
       "          5.9998e-04,  2.5233e-04,  6.1512e-05,  1.0054e-05,  7.2202e-04,\n",
       "         -1.2931e-04, -1.4885e-04,  4.7120e-04, -4.0994e-04, -1.0828e-04,\n",
       "          9.3665e-05, -3.3513e-04, -1.7709e-04, -3.6689e-04,  3.7980e-04,\n",
       "         -1.0690e-03,  2.8501e-04, -3.3911e-05, -4.2985e-04, -1.1116e-04,\n",
       "         -6.5551e-04, -1.9315e-04,  7.9451e-05, -4.9342e-04,  4.3899e-04,\n",
       "          5.0425e-05, -7.9853e-05,  5.8437e-04,  3.5288e-04,  1.1646e-04,\n",
       "         -1.6901e-04,  5.1029e-05, -1.5582e-04, -2.3670e-05, -5.4337e-04,\n",
       "         -4.7391e-05,  7.2267e-05, -6.1773e-04, -1.1889e-04,  5.7013e-04,\n",
       "         -1.9109e-04,  1.1141e-04, -4.0614e-04, -7.3166e-04, -4.2953e-04,\n",
       "         -4.3900e-05,  2.7501e-04,  1.1267e-04,  1.8085e-04,  1.0482e-04,\n",
       "         -5.4090e-05, -3.2646e-04,  8.2701e-07, -7.0619e-04,  3.8978e-04,\n",
       "          7.8771e-06,  2.3838e-04, -1.8355e-04,  5.0482e-05, -3.2641e-04,\n",
       "          1.5591e-04,  2.5782e-04, -9.2334e-04, -2.2434e-04, -1.4263e-04,\n",
       "          8.5155e-04,  5.6301e-04,  1.6455e-04, -8.5372e-04, -6.6612e-04,\n",
       "         -3.5313e-04, -1.9780e-04,  3.5744e-04,  6.6211e-04, -8.0256e-04,\n",
       "          2.9283e-03, -1.8357e-04,  7.3534e-05,  5.3991e-05, -1.6905e-04,\n",
       "          4.7958e-04,  6.6122e-04, -4.4283e-04, -5.5459e-04, -2.7726e-04,\n",
       "         -1.1376e-04, -3.1459e-04,  3.6251e-05, -2.4268e-04, -1.4116e-04,\n",
       "          3.6588e-04,  4.9177e-04, -6.7264e-04, -1.5409e-04, -3.0131e-04,\n",
       "         -1.9623e-04,  1.8177e-04,  2.5240e-04, -1.7068e-04, -3.3895e-04,\n",
       "          5.2180e-04,  4.8980e-05,  4.2720e-04, -5.0837e-05,  4.3543e-05,\n",
       "          6.7557e-04, -5.5675e-04,  3.9621e-04, -2.1185e-04,  2.4648e-05,\n",
       "          8.1488e-05, -4.7516e-06, -4.9923e-04,  1.3080e-04,  6.2114e-04,\n",
       "         -3.7040e-04, -3.1159e-04,  1.2444e-04,  3.1770e-04, -7.9209e-06,\n",
       "          1.3950e-04,  1.1578e-04,  6.3723e-04,  3.4235e-04,  1.7148e-04,\n",
       "          9.2760e-06,  1.1731e-04, -5.3798e-04,  7.6011e-05, -1.1153e-04,\n",
       "          1.8490e-04,  1.1616e-04, -3.8726e-04,  5.1009e-04,  4.0422e-04,\n",
       "          4.6573e-04, -4.3708e-04]),\n",
       " tensor([-6.6954e-04, -1.7150e-03, -1.0212e-03, -1.8546e-03,  3.6687e-04,\n",
       "          5.8472e-04,  7.3981e-04, -2.4640e-03, -1.6603e-03,  1.3973e-03,\n",
       "         -3.2833e-03,  1.4499e-03,  1.0086e-03,  2.3745e-03,  7.2747e-04,\n",
       "          4.8250e-04,  1.6200e-03, -2.1799e-03, -2.6703e-05, -3.0935e-05,\n",
       "          3.9995e-04,  1.4808e-03,  2.4778e-04, -1.0199e-03, -2.7424e-04,\n",
       "         -9.6017e-04,  5.3453e-04,  4.4322e-04,  2.3872e-03,  1.0120e-03,\n",
       "          1.1193e-03, -1.9497e-04,  1.3322e-03,  8.9628e-04,  1.9442e-03,\n",
       "         -3.3677e-04, -6.8545e-04,  1.2153e-03,  1.7736e-03,  7.5305e-04,\n",
       "         -5.9021e-04, -6.9726e-04, -5.3072e-04, -1.4963e-03,  8.3590e-04,\n",
       "         -1.1474e-04,  8.8477e-04,  2.7272e-03,  6.3121e-04, -1.2012e-03,\n",
       "          9.0450e-04,  9.5248e-04, -4.3571e-05, -3.9786e-04,  3.9494e-04,\n",
       "         -2.0933e-04,  1.6928e-03, -2.1404e-03, -1.7267e-04,  2.2590e-04,\n",
       "          9.3347e-04,  1.5292e-03,  7.0632e-04, -3.3284e-03,  6.3902e-04,\n",
       "          2.0301e-04,  2.5855e-03, -6.1548e-04,  7.9381e-04, -5.7840e-04,\n",
       "          2.7716e-04,  2.0995e-03, -3.3474e-04,  3.2783e-06, -1.3970e-03,\n",
       "         -8.7619e-06, -1.8835e-04, -1.9417e-03, -3.8033e-03, -1.1836e-03,\n",
       "         -2.0756e-03,  9.1034e-04, -4.6021e-04,  2.5177e-04, -3.4457e-04,\n",
       "          1.4314e-03,  2.4647e-04,  1.4631e-03, -1.4997e-03,  1.4448e-04,\n",
       "          3.0947e-04,  1.5867e-04,  2.4869e-03,  6.2215e-04,  1.8138e-03,\n",
       "          3.4398e-04,  9.6285e-04, -8.0919e-04, -5.2315e-04,  4.1503e-04,\n",
       "          9.6023e-05, -6.9815e-04, -2.1458e-06, -1.0673e-03,  2.4194e-04,\n",
       "          6.3050e-04,  2.0188e-03,  2.9147e-05,  1.2362e-04,  1.1910e-03,\n",
       "          3.7473e-04, -1.6273e-03,  8.5890e-05,  2.1517e-04, -5.0735e-04,\n",
       "          1.1286e-03,  2.1940e-03,  7.0518e-04, -1.2980e-03, -4.9251e-04,\n",
       "          1.2505e-03,  1.0954e-03, -7.1883e-05,  2.1576e-03,  5.3847e-04,\n",
       "          1.7245e-03,  2.1478e-03, -3.4982e-04, -1.1472e-03, -1.3483e-04,\n",
       "         -7.6813e-04, -2.8311e-03, -1.0084e-03,  8.3387e-04,  3.9500e-04,\n",
       "          1.7343e-03, -8.0478e-04,  1.2369e-03,  1.4933e-03,  1.8043e-03,\n",
       "          2.6897e-03, -9.4688e-04,  8.5163e-04, -1.1015e-03,  5.0163e-04,\n",
       "          1.4964e-03,  8.5098e-04, -3.2425e-05,  1.1486e-03,  7.6681e-04,\n",
       "         -2.6160e-04,  1.5070e-03, -8.4507e-04,  2.9683e-03,  3.0734e-03,\n",
       "         -8.5413e-05,  3.8517e-04, -1.4533e-03, -1.5500e-03,  1.5956e-04,\n",
       "          1.4867e-03, -3.8880e-04,  1.4538e-04,  1.7736e-03,  9.7984e-04,\n",
       "          1.9749e-03,  1.7743e-03, -2.9262e-03,  2.5507e-03, -1.2919e-03,\n",
       "          4.7499e-04, -2.7493e-03,  6.5756e-04, -1.2351e-03, -6.4015e-05,\n",
       "          9.4086e-04,  1.3652e-03, -8.4668e-04,  1.8415e-03,  1.2048e-03,\n",
       "         -1.8111e-03,  1.1643e-03,  1.3451e-03,  2.5409e-04,  9.8443e-04,\n",
       "          2.2924e-04, -1.5957e-03,  1.1445e-03, -1.6987e-05,  1.7272e-03,\n",
       "          2.5672e-04, -1.4056e-03, -6.9404e-04, -5.0443e-04, -6.2287e-05,\n",
       "         -6.6644e-04,  2.7239e-04, -5.2726e-04,  1.3228e-03, -9.1392e-04,\n",
       "          1.1829e-03, -2.5374e-04, -1.4964e-03,  1.7434e-03,  1.0217e-03,\n",
       "         -8.3441e-04,  1.7361e-03, -5.2184e-04, -2.2358e-03, -1.1106e-03,\n",
       "          1.4536e-03, -2.4701e-03,  5.2553e-04,  3.2258e-04, -2.3861e-03,\n",
       "         -1.4349e-03, -1.2256e-03,  1.6547e-03,  1.2310e-03,  2.1048e-03,\n",
       "          1.8049e-03,  2.2843e-03,  3.2246e-04, -2.7955e-05,  3.9458e-05,\n",
       "         -1.2447e-03,  6.2197e-04, -9.4950e-04,  6.4182e-04,  5.4765e-04,\n",
       "         -2.1436e-03,  5.0712e-04,  1.6178e-03, -5.9450e-04, -1.6207e-03,\n",
       "          5.5182e-04, -1.1826e-03,  7.5054e-04,  1.8982e-03, -7.2080e-04,\n",
       "          8.7249e-04, -1.1081e-03, -1.0720e-03,  1.1755e-03, -1.6308e-03,\n",
       "          6.8355e-04, -4.4692e-04, -8.5163e-04,  2.0927e-03,  3.7622e-04,\n",
       "         -3.0154e-04,  1.0522e-03, -3.0762e-04,  7.2324e-04,  2.3349e-03,\n",
       "          1.8661e-03,  2.3770e-04, -1.0520e-03,  1.9826e-03, -1.8808e-03,\n",
       "         -7.3606e-04,  8.3673e-04,  1.0676e-03, -1.6062e-03,  2.5678e-04,\n",
       "          2.5910e-04,  1.6822e-03,  1.6789e-03,  3.0220e-05,  1.4955e-04,\n",
       "         -1.8902e-03, -2.4414e-04,  3.5453e-04, -1.4434e-03, -7.9691e-05,\n",
       "         -7.3236e-04, -1.1103e-03, -2.5251e-03,  3.6454e-04,  4.7386e-04,\n",
       "          1.2187e-03,  1.0887e-03,  1.3170e-03,  3.5191e-04, -1.4110e-03,\n",
       "         -6.2299e-04,  6.4093e-04, -2.9212e-04, -1.9820e-03, -7.2026e-04,\n",
       "          1.9423e-03,  5.7691e-04,  5.2321e-04,  6.0588e-04, -6.7949e-04,\n",
       "         -9.4891e-05,  4.7088e-06, -5.3447e-04,  4.2671e-04,  2.3365e-04,\n",
       "          1.5713e-03,  4.2707e-04,  2.8144e-03,  3.6931e-04,  1.0000e-03,\n",
       "          3.1011e-03,  2.6038e-03, -7.2438e-04, -1.5923e-03,  1.4651e-03,\n",
       "          1.7297e-03, -1.2796e-03, -1.2806e-03,  9.1785e-04,  1.5867e-03,\n",
       "          5.9921e-04,  1.8760e-03, -4.1324e-04,  1.5172e-03, -4.6992e-04,\n",
       "         -3.0696e-05, -1.4265e-03,  1.2985e-03,  1.2372e-03,  7.1388e-04,\n",
       "         -9.5636e-04, -2.5387e-03, -1.5248e-03,  6.3580e-04,  1.0181e-03,\n",
       "          5.1963e-04,  1.4341e-03, -1.1563e-04,  7.5638e-05,  5.1326e-04,\n",
       "         -5.8770e-05,  1.2988e-03, -4.3577e-04,  6.2495e-04, -2.8032e-04,\n",
       "          1.9210e-03, -1.3518e-04,  8.8310e-04,  4.0156e-04,  2.3568e-03,\n",
       "          1.2830e-03,  2.9725e-04, -2.1168e-03, -1.5501e-03,  1.4423e-03,\n",
       "          4.0281e-04,  4.5663e-04,  1.7774e-03,  1.5421e-03,  1.5858e-03,\n",
       "          2.6594e-03,  4.0305e-04,  8.5884e-04, -2.7375e-03,  1.7804e-04,\n",
       "          1.7881e-06,  8.4120e-04,  2.5219e-03,  1.4251e-03, -1.8537e-05,\n",
       "         -3.9363e-04,  9.5648e-04, -1.2648e-03, -1.3137e-03,  2.1737e-03,\n",
       "         -1.0927e-03,  4.5949e-04,  6.4057e-04, -1.2696e-03, -3.8344e-04,\n",
       "          1.0842e-03, -1.3134e-03, -9.4652e-04, -1.7308e-03,  2.0003e-04,\n",
       "          2.3741e-04,  1.5693e-03, -2.4264e-03,  1.6510e-04, -7.9739e-04,\n",
       "          2.5806e-03,  1.5035e-03, -1.7632e-03,  2.5648e-04,  4.7982e-04,\n",
       "          3.8493e-04,  1.0005e-03,  5.5736e-04,  5.7787e-04, -7.6103e-04,\n",
       "         -1.0403e-03, -1.8389e-03, -8.6451e-04, -8.0395e-04, -8.9020e-04,\n",
       "          8.9478e-04, -2.5696e-04,  1.9709e-03, -8.7965e-04,  1.6689e-03,\n",
       "         -1.3460e-03,  1.4632e-03,  6.9517e-04,  2.1741e-03, -4.8143e-04,\n",
       "          9.8264e-04,  1.8602e-03, -6.2793e-04,  2.0583e-03,  2.9461e-03,\n",
       "          1.2413e-03,  1.1938e-03, -2.1415e-03, -7.2849e-04,  2.2274e-03,\n",
       "         -2.1942e-03, -5.4651e-04, -1.4791e-03, -9.3037e-04,  8.8865e-04,\n",
       "         -5.4920e-04,  3.5644e-04, -1.3906e-03, -1.7208e-04,  2.4653e-03,\n",
       "         -2.1939e-03, -1.0797e-03,  3.4797e-04, -5.6201e-04, -3.2139e-04,\n",
       "          2.0718e-03,  5.4085e-04, -1.1170e-04,  1.0836e-04, -1.1106e-03,\n",
       "         -1.9748e-03, -1.3550e-03,  1.0085e-04, -9.5969e-04, -6.5202e-04,\n",
       "         -2.5386e-04,  2.9285e-03, -9.6327e-04, -1.5942e-03,  2.4122e-04,\n",
       "          1.1207e-03, -1.2437e-03,  6.5088e-04,  1.3191e-03,  1.6654e-04,\n",
       "         -1.1015e-04,  8.2523e-04, -4.5514e-04,  4.3976e-04,  2.0246e-03,\n",
       "          1.4406e-03, -9.7722e-04, -1.4965e-03, -2.1458e-04,  4.9371e-04,\n",
       "          8.7935e-04,  2.4098e-03,  4.0638e-04,  2.2528e-03, -9.7257e-04,\n",
       "         -1.2122e-03, -2.8312e-04,  5.9474e-04,  2.7674e-04, -3.9059e-04,\n",
       "          5.6159e-04, -7.9960e-04, -1.2690e-04,  1.1270e-03,  2.1660e-04,\n",
       "         -5.8222e-04, -2.0278e-03,  8.8632e-05,  4.5830e-04,  3.6103e-04,\n",
       "          1.1166e-03, -1.4281e-04,  2.4444e-04, -6.2311e-04,  4.9502e-04,\n",
       "          1.3972e-03,  6.4683e-04,  1.2288e-03, -1.4514e-03, -1.9521e-04,\n",
       "          2.5375e-03,  8.6099e-04,  3.8147e-05,  2.5486e-03,  2.2358e-03,\n",
       "         -9.9170e-04, -1.2866e-03,  3.6418e-04, -3.7706e-04,  1.0551e-03,\n",
       "          1.8799e-03,  2.1306e-03,  1.7763e-03,  1.6309e-03,  1.4665e-03,\n",
       "         -1.0971e-03, -2.5272e-04]),\n",
       " tensor([ 7.1306e-05, -4.7753e-04, -3.4655e-06,  1.6419e-04,  1.0914e-04,\n",
       "          5.6658e-05,  7.7488e-05, -1.3188e-04,  2.8948e-04, -2.8329e-04,\n",
       "         -3.2583e-04,  1.4628e-04,  2.3856e-04,  2.9929e-04,  3.9526e-04,\n",
       "          5.4374e-05,  1.6902e-04, -2.7211e-04, -4.9488e-04, -4.0209e-05,\n",
       "         -4.0972e-05,  2.9137e-04, -1.2706e-04, -3.5795e-04, -1.2588e-05,\n",
       "          2.5811e-04,  3.0598e-04, -2.2629e-04, -3.0212e-04, -1.2010e-04,\n",
       "          5.1326e-04,  3.6511e-05, -3.5974e-04, -2.4788e-04, -4.0490e-05,\n",
       "          2.6079e-05, -7.6265e-04,  5.8389e-04,  5.5371e-04, -3.1846e-04,\n",
       "         -1.3435e-04, -4.1619e-04,  2.8805e-04, -6.2860e-04, -9.3002e-06,\n",
       "          1.8854e-05,  2.8126e-06,  2.0359e-04,  2.2919e-04, -1.6591e-04,\n",
       "          3.3873e-05,  2.6181e-04, -7.3794e-05,  5.0971e-04, -1.9320e-04,\n",
       "          1.1307e-04,  5.2724e-04,  4.3502e-05,  3.8335e-05, -1.2473e-04,\n",
       "          2.6009e-04,  3.1014e-04, -1.2106e-04,  1.7590e-04, -2.7551e-04,\n",
       "          2.6491e-04, -1.1465e-04,  4.6348e-05,  2.0644e-04,  2.8495e-04,\n",
       "         -3.2378e-04,  1.5608e-04, -4.8397e-04, -1.3290e-04, -7.2443e-06,\n",
       "          4.0466e-06, -9.7377e-05, -4.7612e-04, -6.2331e-05,  1.7091e-04,\n",
       "         -9.3270e-05,  1.4791e-04, -4.6564e-04,  1.0680e-04,  6.8002e-04,\n",
       "         -5.1603e-05,  5.2657e-05,  1.8673e-04, -3.7027e-04, -5.5241e-05,\n",
       "         -2.5579e-04, -5.4183e-04,  2.4021e-04,  1.3021e-04,  3.8176e-05,\n",
       "         -5.3804e-05,  3.6651e-04, -2.4333e-04,  5.4954e-04, -3.4088e-04,\n",
       "         -1.2139e-04,  4.8086e-04, -1.0664e-04,  1.5527e-05,  1.4696e-04,\n",
       "         -7.4536e-05,  1.5572e-04, -6.5800e-05, -2.9251e-05,  2.3332e-04,\n",
       "         -5.1940e-04,  2.5223e-04, -5.4054e-04,  4.0828e-05,  6.1923e-05,\n",
       "          4.0460e-04, -2.6160e-04,  2.0554e-04,  1.2440e-04, -3.5650e-04,\n",
       "          1.7744e-04,  1.3156e-04,  1.3911e-05,  3.1505e-04, -3.9350e-05,\n",
       "         -3.4981e-04, -2.8341e-04, -2.3984e-04,  3.0549e-04, -1.0097e-04,\n",
       "          3.7130e-04, -3.8127e-04,  1.6936e-04,  5.1612e-04,  5.1751e-05,\n",
       "          3.0675e-04,  1.3306e-04,  9.2952e-05,  3.1282e-05, -1.5453e-04,\n",
       "         -2.5319e-04,  1.8267e-05, -7.7553e-05, -2.0663e-04,  1.6708e-04,\n",
       "         -6.8590e-05,  3.1087e-04, -9.5923e-05, -3.6703e-05,  3.7330e-05,\n",
       "         -6.4188e-04,  4.8854e-04, -2.9603e-04, -4.2586e-05, -3.1013e-04,\n",
       "         -2.2756e-04,  1.3034e-04, -3.2383e-04, -1.7588e-04,  5.4387e-04,\n",
       "          6.7096e-05, -5.4866e-05,  4.5411e-04,  2.7114e-04,  5.0397e-04,\n",
       "         -6.3761e-04, -7.2517e-05, -5.2288e-04,  4.0648e-04,  5.3983e-04,\n",
       "          1.5923e-04,  1.6727e-04,  1.7028e-04, -2.6399e-04,  4.4336e-04,\n",
       "          1.3386e-04, -4.0136e-05, -4.0076e-04, -1.5615e-04, -1.7602e-04,\n",
       "          8.0280e-06, -3.1751e-04, -7.3869e-05, -8.3500e-05,  3.1790e-04,\n",
       "          8.1765e-04, -4.1790e-05,  3.0506e-04,  6.0331e-05,  3.3589e-04,\n",
       "          1.2325e-04, -2.9285e-04,  4.2049e-04, -1.8293e-04,  2.3091e-04,\n",
       "         -2.3112e-04,  1.5537e-04,  1.4471e-04,  2.6093e-05, -2.9275e-04,\n",
       "         -3.5739e-04,  4.7732e-05,  2.8689e-04,  8.6443e-06,  3.2065e-04,\n",
       "          8.0284e-05, -6.5602e-05,  1.6955e-04,  2.6418e-04, -4.0357e-04,\n",
       "          2.4149e-04, -3.6198e-05,  3.1422e-04, -3.1140e-05,  3.1320e-04,\n",
       "          2.9311e-05,  3.3874e-04,  2.4714e-04,  1.5341e-04, -3.5795e-04,\n",
       "          3.6156e-04, -6.6523e-05,  1.9380e-04, -6.5885e-04, -5.9214e-04,\n",
       "         -1.8056e-04, -3.4400e-04,  2.7141e-04,  3.7373e-04,  8.4057e-05,\n",
       "         -4.2165e-04,  1.9545e-04,  4.7524e-05,  2.6898e-04, -1.6288e-04,\n",
       "          4.3321e-04,  8.5905e-06,  6.0331e-04,  7.6348e-04,  1.5841e-04,\n",
       "          7.9345e-05,  1.2307e-04, -2.1264e-04,  1.4607e-04, -1.9636e-05,\n",
       "         -4.2046e-04,  2.0407e-04,  6.0350e-06,  9.1729e-04, -5.3288e-04,\n",
       "          3.0955e-04, -2.7573e-05, -6.4716e-05,  1.4638e-04, -3.5828e-04,\n",
       "          6.1839e-04,  1.0370e-03, -5.6928e-04,  4.2647e-05, -8.6418e-04,\n",
       "         -3.1262e-04,  1.7546e-05,  1.6090e-04,  8.5352e-05, -1.9441e-04,\n",
       "          1.4795e-04, -2.8899e-04,  1.2169e-04,  1.9237e-05, -7.1451e-05,\n",
       "          7.2633e-05,  2.1736e-04,  3.2043e-05, -4.0514e-05,  5.5254e-05,\n",
       "         -7.0669e-05, -5.8010e-05, -1.8535e-04, -4.7065e-05, -5.2466e-04,\n",
       "          2.7765e-05,  2.0305e-05,  2.4005e-04, -1.3387e-04,  3.9350e-05,\n",
       "          9.3368e-05, -3.1840e-04,  2.2262e-05, -5.4242e-04,  2.0365e-04,\n",
       "          2.3620e-04,  4.6529e-04,  1.2159e-04, -3.4719e-04, -1.0953e-04,\n",
       "          1.0192e-04, -2.0416e-04,  1.6824e-04, -2.2336e-04, -3.6996e-04,\n",
       "          1.7549e-04,  1.1887e-04,  6.1456e-04,  1.1605e-04,  4.9618e-04,\n",
       "          6.0001e-04, -2.0005e-04,  5.3279e-04, -1.2400e-04, -2.8120e-04,\n",
       "          4.2853e-04, -2.7322e-04, -2.0326e-04,  3.7135e-04, -3.2097e-04,\n",
       "         -1.6055e-04, -7.4763e-05, -9.8310e-05, -2.1079e-04,  2.6025e-04,\n",
       "          3.5021e-04,  1.7065e-04, -1.3851e-04, -1.2704e-04, -5.4665e-04,\n",
       "          3.0010e-04,  1.7781e-04, -1.2151e-04,  4.7736e-05,  2.5537e-05,\n",
       "         -8.8308e-06,  3.8094e-04, -1.5264e-04,  9.7293e-05,  5.0630e-04,\n",
       "          1.3632e-04, -2.6533e-04,  3.3035e-04,  1.6292e-04, -7.0053e-04,\n",
       "          5.0092e-05,  1.2507e-04, -1.8029e-04,  1.3092e-04, -4.5330e-04,\n",
       "         -2.3857e-04, -1.0307e-04,  3.2824e-05,  2.3729e-04, -4.1300e-04,\n",
       "         -8.6054e-07,  1.8733e-04,  3.7326e-04, -4.5937e-05, -2.7342e-04,\n",
       "          1.1194e-04, -4.9944e-05, -1.3153e-04,  2.5678e-04,  1.3467e-04,\n",
       "          3.9436e-04,  2.8208e-04,  9.3179e-05, -4.0994e-04, -4.4174e-04,\n",
       "         -3.7611e-05,  1.4191e-04,  8.0249e-05, -1.5929e-04, -4.5669e-04,\n",
       "         -1.4824e-04,  3.3262e-04, -7.5217e-04, -4.3613e-04,  5.8932e-05,\n",
       "          2.7617e-04,  7.1431e-05,  1.0348e-04,  1.5846e-04,  4.3796e-04,\n",
       "         -2.5595e-04, -4.7214e-05,  5.8685e-04,  1.0366e-04,  1.6826e-04,\n",
       "          8.0469e-05, -3.6027e-05,  2.4099e-04,  8.5616e-05,  2.6873e-04,\n",
       "          5.5938e-04, -1.2554e-04, -1.3171e-05, -2.0827e-04, -2.0014e-04,\n",
       "         -6.7972e-04,  2.8824e-04,  3.0836e-04, -9.7852e-05, -1.1287e-04,\n",
       "         -7.7669e-05,  1.0718e-04, -2.4898e-04, -1.8912e-04, -1.3619e-04,\n",
       "          4.8783e-04, -9.9808e-05, -3.6434e-04,  3.9416e-04, -8.2377e-05,\n",
       "         -1.4901e-04, -1.2138e-04,  2.9047e-04,  6.4661e-05,  2.0196e-04,\n",
       "          3.4978e-04, -1.1936e-04, -8.9599e-05, -3.9326e-05, -4.7154e-04,\n",
       "         -1.3784e-07, -1.2943e-04, -1.7394e-04, -3.6958e-04,  2.9312e-04,\n",
       "         -3.6731e-04,  5.2756e-05,  7.0109e-04, -2.4516e-05, -1.9936e-04,\n",
       "          1.3113e-04,  5.7016e-05,  1.3632e-04, -1.3607e-06,  3.7499e-05,\n",
       "         -9.4585e-06,  3.8015e-04, -8.0146e-05, -5.0963e-04,  9.8841e-05,\n",
       "          1.3129e-04,  2.3445e-04,  1.6220e-04, -8.2608e-05, -4.9530e-04,\n",
       "         -2.9652e-04,  4.9327e-04, -1.4596e-04,  3.5169e-04,  1.5543e-04,\n",
       "         -7.9173e-04,  7.4780e-05,  7.3907e-05, -4.8717e-04, -2.3743e-04,\n",
       "          6.3355e-05,  5.4959e-04, -3.3730e-04,  6.5375e-05, -1.6240e-04,\n",
       "          4.7523e-04,  4.4037e-05,  4.0910e-04, -1.9687e-04, -2.8526e-04,\n",
       "          2.8856e-04,  4.4231e-04,  1.1834e-04, -2.6511e-04, -1.0707e-04,\n",
       "         -3.3639e-05, -5.1798e-05, -1.1008e-06,  1.4085e-04,  2.4913e-04,\n",
       "         -1.6405e-04, -1.0924e-04,  2.3417e-05,  1.6294e-04,  7.0583e-05,\n",
       "          3.7519e-04,  2.8332e-04, -2.5380e-04,  2.1835e-04,  3.2211e-04,\n",
       "          3.7652e-04,  6.4969e-05, -2.5641e-04,  8.1863e-05,  4.8178e-04,\n",
       "         -2.3109e-04,  4.5265e-04,  2.6420e-04,  4.2430e-04, -1.4887e-04,\n",
       "         -1.0526e-05,  4.1566e-04, -6.1328e-05,  6.7052e-04, -1.8453e-04,\n",
       "          3.2037e-04,  3.9617e-04, -4.0933e-05,  1.8029e-04, -2.3537e-04,\n",
       "          8.5473e-05,  9.8923e-05,  8.1584e-06, -6.6172e-05,  8.6867e-06,\n",
       "         -4.3930e-04, -1.0966e-04]),\n",
       " tensor([[ 6.0765e-04, -7.7813e-04, -2.0571e-03,  ...,  2.8764e-03,\n",
       "           2.8181e-04, -1.7282e-03],\n",
       "         [ 3.0760e-03, -1.3782e-03,  1.8113e-03,  ..., -5.7584e-05,\n",
       "           3.2238e-03, -4.7655e-03],\n",
       "         [ 3.6854e-03, -9.6777e-04,  8.2709e-05,  ..., -6.0442e-04,\n",
       "           7.5240e-04,  1.6407e-03],\n",
       "         ...,\n",
       "         [-8.4540e-04, -3.7491e-03,  5.4358e-04,  ..., -1.3092e-03,\n",
       "           1.0597e-03, -1.8209e-03],\n",
       "         [ 9.1776e-04,  2.1847e-03,  1.4075e-04,  ...,  3.0763e-03,\n",
       "          -3.2217e-03,  1.2033e-04],\n",
       "         [ 1.0741e-03, -1.2226e-03, -1.7232e-04,  ..., -7.1704e-04,\n",
       "          -4.5284e-03, -1.6602e-03]]),\n",
       " tensor([ 1.6505e-03,  3.1188e-04,  3.2603e-04,  ...,  2.2376e-05,\n",
       "          5.0042e-04, -1.2408e-04]),\n",
       " tensor([[-8.1013e-04,  2.4154e-04,  1.6665e-03,  ..., -2.9280e-03,\n",
       "          -4.2488e-04,  1.6539e-03],\n",
       "         [ 1.2747e-03,  1.5869e-03,  2.3085e-03,  ...,  7.3165e-05,\n",
       "           1.1805e-03,  1.7778e-03],\n",
       "         [ 1.0592e-04,  1.2331e-03, -2.2387e-03,  ..., -7.3001e-05,\n",
       "          -1.9917e-03, -1.9951e-03],\n",
       "         ...,\n",
       "         [ 1.7356e-03,  2.2274e-03, -1.0666e-03,  ...,  2.5221e-04,\n",
       "           1.0870e-03,  2.5015e-03],\n",
       "         [ 1.7745e-03,  7.9221e-04, -2.9763e-03,  ..., -3.7935e-04,\n",
       "           6.3999e-04,  2.0139e-03],\n",
       "         [-1.3133e-03,  6.9004e-05, -3.7175e-04,  ...,  1.0783e-03,\n",
       "          -2.2372e-03,  4.5693e-04]]),\n",
       " tensor([-2.8402e-04,  9.3170e-05, -8.5177e-05,  2.1691e-04,  2.3550e-04,\n",
       "          4.5498e-04,  3.0940e-05, -3.9734e-04, -2.3618e-04,  2.8923e-05,\n",
       "          6.4814e-04,  9.4377e-05,  5.2874e-04, -3.6836e-04,  4.4890e-04,\n",
       "         -1.0022e-03, -7.9460e-05,  6.0072e-04,  1.6114e-04, -1.0895e-03,\n",
       "         -3.3094e-04,  4.1889e-04, -2.0747e-04,  1.5458e-03, -2.4005e-04,\n",
       "         -4.3617e-04, -2.6624e-04, -3.2982e-05, -2.8903e-04,  1.7828e-04,\n",
       "          6.7692e-05,  2.0853e-04, -2.8431e-05, -5.1131e-04, -4.0291e-04,\n",
       "         -1.0752e-04,  3.5395e-04, -7.1944e-04, -3.4784e-04, -3.1704e-04,\n",
       "          3.2206e-04,  5.7659e-04, -2.0808e-04, -1.6622e-04, -5.4516e-04,\n",
       "          1.1852e-04,  9.5007e-04,  5.7436e-04,  8.5272e-06,  3.4987e-04,\n",
       "         -8.0824e-04,  2.8064e-04, -2.1825e-04, -1.9657e-04, -5.7457e-04,\n",
       "          3.8135e-04, -5.4497e-04, -4.1444e-04,  1.5347e-04, -3.8546e-05,\n",
       "          6.8994e-05, -7.8007e-04,  1.8022e-04, -3.4757e-04,  1.8577e-04,\n",
       "         -5.0689e-04,  2.1657e-05,  3.3242e-04,  1.6982e-04,  5.5732e-04,\n",
       "          5.7545e-04,  2.4261e-07,  5.5121e-04, -7.9911e-04,  5.0157e-04,\n",
       "         -1.8430e-04, -1.6170e-04,  4.6889e-04,  4.5085e-04, -1.7706e-04,\n",
       "         -2.2550e-04, -3.8370e-04,  4.4783e-04, -1.0736e-03, -8.8431e-04,\n",
       "          1.0446e-04,  1.8326e-04,  3.8867e-04,  2.5872e-04,  6.2222e-06,\n",
       "          2.9768e-04,  2.3234e-04,  8.3013e-05, -5.0797e-04,  3.2778e-04,\n",
       "         -3.6511e-04, -1.0126e-03,  3.1743e-05, -4.3644e-04,  6.1257e-05,\n",
       "          5.8454e-04, -6.5420e-05, -6.3958e-04, -6.3863e-04, -5.3838e-04,\n",
       "          2.6795e-04,  1.5472e-04,  9.5019e-04,  7.1111e-05, -6.4384e-04,\n",
       "          2.7771e-04,  3.2217e-04,  2.4526e-04, -3.3827e-04, -3.9411e-04,\n",
       "         -3.7042e-04,  3.7905e-04,  3.0697e-04, -5.3917e-04, -6.3036e-04,\n",
       "          3.4723e-04,  3.7672e-05, -1.9126e-04, -3.8832e-04, -1.0505e-03,\n",
       "         -6.7012e-04, -2.4813e-04,  1.6467e-04,  2.4732e-04,  1.4777e-04,\n",
       "         -3.3866e-04, -4.1807e-04, -4.8154e-04,  6.5962e-05, -2.4498e-04,\n",
       "          1.0514e-04,  5.5780e-04,  1.9901e-05, -1.9204e-04, -2.5392e-04,\n",
       "         -4.8366e-04,  3.7991e-05, -1.7041e-04, -3.6469e-04, -9.5759e-05,\n",
       "         -4.6017e-04, -1.2664e-04,  1.0286e-04, -1.0123e-04,  2.6226e-04,\n",
       "          2.2046e-04,  1.8630e-04,  8.9100e-05, -5.6686e-04, -4.1236e-04,\n",
       "         -5.1464e-04,  7.6193e-05, -4.8816e-05,  5.1928e-05,  1.5498e-05,\n",
       "         -1.8625e-04, -5.1805e-04,  1.3673e-04,  8.6046e-04, -5.9864e-04,\n",
       "          4.9837e-04,  7.9746e-04,  2.8491e-04,  1.8343e-04,  3.1933e-04,\n",
       "         -3.5108e-04,  2.6644e-05,  4.6006e-04,  2.7288e-04,  3.3041e-04,\n",
       "         -5.8987e-05,  5.1633e-06,  2.4328e-04, -1.5046e-04, -3.2525e-04,\n",
       "          5.1865e-04,  4.3024e-04,  2.4681e-04,  7.8240e-04, -8.9080e-04,\n",
       "          1.8515e-04, -1.3938e-04,  4.3649e-04, -6.5712e-05, -3.6333e-04,\n",
       "          1.5765e-04, -4.6340e-04, -2.7872e-04,  6.3343e-04,  4.8460e-04,\n",
       "         -1.5996e-04,  8.3287e-04, -5.6554e-05, -3.7891e-04, -1.0398e-04,\n",
       "         -4.7521e-04,  4.7791e-04,  1.1730e-03, -4.3770e-04,  1.5336e-04,\n",
       "         -1.0125e-03, -2.7869e-04,  4.1830e-04, -1.1732e-04,  1.7285e-04,\n",
       "          1.4704e-05, -1.9747e-04,  4.2930e-04, -4.0355e-05, -1.9156e-04,\n",
       "         -6.5424e-05, -6.4275e-04, -5.1741e-04, -1.1854e-04,  1.7082e-04,\n",
       "          1.4693e-04,  6.2757e-04, -6.0826e-05,  5.1873e-04, -5.5090e-05,\n",
       "         -2.6491e-04, -1.3242e-05, -1.1530e-03, -4.5055e-04, -1.7231e-04,\n",
       "         -2.1551e-04, -5.6859e-05,  1.7607e-04,  6.8383e-04,  3.2983e-04,\n",
       "          2.7193e-04, -5.3244e-05, -4.0778e-04,  4.9574e-04,  2.4981e-04,\n",
       "          2.8046e-04,  1.5559e-03,  4.8826e-04,  1.8679e-04, -5.4562e-04,\n",
       "          5.3922e-04, -7.7095e-04,  1.0839e-04, -9.1717e-05, -1.9027e-04,\n",
       "         -1.2940e-04, -1.1871e-04,  5.7878e-04, -3.7342e-04,  4.5563e-04,\n",
       "         -3.2216e-04,  4.2162e-04, -4.1500e-05,  2.6599e-05, -1.3207e-04,\n",
       "          3.9560e-04, -2.4137e-04,  3.0275e-04, -8.5337e-05, -1.7275e-04,\n",
       "         -1.2576e-04, -6.5050e-04, -7.1902e-04, -4.8524e-04,  5.4211e-05,\n",
       "         -3.8025e-04,  7.8613e-04, -1.7823e-04, -8.2028e-04,  3.8686e-04,\n",
       "         -4.4504e-04,  8.0685e-04,  5.5076e-04, -3.9771e-04, -3.1389e-04,\n",
       "         -5.5328e-04, -6.4365e-05,  1.0449e-04,  1.3086e-03,  6.2814e-04,\n",
       "         -2.7584e-04,  6.4122e-04, -6.0750e-04,  1.2449e-04, -1.0508e-04,\n",
       "         -1.2024e-04,  5.5131e-04,  5.0824e-05,  1.3110e-04,  3.0363e-04,\n",
       "         -3.3073e-04,  6.9490e-04,  1.1719e-03, -6.2414e-04, -2.3591e-04,\n",
       "          1.9272e-05, -5.3557e-04, -1.1330e-03,  2.4972e-04, -2.2887e-04,\n",
       "          5.2398e-04, -2.5949e-04, -8.4629e-04,  4.8345e-04,  2.1503e-04,\n",
       "          3.3029e-05, -5.9198e-04,  1.9387e-04,  3.7611e-05,  2.0320e-04,\n",
       "         -3.7291e-04, -8.7505e-05,  4.4900e-04, -1.0429e-04,  4.4204e-04,\n",
       "          7.1605e-05, -3.7819e-04, -1.1257e-04, -5.8915e-04,  1.2706e-04,\n",
       "         -4.8701e-04, -8.2409e-04,  2.7538e-04, -4.7164e-04, -1.7624e-04,\n",
       "         -1.1758e-04,  2.4277e-04,  1.0517e-04, -8.6792e-05,  1.2187e-04,\n",
       "         -7.0874e-04, -3.0545e-04,  6.4576e-04,  1.0230e-04,  1.5652e-04,\n",
       "         -1.9513e-04,  2.2516e-04,  7.5276e-04, -2.8864e-04,  3.4909e-04,\n",
       "         -2.7436e-04,  8.0460e-04,  1.4860e-04,  2.8793e-04,  1.9133e-04,\n",
       "          3.4840e-04, -1.8353e-04, -3.6262e-04,  8.4829e-05, -9.6144e-05,\n",
       "          1.7821e-04,  5.3310e-04,  5.6291e-04,  1.4383e-04,  5.2283e-04,\n",
       "         -1.4600e-04,  1.8802e-04, -5.1074e-04, -1.3841e-04, -6.8415e-04,\n",
       "          1.0124e-04,  3.5620e-04,  7.2000e-04,  6.2482e-05, -1.9568e-04,\n",
       "          5.6086e-04,  2.8859e-04, -3.7601e-04, -2.0519e-04, -1.2309e-04,\n",
       "          4.9254e-04,  1.4479e-04,  7.5866e-05,  5.2855e-05,  6.3881e-04,\n",
       "          2.1318e-06, -1.2379e-04,  2.2793e-04, -5.6690e-04, -2.0394e-04,\n",
       "          7.7821e-05, -2.7156e-04, -3.0841e-04, -4.3408e-04,  2.6246e-04,\n",
       "         -1.3646e-03,  4.1484e-04,  8.6405e-05, -2.8743e-04, -3.0690e-05,\n",
       "         -3.3528e-04, -3.3958e-04, -9.2505e-05, -3.7358e-04,  4.9474e-04,\n",
       "          1.0693e-04, -1.8907e-04,  7.2540e-04,  3.8710e-04,  2.1265e-04,\n",
       "         -4.1191e-04,  8.3488e-05,  4.9737e-04, -3.0194e-04, -4.9910e-04,\n",
       "          3.4083e-05,  1.7909e-04, -7.9004e-04, -1.7192e-04,  5.1701e-04,\n",
       "         -4.2246e-04,  1.1451e-04, -3.7090e-04, -6.1869e-04, -2.2285e-05,\n",
       "         -8.1506e-05,  3.1927e-04,  1.7291e-04,  3.6641e-04, -3.3274e-05,\n",
       "          2.2176e-04, -3.5213e-04, -3.3648e-04, -6.1518e-04,  5.0068e-04,\n",
       "         -3.7509e-05,  2.1352e-04, -2.5524e-04,  4.5574e-05, -4.2010e-04,\n",
       "          9.3239e-05,  1.0370e-04, -1.0294e-03,  1.0085e-04, -2.2650e-04,\n",
       "          8.9549e-04,  3.7919e-04,  4.6596e-05, -7.3376e-04, -3.6928e-04,\n",
       "         -1.3974e-04, -3.5384e-04,  4.5369e-04,  5.9298e-04, -8.3718e-04,\n",
       "          3.0280e-03, -2.2253e-04,  6.3120e-05,  4.7028e-04, -7.7818e-05,\n",
       "          3.8613e-04,  3.5515e-04, -2.1376e-04, -4.7717e-04, -2.7578e-04,\n",
       "         -2.7666e-04, -3.5391e-04, -8.0056e-05, -1.3201e-04,  8.2387e-05,\n",
       "          2.5296e-04,  3.8582e-04, -7.1905e-04, -6.4792e-05, -2.6045e-04,\n",
       "         -2.5232e-04,  1.9677e-04,  3.4300e-04, -2.7270e-04, -4.7652e-04,\n",
       "          6.4143e-04,  1.7072e-04,  4.8178e-04, -1.0880e-04, -7.1237e-05,\n",
       "          5.8932e-04, -7.1938e-04,  4.9741e-04, -2.3785e-04, -1.5374e-04,\n",
       "         -2.2087e-05, -3.7010e-04, -4.1716e-04,  9.0385e-05,  3.9365e-04,\n",
       "         -2.4548e-04, -4.2974e-04, -4.0254e-05,  1.8753e-04,  9.3137e-05,\n",
       "          1.4801e-04, -2.3768e-04,  6.6319e-04,  2.1064e-04,  2.3473e-04,\n",
       "         -2.9253e-04, -1.5550e-04, -5.1287e-04,  1.1079e-05,  4.9938e-06,\n",
       "          2.9674e-04,  3.4351e-05, -3.1362e-04,  5.7068e-04,  4.1053e-04,\n",
       "          6.9494e-04, -3.9777e-04]),\n",
       " tensor([ 2.8098e-04,  1.0443e-03, -3.7622e-04, -2.3162e-03, -3.6076e-03,\n",
       "         -4.1389e-04, -1.0481e-03, -1.5992e-03, -2.8845e-03, -1.9540e-03,\n",
       "         -6.7031e-04, -1.9655e-03, -5.0747e-04, -2.3172e-03,  1.3622e-03,\n",
       "         -1.4049e-03, -8.3888e-04,  3.4446e-04,  1.8225e-03, -1.6753e-03,\n",
       "         -1.0754e-03,  2.8193e-04,  1.7680e-03, -2.8956e-04, -5.7173e-04,\n",
       "         -4.3914e-03,  2.1635e-03, -9.6935e-04,  2.5547e-04, -2.0170e-03,\n",
       "         -4.1888e-03,  1.5384e-03, -2.5661e-03,  1.1359e-03,  1.3497e-03,\n",
       "          3.3685e-03, -6.1655e-04, -1.0365e-03, -1.3676e-03,  2.7633e-04,\n",
       "         -2.0417e-03,  6.5720e-04, -1.1868e-03, -6.0481e-04, -2.2311e-03,\n",
       "         -1.7091e-03, -5.3692e-04, -1.6673e-03, -1.8425e-03, -1.2167e-03,\n",
       "         -1.7811e-03,  5.5456e-04,  1.0004e-03,  2.3338e-03, -3.3660e-03,\n",
       "         -9.7251e-04, -3.4750e-04,  2.6342e-03,  1.4418e-03,  2.9204e-03,\n",
       "          3.6037e-04, -1.0703e-03, -9.5713e-04, -1.8612e-03,  1.6013e-03,\n",
       "         -1.5545e-03, -1.4296e-03,  4.4227e-05, -1.5267e-03,  1.8159e-03,\n",
       "         -1.8330e-03, -1.3447e-04,  1.2162e-03,  8.0800e-04, -8.4054e-04,\n",
       "         -1.5694e-03, -6.1119e-04,  1.9769e-03,  1.9330e-03,  1.8723e-03,\n",
       "          1.0457e-03, -2.6049e-03, -1.2571e-03,  1.1621e-03, -7.5173e-04,\n",
       "          1.1106e-03,  3.0696e-04, -1.7599e-03, -1.4532e-04, -2.7746e-04,\n",
       "          1.0746e-03,  1.5336e-03,  2.9809e-03, -1.6872e-03,  1.0216e-04,\n",
       "         -2.4070e-03,  1.0550e-04,  2.8977e-03, -3.1151e-03,  7.1585e-04,\n",
       "          2.8476e-03,  7.1013e-04,  5.1868e-04, -3.1758e-03,  1.9994e-03,\n",
       "          2.1067e-03, -1.3053e-05, -5.9175e-04, -5.5773e-03,  9.8217e-04,\n",
       "          2.1901e-03, -1.7285e-06, -5.4669e-04,  1.4553e-03, -3.9582e-03,\n",
       "          1.3187e-03,  3.7158e-04, -2.1056e-03, -3.5822e-04, -1.7470e-03,\n",
       "         -3.1194e-03, -2.6448e-03,  2.0908e-03, -2.2459e-03,  4.6408e-04,\n",
       "         -3.5155e-04,  6.9559e-04, -2.0078e-03,  6.0594e-04, -1.7670e-03,\n",
       "         -2.4023e-03,  4.3380e-04,  1.4155e-03, -6.9797e-04, -3.1887e-03,\n",
       "          1.4553e-03, -1.6317e-03,  1.1483e-03, -4.0209e-03, -3.8526e-03,\n",
       "         -2.1507e-03, -1.3677e-03, -4.5565e-03,  4.6229e-04,  1.2938e-03,\n",
       "         -3.5194e-03,  8.2821e-04, -1.7059e-03,  1.1376e-03,  3.5310e-04,\n",
       "          1.4507e-03, -1.8901e-03,  8.6129e-04,  1.8442e-04, -3.7898e-03,\n",
       "          2.2769e-04, -2.0068e-03,  1.4338e-03,  2.2815e-03, -1.2010e-03,\n",
       "         -1.9182e-03, -1.6869e-03, -2.1033e-03, -2.2852e-04, -4.8518e-04,\n",
       "         -4.2951e-04, -1.7786e-03, -2.3839e-03, -3.2961e-05, -6.8611e-04,\n",
       "          4.7231e-04, -6.2406e-05, -2.3226e-03,  6.6137e-04,  4.9984e-04,\n",
       "         -2.4538e-03,  2.2423e-04,  9.1958e-04, -2.7466e-04, -1.6549e-03,\n",
       "         -9.9832e-04, -2.4711e-03,  1.4665e-03,  1.3566e-04, -6.3426e-03,\n",
       "         -1.0252e-05, -3.5477e-04,  2.6295e-03,  8.6999e-04,  6.3372e-04,\n",
       "          7.0572e-04,  2.0249e-03,  2.6180e-03, -1.5602e-03, -2.7626e-03,\n",
       "         -1.0833e-03, -3.6160e-03, -3.8177e-03, -1.1920e-03, -2.1857e-03,\n",
       "          7.2956e-04,  2.5425e-03, -1.8123e-03, -1.1382e-03, -8.2397e-04,\n",
       "         -1.1104e-03,  1.6981e-03,  6.9153e-04, -1.2076e-04,  4.7863e-05,\n",
       "         -3.0956e-03, -2.2054e-03,  5.3549e-04, -2.6609e-03, -7.0232e-04,\n",
       "         -1.2591e-03,  4.8590e-04, -1.1706e-03, -2.1274e-03,  3.0597e-03,\n",
       "          3.6275e-04,  1.5711e-03, -1.2958e-04,  1.6466e-03, -4.2403e-04,\n",
       "          1.0812e-03,  1.9836e-04, -2.6056e-03, -1.4954e-03, -1.9135e-03,\n",
       "         -1.2051e-03, -1.8837e-03,  9.0265e-04, -6.3694e-04,  1.4001e-03,\n",
       "         -1.0530e-03, -3.0282e-03,  2.2852e-03,  1.1576e-03,  1.3542e-04,\n",
       "         -5.0005e-03, -1.8609e-03,  2.7150e-04,  2.2020e-03, -1.0257e-03,\n",
       "          2.0511e-03, -4.1189e-03, -1.6332e-04, -3.5579e-03, -2.3401e-04,\n",
       "         -1.6271e-03, -3.4066e-03,  9.6428e-04, -2.3165e-03, -2.5183e-03,\n",
       "         -1.9538e-03,  5.5838e-04,  1.9420e-03, -3.8278e-03, -2.8968e-04,\n",
       "         -3.5565e-03, -3.7693e-03, -3.1491e-03, -2.5697e-03,  2.5691e-03,\n",
       "         -1.8502e-03, -1.1930e-03, -9.8896e-04,  6.1047e-04, -3.4205e-03,\n",
       "         -1.9469e-03, -2.2557e-03,  2.4188e-04,  3.2804e-03, -2.7192e-03,\n",
       "         -9.5427e-04, -2.4903e-03,  4.2844e-04, -1.1991e-03, -1.1808e-03,\n",
       "          5.4801e-04, -1.6499e-03,  1.1607e-03, -3.0476e-03, -2.3559e-03,\n",
       "          3.1459e-04, -1.7759e-03,  3.2365e-04, -2.5764e-03, -1.2488e-03,\n",
       "         -7.7558e-04, -2.2173e-03, -1.1191e-03,  4.6182e-04, -2.7394e-04,\n",
       "         -2.0403e-03,  8.8727e-04, -2.3485e-03,  7.9072e-04,  1.0538e-04,\n",
       "         -1.5126e-03,  1.3665e-03, -1.8270e-03,  3.3116e-04, -7.6753e-04,\n",
       "          1.6756e-03,  9.2924e-04, -2.1219e-05,  7.0107e-04,  1.2056e-03,\n",
       "         -3.5531e-03, -5.9009e-05, -6.1691e-04,  1.2469e-04,  1.5259e-05,\n",
       "          2.1091e-03, -3.3319e-03,  2.7503e-03, -1.6025e-03,  3.1400e-03,\n",
       "          8.2147e-04, -3.7658e-04, -1.1427e-03, -1.3161e-04,  1.7082e-03,\n",
       "         -3.3984e-03, -8.0168e-05, -3.4748e-03,  6.0791e-04,  1.9395e-04,\n",
       "          9.8848e-04,  3.0966e-03, -2.7537e-04, -1.1688e-03, -1.9673e-03,\n",
       "         -1.0058e-03, -2.1672e-03, -3.9070e-03,  6.7329e-04, -1.8774e-03,\n",
       "          1.3208e-04,  4.7028e-04, -3.6097e-04, -1.1503e-03, -2.7597e-04,\n",
       "          9.0593e-04, -1.1793e-03,  9.8443e-04,  8.4341e-04,  2.3234e-03,\n",
       "          1.0233e-03, -1.9315e-03,  4.6492e-05, -5.2619e-04,  4.2105e-04,\n",
       "         -4.0134e-03, -6.4969e-05, -2.5630e-05, -2.6877e-03, -3.0994e-03,\n",
       "         -4.4581e-03, -1.7748e-03, -1.8787e-03, -1.9598e-03, -1.2770e-03,\n",
       "         -3.3586e-03, -1.6677e-03,  2.3633e-03, -6.2561e-04,  3.1855e-03,\n",
       "         -3.0774e-03,  2.1924e-03, -3.7103e-03, -2.1840e-03, -2.0897e-04,\n",
       "         -6.0534e-04,  4.4560e-04, -7.0810e-05,  1.2118e-04,  9.5046e-04,\n",
       "         -5.2035e-04, -5.2941e-03,  2.4223e-04,  1.4945e-03, -2.8712e-04,\n",
       "         -9.7477e-04, -2.4735e-03, -2.0458e-03,  6.6572e-04,  1.5107e-03,\n",
       "          5.3644e-04,  3.4869e-04,  8.0407e-04, -1.6236e-03, -1.2898e-04,\n",
       "          1.8591e-04, -3.9178e-04,  1.4374e-03,  2.9516e-04,  1.6713e-04,\n",
       "         -1.2983e-03, -5.5909e-04, -1.3011e-03,  1.6581e-03, -6.4266e-04,\n",
       "         -4.2367e-04,  3.3545e-04, -1.8808e-03, -3.2055e-04, -1.3566e-04,\n",
       "          2.3181e-03, -2.4110e-03,  6.6471e-04, -3.9743e-03, -1.1015e-04,\n",
       "         -1.2354e-03, -2.0807e-03, -1.5225e-03,  7.4100e-04,  1.3139e-03,\n",
       "          2.5235e-03,  1.8588e-03, -5.9724e-05, -7.7689e-04, -7.3421e-04,\n",
       "         -2.4321e-03, -1.3649e-04, -1.3523e-03, -1.8730e-03, -1.0823e-03,\n",
       "         -6.1768e-04, -2.0492e-04, -1.2506e-03,  6.3533e-04,  1.4925e-04,\n",
       "          3.3271e-04, -1.9410e-03, -1.0056e-03,  1.4060e-03, -2.2478e-03,\n",
       "          1.4098e-03,  2.0623e-05,  2.0819e-03,  3.8246e-03, -2.6212e-03,\n",
       "          2.0633e-03,  1.0017e-03,  1.6629e-03,  1.5413e-03, -8.0711e-04,\n",
       "          3.5337e-03,  2.4348e-04,  3.7956e-04,  8.4960e-04, -2.5171e-04,\n",
       "         -2.1029e-04, -1.8775e-04, -1.9380e-03,  2.1147e-03, -1.5209e-03,\n",
       "          1.3702e-03, -9.6953e-04,  4.3029e-04,  1.2357e-03, -3.5933e-03,\n",
       "         -2.4570e-03,  1.5171e-03,  3.4642e-04, -8.8608e-04, -1.3292e-04,\n",
       "         -2.1012e-03,  9.5749e-04, -2.3815e-03,  3.5083e-04,  3.3077e-03,\n",
       "          1.4911e-03, -4.5931e-04, -6.7520e-04, -6.6161e-04, -2.3963e-03,\n",
       "         -1.5389e-03,  3.0252e-03, -1.0849e-03,  1.3231e-03,  1.7692e-03,\n",
       "          1.5786e-03,  1.8733e-03, -1.9263e-03,  1.0425e-03,  9.8288e-04,\n",
       "         -1.7491e-03, -8.4198e-04, -3.3891e-03,  8.3894e-04, -8.7357e-04,\n",
       "         -6.5911e-04,  1.6335e-03,  1.6665e-04, -7.1275e-04, -1.3739e-03,\n",
       "          1.0244e-03,  8.0693e-04, -1.2009e-03, -2.6931e-03, -1.9654e-03,\n",
       "         -1.4980e-03,  2.3961e-05, -2.6046e-03, -8.1527e-04,  3.9256e-03,\n",
       "         -9.2030e-05,  1.1082e-03]),\n",
       " tensor([ 3.8755e-04,  1.7377e-04,  5.5571e-04, -8.1658e-04,  3.8172e-04,\n",
       "         -1.1834e-03,  5.6711e-04,  5.3623e-04, -9.5735e-04,  6.9204e-04,\n",
       "         -7.7697e-04,  3.1555e-04, -1.3264e-04,  6.1025e-04, -3.0831e-04,\n",
       "          3.9937e-04,  2.3113e-04, -1.5971e-04,  6.8179e-04, -7.6678e-04,\n",
       "          4.9007e-04,  1.6037e-03,  5.6572e-04, -4.6369e-04, -1.9685e-04,\n",
       "          3.2488e-04, -7.8332e-04, -4.9139e-04, -8.7391e-04, -6.8434e-04,\n",
       "          7.4760e-04,  1.9889e-04, -3.5694e-04, -6.5424e-04, -5.6852e-05,\n",
       "          1.7137e-03,  1.0259e-03,  6.7391e-05, -2.6353e-05, -1.0718e-03,\n",
       "         -5.9224e-04, -2.4035e-04, -6.3140e-05, -7.0150e-04,  5.4120e-04,\n",
       "         -3.1466e-04,  3.3862e-04,  4.2342e-04,  6.5732e-04, -5.0364e-04,\n",
       "         -1.1376e-03, -1.7270e-04,  6.1740e-04, -1.0342e-03,  1.2861e-03,\n",
       "         -4.3310e-04,  1.5068e-03,  3.2186e-04, -1.1883e-03,  4.3312e-04,\n",
       "          3.7507e-04,  7.4103e-04, -4.9174e-04,  2.0507e-04, -5.5333e-04,\n",
       "          3.9221e-04, -9.0629e-05, -3.7178e-06, -1.1371e-03, -2.3077e-04,\n",
       "         -8.8517e-04,  6.6735e-04, -1.7387e-04,  2.0299e-04, -1.1920e-03,\n",
       "         -9.8594e-04, -6.4813e-04, -5.3776e-04, -7.6880e-04,  3.7677e-04,\n",
       "          1.1809e-03,  6.5755e-04, -9.6276e-05,  1.9517e-03, -1.2963e-03,\n",
       "          8.2886e-04,  1.1640e-04,  2.1746e-04, -8.4376e-04, -4.8507e-04,\n",
       "         -1.2098e-03,  7.5627e-04,  2.2215e-03, -1.6810e-04, -3.8371e-04,\n",
       "          4.6304e-04,  5.2456e-04,  7.3699e-04, -9.3771e-05,  6.7281e-04,\n",
       "         -2.9856e-04,  6.2557e-04,  1.2438e-03, -5.5506e-04,  5.2254e-05,\n",
       "          8.4862e-06, -1.0281e-03,  2.7463e-05,  4.1659e-04,  3.2698e-04,\n",
       "         -7.1408e-05, -8.4832e-04,  8.7041e-04, -6.3010e-04,  1.7918e-04,\n",
       "         -1.3318e-03,  6.0916e-04, -6.4645e-04,  2.7395e-04, -1.7330e-04,\n",
       "         -1.4385e-03, -1.2937e-03,  1.1658e-03, -4.1447e-05,  1.2243e-03,\n",
       "         -1.8197e-04, -3.7748e-04, -9.6179e-05, -6.2665e-04, -6.1321e-04,\n",
       "          1.0808e-03,  5.7885e-04,  1.0872e-03,  4.4152e-05, -3.0992e-04,\n",
       "         -2.0789e-03, -1.6970e-04, -8.3870e-06, -3.4355e-05, -1.8068e-04,\n",
       "          8.7999e-04, -3.9004e-04, -1.3677e-03,  6.9924e-04,  2.4168e-04,\n",
       "          2.7305e-04,  2.0569e-04, -5.9844e-05, -6.2640e-04,  2.6266e-04,\n",
       "         -1.3587e-04, -5.8251e-04,  8.1564e-04, -2.8601e-04, -5.4065e-05,\n",
       "          1.8944e-03,  1.4960e-04,  1.2772e-05, -2.1373e-04, -5.7773e-04,\n",
       "          3.5972e-04,  8.0731e-04, -5.0798e-05, -2.8384e-04,  1.1560e-03,\n",
       "         -1.6299e-04, -4.2972e-04, -7.6449e-04,  6.4124e-04,  6.5594e-04,\n",
       "          8.1763e-04, -2.0845e-03,  1.0375e-04,  8.0243e-05, -1.0556e-03,\n",
       "         -3.5949e-04, -3.8154e-05, -8.9446e-04, -3.0340e-04, -7.4265e-05,\n",
       "         -7.7285e-05, -1.8975e-03, -7.7204e-04, -1.4027e-03,  1.4687e-03,\n",
       "          1.0381e-03, -2.4343e-04, -6.6683e-07,  5.2921e-04, -1.2091e-03,\n",
       "         -7.3652e-04,  1.7567e-04, -4.6862e-04, -1.2994e-04, -1.4827e-04,\n",
       "          7.7391e-04, -9.3226e-04, -3.1943e-05,  5.5023e-04, -8.6240e-05,\n",
       "          9.1800e-04,  3.6556e-04, -7.6033e-04,  1.1266e-03,  2.8317e-04,\n",
       "          2.3288e-04,  1.7553e-04,  7.4790e-04, -1.0187e-04, -1.5086e-04,\n",
       "         -5.3465e-04,  5.4535e-04, -1.0211e-03,  7.1133e-05,  1.9548e-03,\n",
       "          3.5638e-04,  8.3459e-04,  1.6044e-03, -4.2390e-04, -5.8843e-04,\n",
       "          3.8798e-04,  4.0880e-04, -8.9394e-04, -2.2746e-04,  1.4717e-03,\n",
       "          1.1359e-03,  1.4991e-04,  7.3561e-05,  1.6757e-03, -7.9156e-04,\n",
       "         -4.5352e-04, -6.1852e-04,  3.4253e-04, -1.2423e-03,  4.6364e-04,\n",
       "          3.4028e-04,  1.5216e-04, -7.5115e-04, -1.3170e-03,  1.1143e-04,\n",
       "          1.1213e-03, -8.1700e-04,  1.1613e-03, -2.5391e-04,  1.8229e-03,\n",
       "          7.3642e-05, -1.5517e-04, -1.6532e-03, -2.0936e-04,  7.6151e-04,\n",
       "          6.2136e-04,  4.0277e-05, -2.8430e-04, -3.0457e-04,  9.3648e-04,\n",
       "          7.8185e-06, -1.7647e-04, -9.2407e-04, -5.7812e-07,  1.4599e-03,\n",
       "          2.7864e-04,  5.9890e-04,  6.3678e-05,  6.7180e-04, -4.9689e-04,\n",
       "          2.0907e-04,  1.3269e-04,  2.4838e-04, -7.4363e-04, -4.2461e-04,\n",
       "          1.4415e-03, -1.3385e-05, -1.8205e-03,  2.8082e-04, -1.1643e-03,\n",
       "          1.0978e-03, -2.3968e-04, -1.0438e-04, -6.6777e-04,  8.4440e-04,\n",
       "          1.4450e-03,  1.8433e-05,  1.2657e-03, -1.5708e-03,  7.4901e-05,\n",
       "         -1.3881e-03, -1.0995e-03,  5.3614e-04,  6.8069e-04,  2.1875e-04,\n",
       "         -3.2051e-04,  1.4946e-04,  2.3657e-04, -2.0889e-04,  1.0575e-03,\n",
       "         -1.2320e-03,  1.3141e-03, -8.8065e-04,  1.0256e-03,  2.6055e-04,\n",
       "         -9.6352e-04,  4.3587e-04, -2.3932e-04, -4.9976e-04, -1.3563e-04,\n",
       "          1.1112e-03, -4.3742e-04,  9.3023e-04, -4.0561e-04,  7.0202e-04,\n",
       "          6.2008e-04,  1.5664e-03, -9.2521e-05, -1.8695e-03,  1.3433e-04,\n",
       "         -1.1379e-03, -2.0789e-04,  1.2678e-03,  1.6931e-04,  1.6508e-04,\n",
       "          2.8693e-04,  6.3112e-05,  1.8942e-03, -7.2419e-04, -1.4547e-05,\n",
       "         -4.1071e-05,  1.1347e-04,  2.4191e-04,  1.9453e-03, -1.0019e-03,\n",
       "         -3.1690e-04,  1.0242e-03, -1.1471e-03,  9.9373e-04,  5.0966e-04,\n",
       "          2.5814e-04,  3.7666e-05, -1.0053e-03,  1.5849e-03,  3.5076e-04,\n",
       "         -7.4001e-04,  5.0645e-04, -1.5707e-03, -1.1109e-03, -8.0839e-05,\n",
       "          1.3471e-04, -5.5776e-04,  1.3897e-04,  1.4356e-03, -3.8124e-04,\n",
       "         -4.0977e-04, -4.6057e-04,  1.5098e-03, -2.5376e-04,  1.2076e-03,\n",
       "         -1.0448e-03,  6.3945e-04,  7.9279e-04,  1.2244e-04,  1.6242e-04,\n",
       "         -4.7822e-04,  1.0614e-03, -1.1560e-03, -5.7824e-04,  4.3528e-04,\n",
       "         -1.4864e-04,  2.1270e-04, -3.9244e-04, -4.9182e-04,  1.6261e-03,\n",
       "          1.2143e-03,  3.0702e-04, -5.3007e-04, -7.4387e-05,  3.7669e-04,\n",
       "         -4.1341e-04, -8.0456e-04, -3.9211e-04,  2.9160e-04, -1.1134e-03,\n",
       "          7.7571e-04,  1.3170e-04, -1.1938e-03,  6.9850e-04,  5.2310e-04,\n",
       "         -8.5801e-04, -1.3687e-03, -6.9487e-04,  2.0002e-04, -2.1271e-03,\n",
       "          4.9274e-05, -5.2217e-04, -6.9009e-04, -3.9262e-04, -2.1979e-04,\n",
       "          1.8123e-05, -4.7753e-04,  2.7580e-04,  1.4406e-04, -9.7309e-04,\n",
       "          1.9256e-04,  8.1894e-04,  9.6105e-04, -5.2200e-04, -1.3816e-03,\n",
       "          1.5758e-04, -4.6994e-04,  4.5032e-04, -6.0415e-04,  1.0351e-03,\n",
       "          4.7673e-04, -1.4303e-03,  6.0564e-04, -5.4010e-04, -8.1562e-05,\n",
       "          1.0025e-03,  6.6753e-04,  1.1401e-03,  1.0155e-03,  1.1135e-03,\n",
       "         -6.0630e-04, -5.3243e-04, -8.8859e-04, -2.2851e-05, -4.3838e-04,\n",
       "          2.6255e-04, -9.6985e-04, -4.9780e-04,  4.0799e-04, -3.3641e-04,\n",
       "         -1.5239e-04, -3.1582e-04,  1.3650e-03, -1.0110e-03,  3.4008e-05,\n",
       "         -3.7705e-04,  3.3546e-04,  4.9944e-04,  1.5607e-03,  3.6433e-04,\n",
       "         -1.0778e-03, -5.2476e-04, -1.1391e-03, -1.7923e-04,  2.9833e-04,\n",
       "          1.8830e-04, -1.9936e-04,  6.7703e-05, -1.2960e-04,  9.5247e-04,\n",
       "         -3.5009e-03,  6.6965e-04,  3.3699e-05, -9.5085e-04, -7.5117e-04,\n",
       "         -1.0263e-03, -4.1977e-05,  8.1415e-04, -1.6775e-04, -7.1958e-05,\n",
       "          5.3936e-04,  3.8258e-04, -2.8762e-04,  7.9164e-04,  1.6150e-03,\n",
       "         -8.0072e-04, -8.0617e-04,  7.0965e-04, -1.1456e-03,  6.2178e-04,\n",
       "          1.5201e-04,  6.5077e-05, -1.5061e-04, -3.1925e-04, -6.0483e-04,\n",
       "         -9.0993e-04,  6.3156e-04,  9.2343e-04,  9.3289e-05, -5.4810e-04,\n",
       "         -2.3064e-03,  4.3708e-04, -3.5828e-04, -2.9090e-04,  1.7848e-04,\n",
       "          5.8699e-04, -1.5187e-03, -1.3495e-03,  2.2784e-05,  6.0276e-04,\n",
       "          1.2203e-03,  7.9500e-04, -2.7806e-04,  4.7607e-04, -6.6194e-04,\n",
       "         -1.6178e-03,  9.3435e-06,  6.6608e-06, -1.6949e-04,  8.7427e-04,\n",
       "          7.7639e-04,  7.5080e-04,  3.5394e-04,  1.5218e-03,  5.2613e-04,\n",
       "         -3.1754e-05, -6.3585e-04,  5.5410e-05,  1.3325e-04, -7.3413e-04,\n",
       "          3.8592e-04,  2.3012e-05]),\n",
       " tensor([[-3.5652e-03, -1.2118e-03,  2.1273e-04,  ...,  2.6077e-08,\n",
       "           1.2999e-04, -1.5726e-04],\n",
       "         [ 3.7100e-03, -2.6474e-03, -1.9040e-03,  ...,  9.7007e-04,\n",
       "           2.9253e-03, -3.0351e-04],\n",
       "         [-1.7382e-04,  8.9371e-04,  1.6851e-03,  ...,  8.1207e-04,\n",
       "           1.0384e-03, -7.2131e-04],\n",
       "         ...,\n",
       "         [ 6.2829e-04, -6.9149e-04, -5.6903e-04,  ..., -8.7887e-04,\n",
       "           1.4002e-03, -4.1509e-04],\n",
       "         [ 2.2495e-03,  1.2219e-03,  4.2361e-04,  ...,  2.2981e-04,\n",
       "          -6.2100e-04, -4.3520e-03],\n",
       "         [ 4.6580e-03,  1.0808e-03, -7.3750e-04,  ..., -2.9694e-03,\n",
       "           2.4804e-04, -9.6216e-04]]),\n",
       " tensor([ 3.7528e-04, -9.0124e-04, -2.5713e-05,  ..., -7.7380e-04,\n",
       "         -2.2013e-03,  2.1536e-03]),\n",
       " tensor([[-1.2264e-03,  7.0076e-04, -3.0930e-03,  ...,  4.2178e-05,\n",
       "          -4.2089e-03,  2.3809e-03],\n",
       "         [ 1.8552e-03,  5.4191e-04,  2.2475e-03,  ..., -2.0547e-04,\n",
       "          -1.1497e-04,  1.7668e-04],\n",
       "         [-2.0072e-03,  1.9342e-03,  6.4500e-04,  ...,  1.3390e-04,\n",
       "          -1.8908e-03, -1.8923e-03],\n",
       "         ...,\n",
       "         [-1.6360e-03, -1.5696e-03,  1.7675e-04,  ...,  1.7686e-03,\n",
       "           1.2138e-03,  9.5752e-04],\n",
       "         [-8.1966e-04,  7.5741e-04, -1.0004e-03,  ...,  5.0005e-04,\n",
       "           1.1470e-03,  2.2773e-04],\n",
       "         [-1.1435e-03, -4.5474e-04, -1.4384e-03,  ..., -2.1820e-04,\n",
       "          -3.2417e-03, -1.5121e-03]]),\n",
       " tensor([-3.8103e-04,  3.8078e-05, -2.8667e-04,  2.9206e-04,  1.7357e-04,\n",
       "          7.0836e-04, -7.6741e-05, -4.6183e-04,  4.6410e-05, -2.8566e-04,\n",
       "          8.0427e-04,  2.2175e-05,  5.3542e-04, -4.9167e-04,  4.7662e-04,\n",
       "         -1.0317e-03, -6.7003e-05,  6.7937e-04, -3.1665e-07, -8.4095e-04,\n",
       "         -3.5937e-04, -3.6582e-05, -3.4620e-04,  1.5942e-03, -2.4167e-04,\n",
       "         -5.7721e-04, -2.1980e-04,  1.3795e-04, -4.5554e-05,  2.9438e-04,\n",
       "         -1.5284e-04,  1.2090e-04,  6.6940e-05, -3.7976e-04, -3.2695e-04,\n",
       "         -5.1145e-04,  2.2924e-04, -5.9552e-04, -3.0008e-04, -1.0363e-04,\n",
       "          4.8489e-04,  5.9340e-04, -1.6156e-04, -1.8326e-04, -5.7106e-04,\n",
       "          1.8142e-04,  8.5591e-04,  3.7111e-04, -1.9353e-05,  4.9493e-04,\n",
       "         -5.1369e-04,  2.6668e-04, -2.5434e-04,  3.0216e-05, -7.9136e-04,\n",
       "          4.5585e-04, -9.0198e-04, -4.2088e-04,  4.4379e-04, -1.3220e-04,\n",
       "         -1.6692e-05, -8.9104e-04,  3.0833e-04, -3.7022e-04,  3.3881e-04,\n",
       "         -5.1212e-04,  8.3042e-05,  2.6432e-04,  3.7791e-04,  6.1828e-04,\n",
       "          7.1322e-04, -1.8744e-04,  5.9507e-04, -9.0415e-04,  7.3197e-04,\n",
       "         -2.8219e-05, -5.8224e-05,  5.9031e-04,  5.5438e-04, -2.6257e-04,\n",
       "         -4.9384e-04, -4.8495e-04,  4.4488e-04, -1.4072e-03, -4.9438e-04,\n",
       "          2.2482e-05,  2.1545e-04,  3.3786e-04,  3.9083e-04,  3.2402e-05,\n",
       "          5.5198e-04,  3.3876e-05, -2.7566e-04, -4.3790e-04,  3.3791e-04,\n",
       "         -4.7487e-04, -1.1327e-03, -1.4129e-04, -3.9531e-04, -5.9262e-05,\n",
       "          6.7784e-04, -2.1822e-04, -8.0893e-04, -4.9452e-04, -5.6158e-04,\n",
       "          2.4666e-04,  3.8512e-04,  9.5247e-04,  1.9248e-05, -6.5755e-04,\n",
       "          2.5516e-04,  4.6014e-04,  8.5212e-05, -1.7518e-04, -4.9840e-04,\n",
       "          3.2865e-05,  9.4120e-05,  3.9052e-04, -5.4680e-04, -5.1807e-04,\n",
       "          6.0442e-04,  3.7356e-04, -5.5621e-04, -4.9143e-04, -1.2873e-03,\n",
       "         -6.7784e-04, -1.8445e-04,  1.6015e-04,  3.6675e-04,  2.3950e-04,\n",
       "         -4.9816e-04, -5.6287e-04, -6.4647e-04,  8.4545e-05, -2.2734e-04,\n",
       "          6.2121e-04,  6.1119e-04, -4.4084e-05, -1.8424e-04, -1.7976e-04,\n",
       "         -6.9664e-04,  5.6075e-05,  2.0739e-04, -4.9956e-04, -1.4912e-04,\n",
       "         -5.2401e-04, -8.0423e-05,  8.2932e-05,  8.1539e-05,  1.8113e-04,\n",
       "          1.8735e-04,  3.0257e-04, -1.3602e-04, -5.4401e-04, -3.2887e-04,\n",
       "         -9.1589e-04,  6.1612e-05,  7.0524e-06,  9.0906e-05,  2.3739e-04,\n",
       "         -2.6143e-04, -6.7444e-04,  1.5130e-04,  9.1601e-04, -8.4841e-04,\n",
       "          4.9620e-04,  8.1493e-04,  5.0427e-04,  9.7366e-05,  2.3268e-04,\n",
       "         -5.9297e-04,  4.5762e-04,  4.8557e-04,  2.2070e-04,  4.3893e-04,\n",
       "         -3.1118e-05,  2.0673e-05,  3.8807e-04, -1.4068e-04, -3.1790e-04,\n",
       "          4.2847e-04,  9.3751e-04,  4.8535e-04,  1.1308e-03, -1.1557e-03,\n",
       "         -3.0231e-06, -9.8112e-05,  4.1625e-04, -1.6663e-04, -1.4483e-04,\n",
       "          2.8813e-04, -4.9111e-04, -1.1807e-04,  6.5639e-04,  5.5219e-04,\n",
       "         -3.5900e-04,  1.0573e-03, -2.2458e-05, -5.5886e-04, -1.1543e-04,\n",
       "         -7.0050e-04,  3.8072e-04,  1.3846e-03, -7.4241e-04,  1.3001e-04,\n",
       "         -1.0594e-03, -3.3487e-04,  1.6537e-04, -2.1087e-04,  1.4205e-04,\n",
       "          1.5374e-04, -2.7993e-04,  7.3134e-04, -7.7100e-05, -4.9712e-04,\n",
       "         -2.3694e-04, -8.0861e-04, -8.7373e-04, -4.9029e-05,  3.5179e-04,\n",
       "          7.7892e-05,  5.6361e-04,  8.8408e-05,  6.0639e-04, -3.4777e-04,\n",
       "         -5.8478e-04,  8.1211e-07, -1.1700e-03, -7.6493e-04,  1.3684e-04,\n",
       "         -1.0832e-04,  6.8896e-05,  9.7249e-05,  9.8394e-04,  2.9857e-04,\n",
       "          1.8692e-04, -1.5451e-04, -2.4284e-04,  6.9437e-04,  2.3539e-04,\n",
       "         -1.6926e-04,  1.7335e-03,  2.9402e-04,  2.0906e-04, -9.6701e-04,\n",
       "          5.6749e-04, -7.4490e-04,  4.5569e-04,  4.3368e-05, -3.5341e-04,\n",
       "         -2.9962e-04, -1.3405e-04,  6.8694e-04, -2.6830e-04,  2.5003e-04,\n",
       "         -1.8976e-04,  5.4052e-04,  2.0752e-04,  6.2253e-05, -3.1633e-04,\n",
       "          2.5286e-04, -4.2951e-04,  2.3608e-04, -2.3992e-04, -2.9156e-05,\n",
       "         -1.8331e-04, -7.2223e-04, -7.6561e-04, -2.6216e-04,  3.4583e-05,\n",
       "         -8.0909e-04,  5.9760e-04,  2.1360e-04, -8.7002e-04,  5.8865e-04,\n",
       "         -7.0610e-04,  8.2939e-04,  6.1188e-04, -2.3699e-04, -4.5609e-04,\n",
       "         -8.8187e-04, -6.7115e-05, -1.5944e-04,  1.6989e-03,  4.6604e-04,\n",
       "          1.0557e-04,  8.9934e-04, -6.9795e-04,  4.0343e-05, -1.4400e-04,\n",
       "          5.3074e-05,  5.3124e-04, -3.8321e-05,  1.7013e-04,  5.8643e-05,\n",
       "          8.3130e-05,  3.5181e-04,  1.2896e-03, -8.8153e-04, -2.4462e-04,\n",
       "          2.6124e-04, -6.6944e-04, -7.8209e-04,  3.5778e-04, -2.6131e-04,\n",
       "          3.5709e-04, -1.9295e-04, -9.5034e-04,  6.2138e-04,  4.8615e-05,\n",
       "          2.5972e-05, -9.9114e-04,  1.9759e-04,  3.3359e-04,  2.0813e-04,\n",
       "         -9.9991e-05,  6.8715e-05,  1.9531e-04, -1.3288e-04,  4.1675e-04,\n",
       "          7.7070e-05, -4.0209e-04, -5.3408e-04, -4.1396e-04,  1.1397e-04,\n",
       "         -5.5703e-04, -7.7380e-04,  2.5814e-04, -8.0069e-04,  1.2896e-04,\n",
       "         -6.4935e-05,  1.7165e-05,  3.8727e-04, -3.5088e-04,  8.4319e-05,\n",
       "         -7.2267e-04, -2.6469e-04,  7.6784e-04, -3.1234e-04,  3.3931e-05,\n",
       "         -4.5575e-05,  1.4504e-04,  9.6982e-04, -6.5711e-05,  3.0410e-04,\n",
       "         -2.4066e-04,  9.6234e-04,  1.8254e-05, -1.0408e-04,  2.4419e-04,\n",
       "          5.0456e-04, -3.1292e-05, -7.6152e-04,  1.7616e-04, -4.4146e-04,\n",
       "          3.9653e-04,  3.6596e-04,  2.2723e-04,  8.4934e-05,  4.8203e-04,\n",
       "         -4.1742e-06,  3.8320e-05, -2.8313e-04,  1.9962e-05, -7.6836e-04,\n",
       "          1.3064e-04,  3.5620e-04,  7.7217e-04,  1.4670e-04, -4.9539e-04,\n",
       "          2.6476e-04,  2.6544e-04, -3.1431e-04, -2.1713e-04, -1.8951e-04,\n",
       "          5.8223e-04,  2.9057e-04,  1.6159e-04,  3.3934e-05,  7.8030e-04,\n",
       "         -2.5989e-04, -1.5676e-04,  4.7839e-04, -6.4659e-04, -2.3915e-04,\n",
       "          2.1252e-04, -1.8874e-04, -1.7686e-04, -4.9436e-04,  5.7240e-04,\n",
       "         -1.2700e-03,  5.5185e-04,  1.8578e-04, -1.8198e-04,  1.7468e-06,\n",
       "         -3.1584e-04, -2.5808e-04, -9.4665e-05, -3.7282e-04,  6.7950e-04,\n",
       "          6.6753e-05, -3.4776e-04,  5.0422e-04,  4.8400e-04,  5.3236e-04,\n",
       "         -4.0477e-04,  2.4932e-04,  1.2193e-04, -2.0572e-04, -6.6803e-04,\n",
       "         -8.5359e-05,  4.4740e-04, -8.9225e-04, -7.6950e-05,  4.7805e-04,\n",
       "         -6.2249e-04,  5.4648e-05, -5.5363e-04, -8.5524e-04, -3.4139e-04,\n",
       "          3.3297e-05,  4.2113e-04,  3.2501e-04,  3.7005e-04,  1.4286e-04,\n",
       "          1.9650e-04, -1.9514e-04, -2.9086e-04, -6.6225e-04,  5.2220e-04,\n",
       "         -4.4497e-05,  2.9988e-04, -5.7780e-04,  2.6572e-04, -3.5641e-04,\n",
       "          1.6680e-04,  6.2896e-05, -1.1499e-03, -1.4847e-04, -2.2883e-04,\n",
       "          1.0995e-03,  4.4950e-04,  2.6991e-04, -6.8291e-04, -4.6203e-04,\n",
       "         -2.0332e-04, -2.7201e-04,  5.0414e-04,  6.7215e-04, -9.6157e-04,\n",
       "          3.7772e-03, -3.4537e-04,  9.0990e-05,  6.9388e-04,  5.1763e-06,\n",
       "          5.8753e-04,  3.6516e-04, -3.3885e-04, -4.9918e-04, -2.6157e-04,\n",
       "         -4.0442e-04, -3.3361e-04, -6.4489e-05, -3.0873e-04, -2.0673e-04,\n",
       "          3.4127e-04,  5.8221e-04, -8.7734e-04,  2.7490e-04, -4.2958e-04,\n",
       "         -2.6994e-04,  1.5958e-04,  4.0687e-04, -1.8365e-04, -3.1559e-04,\n",
       "          8.6330e-04,  6.6359e-05,  3.5644e-04, -6.8497e-05,  8.7976e-05,\n",
       "          1.0164e-03, -7.9612e-04,  5.7858e-04, -2.3613e-04, -1.9291e-04,\n",
       "         -5.7178e-05, -2.2069e-05, -6.4336e-05,  1.1283e-04,  2.6253e-04,\n",
       "         -4.2794e-04, -5.8088e-04,  3.4660e-05,  1.2454e-04,  2.4474e-04,\n",
       "          5.2893e-04, -2.5281e-04,  6.4340e-04,  1.9447e-04,  3.7666e-05,\n",
       "         -4.8436e-04, -3.5199e-04, -5.4587e-04, -4.2237e-04, -6.0983e-05,\n",
       "          4.0539e-04,  2.1605e-04, -3.0484e-04,  4.7133e-04,  5.5301e-04,\n",
       "          6.5481e-04, -4.2607e-04]),\n",
       " tensor([ 4.3261e-04,  1.5060e-03,  2.8506e-03,  2.6953e-04,  1.7722e-03,\n",
       "          8.6844e-04,  1.7246e-03,  1.1528e-04,  1.4170e-03, -2.3115e-04,\n",
       "         -2.0233e-03,  1.4555e-03,  9.4706e-04,  4.9454e-04,  4.3589e-04,\n",
       "         -1.2541e-04,  1.0591e-03,  1.2875e-05,  2.5922e-03,  2.3568e-03,\n",
       "          1.5911e-03,  2.8706e-03,  2.4492e-04, -1.6326e-03,  1.0458e-03,\n",
       "          6.5506e-05,  1.7116e-03,  2.0715e-03,  1.2976e-03,  6.5744e-05,\n",
       "          1.5017e-03, -4.9579e-04, -1.0403e-03, -5.1576e-04,  3.8689e-04,\n",
       "          2.8551e-04,  1.5428e-03, -1.1504e-05,  2.2771e-03, -6.0147e-04,\n",
       "          1.4035e-03,  2.8000e-03,  7.8583e-04,  2.2013e-03,  2.3663e-04,\n",
       "          1.9636e-03,  1.1712e-03, -6.1959e-04,  1.7337e-03, -2.7308e-03,\n",
       "          1.1342e-03,  8.5533e-05,  9.2900e-04, -3.1650e-04,  3.5739e-04,\n",
       "          1.2029e-03,  1.0053e-03, -1.0198e-03,  9.4187e-04,  2.8389e-03,\n",
       "         -4.7219e-04,  1.3193e-03, -1.3305e-03, -7.9167e-04,  1.5432e-03,\n",
       "          9.1177e-04, -1.2457e-05,  1.0553e-03,  4.0692e-04,  5.2309e-04,\n",
       "          4.9740e-04,  2.5642e-04,  3.7837e-04,  1.4893e-03,  3.3531e-03,\n",
       "          1.0284e-03, -7.9465e-04,  6.8897e-04, -5.2363e-04,  9.7156e-05,\n",
       "          1.4311e-04, -1.0250e-03,  3.4183e-04,  2.0546e-03,  1.1666e-03,\n",
       "          1.6090e-03,  1.4739e-03, -2.9004e-04,  1.1604e-03, -7.9364e-04,\n",
       "         -3.7980e-04, -2.0010e-03, -1.0480e-03, -1.0048e-03,  1.5318e-03,\n",
       "          2.1824e-03,  7.2432e-04, -1.0372e-03, -7.7075e-04, -9.5284e-04,\n",
       "          3.7289e-04,  2.7788e-03, -1.6379e-04, -1.7107e-04,  1.5420e-03,\n",
       "          2.2787e-04,  2.1362e-03,  9.3186e-04, -1.7643e-04,  4.6933e-04,\n",
       "          1.4484e-03, -2.3019e-04,  2.2334e-03,  3.4899e-04, -2.1459e-03,\n",
       "          5.1761e-04, -9.8580e-04,  1.4358e-03,  9.7013e-04,  1.8811e-03,\n",
       "          3.4961e-03,  1.6602e-03,  1.4413e-03, -7.6085e-04,  7.7915e-04,\n",
       "          2.2004e-03, -1.3638e-04,  1.4652e-03,  4.5478e-04,  4.0770e-04,\n",
       "          2.0254e-04, -1.6145e-03,  1.3149e-03,  1.3682e-03, -6.1500e-04,\n",
       "         -1.7768e-04,  1.3338e-03,  1.4183e-03,  4.3821e-04,  2.2427e-03,\n",
       "          3.0843e-03,  1.4633e-04, -7.2503e-04, -1.2702e-04,  9.2596e-04,\n",
       "          5.0074e-04,  2.3786e-03,  1.6396e-03,  4.5955e-05,  3.2508e-03,\n",
       "         -5.8472e-04,  5.3185e-04,  2.0233e-03,  1.4936e-03,  1.6735e-03,\n",
       "          2.9916e-04,  2.3621e-04,  1.9818e-03,  1.5545e-03,  8.9127e-04,\n",
       "          2.8542e-03, -3.7199e-04,  8.6725e-04,  1.6133e-03,  2.3347e-04,\n",
       "          5.2232e-04,  2.2197e-03,  1.6969e-03,  2.3903e-03,  2.2320e-03,\n",
       "          4.2540e-04, -1.6969e-04,  2.5029e-03, -1.1754e-03,  9.7328e-04,\n",
       "          1.3964e-03,  1.6227e-03,  7.4565e-05,  1.7029e-04,  7.6133e-04,\n",
       "         -2.0515e-03,  6.0409e-04,  8.4859e-04, -5.0014e-04,  5.0604e-04,\n",
       "          2.0191e-03,  1.3519e-03, -7.7486e-05,  1.9004e-03,  6.2072e-04,\n",
       "         -6.1762e-04,  1.3068e-03,  2.6441e-04,  2.3365e-05,  1.1562e-03,\n",
       "         -5.3918e-04,  1.5466e-03, -2.0091e-03,  2.7014e-03, -4.8429e-04,\n",
       "          2.7888e-03,  4.1378e-04,  6.5863e-04,  4.5902e-04,  2.3850e-03,\n",
       "          1.2000e-03,  3.8153e-04,  2.5328e-03, -1.9544e-04,  9.3687e-04,\n",
       "          2.1265e-03,  1.0678e-03, -2.6876e-04,  1.3648e-03, -9.1505e-04,\n",
       "          1.3505e-03,  2.5618e-04,  2.8934e-03,  2.6894e-03,  1.8703e-03,\n",
       "          8.3387e-05,  1.3917e-03, -4.1747e-04, -2.3659e-03,  7.3636e-04,\n",
       "          4.2087e-03,  1.9842e-03, -1.2802e-03,  3.8540e-04,  1.5372e-03,\n",
       "         -8.8078e-04,  1.9134e-03,  7.9650e-04,  1.4654e-03, -1.3137e-03,\n",
       "          4.4435e-04,  1.1994e-03,  9.9772e-04,  2.1747e-03,  6.3777e-04,\n",
       "          1.5982e-03,  9.1672e-05,  1.2474e-03,  2.9892e-04, -1.6710e-03,\n",
       "          1.6499e-03,  7.8845e-04,  4.0722e-04,  9.8592e-04,  9.3865e-04,\n",
       "          1.7449e-03, -8.1605e-04,  1.0040e-03, -8.6194e-04, -7.7820e-04,\n",
       "          3.1435e-04,  6.5750e-04,  4.3595e-04,  1.0852e-03, -5.4955e-04,\n",
       "         -7.2676e-04,  8.7732e-04, -7.8958e-04,  1.8322e-03,  8.1062e-06,\n",
       "          7.1090e-04,  9.5469e-04,  4.3780e-04,  1.4331e-03,  3.5554e-04,\n",
       "          4.5276e-04,  1.3540e-03,  6.7723e-04,  1.7235e-03, -6.4248e-04,\n",
       "          9.2810e-04,  4.5264e-04, -6.1005e-04,  7.6413e-05, -1.3970e-03,\n",
       "         -1.7560e-04,  1.5863e-03,  1.3455e-03,  1.6230e-03, -7.3481e-04,\n",
       "          3.5980e-03,  5.2112e-04,  1.4082e-03,  1.7099e-03, -5.9307e-04,\n",
       "         -1.1694e-04, -1.6005e-03,  6.7037e-04,  3.5558e-03, -7.2521e-04,\n",
       "          3.6514e-04,  1.9426e-03,  2.5833e-03,  2.0466e-03,  1.3148e-03,\n",
       "         -6.7616e-04, -8.4639e-05,  2.3589e-03,  1.9740e-03, -7.6759e-04,\n",
       "         -4.4525e-05, -4.6539e-04, -1.2068e-03, -9.5111e-04,  1.0565e-03,\n",
       "          1.3922e-03,  3.5954e-04,  1.0073e-03, -1.1394e-03,  1.9223e-03,\n",
       "          5.5915e-04,  8.3500e-04,  1.0794e-03,  9.4849e-04,  7.7701e-04,\n",
       "          4.7094e-04,  1.2548e-03,  2.0216e-03,  1.0743e-03, -3.2550e-04,\n",
       "          3.0907e-03, -1.7680e-03,  1.2774e-03, -7.3618e-04,  2.0711e-03,\n",
       "          1.4054e-03,  1.5447e-03,  1.4560e-03, -1.8573e-04,  5.0974e-04,\n",
       "          1.0740e-03, -5.5671e-05,  8.4472e-04,  2.0116e-03, -8.6844e-05,\n",
       "          1.1854e-03, -9.9719e-05,  1.3419e-03,  2.0689e-04,  3.2266e-03,\n",
       "         -1.3621e-03,  1.0668e-03,  1.6485e-03,  2.5278e-04, -1.4418e-04,\n",
       "          1.9509e-04,  1.7681e-03,  3.0273e-04,  2.9821e-03,  5.1349e-04,\n",
       "          2.6224e-03,  1.1139e-03,  1.6798e-03, -3.6538e-04, -8.4460e-05,\n",
       "          3.9240e-03,  1.7937e-03,  1.0705e-04,  3.8409e-04,  2.3844e-03,\n",
       "          1.2683e-03, -4.8780e-04,  1.9624e-03,  4.7684e-04,  7.5769e-04,\n",
       "         -3.1614e-04, -1.2563e-03,  3.1817e-04,  1.8770e-04, -2.2525e-04,\n",
       "          6.8414e-04,  7.5543e-04,  1.0090e-03,  2.2994e-03,  7.6187e-04,\n",
       "          2.0010e-03,  7.6562e-04, -1.5659e-03, -3.3164e-04,  1.5278e-03,\n",
       "         -3.8385e-05,  9.4283e-04,  1.4615e-04, -5.0002e-04, -3.0494e-04,\n",
       "          5.6756e-04,  3.0349e-03,  1.1074e-03, -4.8941e-04,  1.8996e-03,\n",
       "          2.4444e-04, -3.7235e-04, -4.2379e-04,  1.4589e-03, -3.2645e-04,\n",
       "          4.2307e-04,  7.5132e-04,  9.3168e-04, -5.5164e-04, -9.3591e-04,\n",
       "          1.1045e-03,  6.9314e-04,  1.6372e-03,  2.2265e-03,  9.8515e-04,\n",
       "         -5.9676e-04,  1.2357e-03, -9.3615e-04,  3.4325e-03,  1.5659e-03,\n",
       "         -1.9836e-04,  1.1090e-03,  1.2369e-03,  3.0658e-03,  5.8067e-04,\n",
       "          1.9097e-04, -3.2769e-03, -1.7526e-03,  2.1061e-03,  2.2697e-03,\n",
       "          1.0926e-03,  9.7072e-04,  2.0454e-03, -2.1458e-06,  5.2458e-04,\n",
       "          1.2506e-03, -9.5040e-04, -4.4125e-04, -2.6053e-04, -1.0855e-03,\n",
       "          1.8653e-03,  2.2780e-03, -2.7786e-03, -6.4749e-04,  1.6289e-03,\n",
       "         -1.4722e-03,  4.9323e-04,  2.8718e-04,  2.2274e-04,  1.1098e-04,\n",
       "         -1.0428e-03,  9.3311e-04, -6.9433e-04, -1.4764e-04,  1.6624e-03,\n",
       "          2.1479e-03,  3.6448e-04,  3.0262e-03, -4.7880e-04, -2.2568e-03,\n",
       "          5.1644e-03,  8.4156e-04, -7.9477e-04,  1.1170e-03, -6.8843e-05,\n",
       "         -3.6615e-04,  1.2184e-03,  1.5918e-03, -2.7460e-03, -6.2585e-06,\n",
       "          1.0548e-03,  2.5229e-03,  1.1004e-03, -1.2028e-03, -1.2559e-04,\n",
       "          3.6433e-03,  4.3589e-04, -6.1578e-04,  3.5148e-03,  1.0433e-03,\n",
       "          1.1944e-03,  5.8645e-04, -1.5727e-03,  2.4134e-03,  5.8925e-04,\n",
       "          1.3925e-03, -1.9845e-03,  7.3725e-04, -3.8642e-04, -1.2209e-03,\n",
       "          8.1873e-04,  7.4965e-04,  5.3155e-04,  5.1397e-04, -3.6818e-04,\n",
       "          2.1819e-03, -2.0897e-04,  1.2704e-03,  7.2533e-04,  7.3647e-04,\n",
       "         -4.3249e-04,  1.6984e-03, -9.0778e-05,  1.6643e-03,  1.2871e-03,\n",
       "          5.5492e-04, -1.7381e-04,  1.6026e-03,  3.5161e-03,  9.6834e-04,\n",
       "          9.6303e-04, -8.0127e-04,  2.0532e-03,  2.1296e-03, -8.7881e-04,\n",
       "         -2.9299e-03,  6.9845e-04]),\n",
       " tensor([-4.9170e-04,  1.4642e-04, -6.2924e-05, -1.4099e-04, -1.4073e-04,\n",
       "          1.5033e-04, -5.4109e-04, -2.2084e-04,  3.8137e-04, -4.3803e-04,\n",
       "          4.8014e-04,  2.0680e-05, -4.1155e-04, -1.3234e-04, -1.4700e-04,\n",
       "          6.9026e-05,  1.0306e-04,  2.6328e-04, -3.0988e-04,  1.5745e-04,\n",
       "          4.6214e-05,  5.3424e-05,  2.6213e-04,  1.4853e-04, -4.3453e-04,\n",
       "         -3.4960e-04, -4.4569e-05, -2.4699e-06, -1.5664e-04, -1.2296e-04,\n",
       "         -2.2715e-04, -3.5525e-04,  1.2767e-05,  1.4871e-04,  1.6956e-04,\n",
       "         -9.9550e-05,  1.7164e-04,  5.6796e-04,  1.3318e-04,  1.7927e-04,\n",
       "         -9.0711e-05, -2.9739e-04, -4.8731e-05,  3.9666e-04,  2.1011e-05,\n",
       "         -2.3680e-04,  5.5876e-05,  3.4846e-04, -1.5695e-04, -1.0695e-04,\n",
       "          6.0773e-06, -1.6193e-04, -4.2147e-04,  5.0388e-04, -8.2180e-05,\n",
       "          5.4450e-05, -5.2258e-05, -2.3857e-04,  4.2509e-04,  1.9725e-04,\n",
       "          1.3211e-04,  2.2600e-04, -2.1368e-04, -2.1600e-04, -2.7250e-05,\n",
       "          2.3336e-04,  2.3663e-04, -1.7017e-04, -9.1275e-05,  3.8966e-04,\n",
       "          6.6968e-05,  4.7634e-04, -3.6648e-07,  6.5964e-05,  2.0098e-04,\n",
       "          2.9855e-04, -2.7665e-04,  1.4398e-05, -2.0584e-04, -3.2918e-04,\n",
       "         -1.0294e-04,  4.8339e-05,  3.5182e-04,  2.9866e-04, -4.2319e-04,\n",
       "          3.4946e-04, -1.5954e-05,  2.7342e-04, -1.1179e-04,  1.1633e-04,\n",
       "          3.6968e-04, -2.3676e-05,  4.2280e-05,  3.3252e-04,  4.0997e-04,\n",
       "          2.4983e-04,  3.1814e-05,  1.2060e-04,  2.7442e-04,  2.8909e-04,\n",
       "          3.6746e-05,  9.7305e-05,  2.6543e-04, -5.6904e-05,  1.2505e-04,\n",
       "         -1.2913e-04,  3.8627e-04, -1.4546e-04, -1.4566e-04,  7.6722e-05,\n",
       "         -1.2875e-05, -2.8214e-04,  5.1195e-06,  5.3426e-05, -1.3158e-04,\n",
       "          8.5945e-04,  1.2800e-04,  1.4408e-04,  2.2458e-04, -2.0192e-04,\n",
       "          2.1338e-04, -1.6439e-04, -1.6431e-04,  1.3160e-04,  3.7143e-04,\n",
       "          1.4359e-05,  1.5851e-04,  5.4482e-05,  1.7214e-04,  3.0804e-05,\n",
       "          1.4835e-04,  6.0803e-05, -1.6969e-04,  3.1218e-04, -3.2967e-04,\n",
       "          3.9032e-04, -2.6027e-04,  3.3684e-04, -1.3562e-04,  3.2559e-04,\n",
       "          1.5692e-04,  3.1604e-04, -3.5977e-06, -5.8740e-05,  4.4824e-04,\n",
       "         -3.0308e-04,  4.3967e-04, -2.3944e-05, -3.0004e-04, -1.1550e-04,\n",
       "         -1.5499e-04,  1.1008e-05,  1.7432e-04,  1.4297e-04, -1.6338e-04,\n",
       "         -7.9954e-05, -1.2924e-04, -6.0532e-05, -6.5088e-05, -2.1427e-04,\n",
       "         -1.3846e-04,  3.1474e-04, -2.1913e-04,  3.5529e-05,  3.6100e-04,\n",
       "         -3.7786e-04,  3.0641e-06, -6.6282e-05,  1.0476e-05, -1.3414e-04,\n",
       "         -4.9885e-05, -1.1086e-04,  3.5778e-05, -3.7912e-04,  7.3239e-06,\n",
       "          1.3160e-04, -4.3310e-05,  2.3809e-04,  3.3110e-05,  5.2716e-05,\n",
       "         -1.2001e-04,  1.1832e-04, -1.8420e-04, -3.6077e-05,  6.3390e-04,\n",
       "          2.7449e-04, -3.1103e-04, -1.7742e-05, -1.0571e-04, -8.2173e-05,\n",
       "         -2.2353e-04, -5.5172e-05, -2.5405e-04, -1.5930e-05,  3.7430e-04,\n",
       "         -2.4518e-04,  4.2408e-04,  3.1361e-04, -1.6194e-05, -7.8046e-04,\n",
       "          1.9163e-04, -1.9965e-04, -5.3788e-05,  7.7780e-05, -3.2025e-04,\n",
       "         -4.8101e-04, -6.1862e-05,  5.5563e-04,  2.2852e-04, -1.2374e-04,\n",
       "         -2.2355e-04, -8.2191e-05, -2.8475e-04, -2.6159e-05, -1.7767e-04,\n",
       "         -7.9866e-05,  2.2782e-04, -1.1585e-04,  3.6922e-04, -2.6785e-04,\n",
       "          3.0329e-04,  2.2616e-04,  6.2714e-04,  3.1208e-04,  1.0048e-04,\n",
       "         -1.2779e-04,  3.5817e-05,  4.1024e-04, -4.6249e-04,  4.0988e-04,\n",
       "          3.0421e-04,  7.6288e-04, -4.9607e-04,  1.2312e-06, -5.5328e-05,\n",
       "          2.5450e-04, -2.8392e-04,  1.5163e-06,  3.7727e-04,  8.2739e-06,\n",
       "          7.2281e-04, -2.4708e-04, -1.5141e-04,  3.0572e-04,  1.3916e-04,\n",
       "          2.0206e-04,  3.4719e-04, -8.5466e-05,  1.1412e-03,  9.1737e-05,\n",
       "         -3.5395e-05, -4.9753e-04, -2.4832e-04, -3.5649e-04, -5.9034e-04,\n",
       "          4.4753e-04,  4.1502e-04,  3.5035e-04,  1.8739e-04,  1.1935e-04,\n",
       "         -5.9242e-04, -1.8403e-05,  8.9527e-04, -4.7978e-04,  3.7967e-04,\n",
       "          3.7560e-04,  4.8764e-05, -2.3887e-05, -6.8456e-05, -4.7962e-04,\n",
       "          1.2610e-04, -9.5073e-05, -9.6187e-05,  1.1492e-04,  3.2652e-04,\n",
       "          9.1072e-05,  3.5734e-04,  2.6508e-04, -1.3221e-04,  2.1710e-04,\n",
       "         -2.2613e-04,  3.8086e-04,  3.3031e-04,  3.0985e-04, -5.4253e-04,\n",
       "         -1.0780e-04, -3.7913e-04, -7.0222e-06, -2.2759e-04,  6.5286e-05,\n",
       "          4.6144e-04,  2.7255e-04, -5.9959e-05, -3.0369e-05, -3.9935e-06,\n",
       "          3.7625e-05,  2.4805e-04, -4.6616e-04,  3.8466e-04, -2.6202e-05,\n",
       "          1.1837e-04, -1.7639e-04,  6.4191e-04,  3.2056e-04,  9.3333e-04,\n",
       "          4.1931e-04, -1.7040e-04,  1.5745e-04,  1.7338e-04,  1.0364e-04,\n",
       "          2.6813e-04,  2.4353e-04, -1.5006e-04,  6.0654e-04,  2.8152e-05,\n",
       "         -8.9722e-05, -5.1790e-04, -7.6458e-05,  1.4526e-04, -4.3594e-04,\n",
       "         -6.0052e-06,  3.7348e-05, -1.9084e-04,  4.2081e-04, -2.4480e-04,\n",
       "          1.3702e-05,  3.0600e-04, -3.3762e-04,  4.0435e-04, -1.6541e-04,\n",
       "          1.7410e-04, -4.7471e-04, -1.3378e-04, -2.1429e-04, -1.1925e-04,\n",
       "          1.7865e-04, -1.4009e-04,  2.3613e-04, -2.9416e-04, -4.1223e-04,\n",
       "          3.5693e-04,  5.9396e-04, -2.1346e-06, -4.1106e-04,  1.4268e-05,\n",
       "         -1.5286e-04,  9.0815e-05, -3.9168e-04,  1.3582e-04,  1.5255e-04,\n",
       "          3.1981e-04, -2.0820e-04, -6.1306e-04,  2.0491e-04, -4.9829e-04,\n",
       "          8.5682e-05,  2.3672e-04, -6.9067e-04, -8.8315e-05, -9.3937e-05,\n",
       "          6.6117e-05, -2.4775e-04, -1.6727e-05,  2.8059e-04,  2.0512e-04,\n",
       "         -1.4016e-04,  7.3178e-04,  3.6459e-04,  1.0578e-04,  2.3145e-04,\n",
       "          6.8408e-05, -2.1174e-04, -1.2022e-04, -1.6352e-04, -2.1562e-04,\n",
       "          2.6151e-04,  4.4719e-05,  1.5021e-04, -1.0889e-05,  1.9360e-05,\n",
       "          1.2619e-04, -5.9490e-04,  2.0587e-04,  5.4405e-04,  2.2704e-04,\n",
       "          8.2923e-05,  4.2994e-05,  1.2699e-04,  2.6425e-04, -5.2802e-05,\n",
       "         -2.9941e-04,  5.3629e-04, -2.1067e-04, -1.6304e-04, -6.5058e-05,\n",
       "          2.8364e-04, -3.4415e-04,  5.7068e-05, -3.2958e-04,  4.7193e-04,\n",
       "          7.1662e-04,  1.0325e-04, -2.8150e-04,  2.4147e-04,  1.0511e-04,\n",
       "          3.0443e-05,  4.3131e-05, -2.0239e-04,  3.2391e-05, -2.9577e-04,\n",
       "         -2.2169e-05, -8.2994e-05,  2.8834e-04,  4.7334e-04,  2.6282e-06,\n",
       "         -2.7725e-04,  5.4302e-05,  3.2688e-04, -3.5692e-05, -9.9748e-05,\n",
       "          3.4064e-04,  3.2913e-05, -4.0234e-04,  1.0631e-04,  2.6049e-04,\n",
       "         -1.5656e-06, -1.5942e-04,  2.7148e-06,  4.8192e-05, -2.7668e-04,\n",
       "          2.5309e-04,  3.3042e-04, -1.6718e-04,  1.8584e-04, -1.3272e-04,\n",
       "         -1.7416e-04, -2.1092e-04,  2.0429e-04, -2.7816e-04,  2.8576e-04,\n",
       "          2.1742e-04, -2.5660e-04, -2.0562e-04, -1.3791e-04,  3.1947e-04,\n",
       "          1.7788e-04,  7.4865e-04,  3.1968e-04,  1.8682e-04,  5.2145e-04,\n",
       "         -1.9149e-03, -3.3398e-04,  8.5230e-05,  8.1860e-05, -3.9856e-04,\n",
       "         -3.7683e-05, -4.9982e-05,  1.7848e-04, -2.2643e-04,  1.9565e-04,\n",
       "         -6.2067e-05,  4.3899e-04,  1.3500e-04,  2.2354e-04, -7.3448e-05,\n",
       "          7.4062e-05,  3.2702e-05,  1.4200e-04,  3.0871e-04, -2.6112e-04,\n",
       "          1.1694e-04, -8.5304e-05, -5.2990e-05, -1.0653e-04, -2.2943e-05,\n",
       "          1.6977e-04,  3.3095e-05, -1.3938e-04,  6.6360e-04,  9.7945e-05,\n",
       "          1.0047e-04,  1.1473e-04,  4.9568e-04, -3.5431e-04,  1.5364e-04,\n",
       "          2.4561e-05,  3.3066e-05,  6.4708e-06,  2.9694e-04,  6.8306e-05,\n",
       "         -9.2631e-05, -6.5472e-05,  1.3838e-04,  3.1112e-04,  2.1235e-04,\n",
       "          2.1947e-04, -1.2906e-04, -2.3626e-04, -2.7885e-04,  2.5968e-04,\n",
       "          3.4701e-04, -8.5565e-05, -2.7094e-05, -4.6495e-04,  5.2522e-05,\n",
       "          6.1555e-04, -5.5729e-04,  3.6386e-05,  9.6941e-05, -2.0223e-04,\n",
       "         -2.5511e-05, -7.3375e-05]),\n",
       " tensor([[-3.7401e-03,  8.5046e-04,  1.1508e-03,  ..., -1.7140e-03,\n",
       "           2.9210e-03, -3.5859e-03],\n",
       "         [-4.7344e-04, -1.5700e-03, -5.0946e-04,  ...,  2.5728e-03,\n",
       "           3.3655e-04,  1.8262e-03],\n",
       "         [-3.9064e-03, -2.5585e-03,  1.6652e-04,  ...,  1.2781e-03,\n",
       "          -1.2989e-03,  3.2517e-03],\n",
       "         ...,\n",
       "         [-1.2424e-03, -3.0107e-04,  1.5192e-03,  ...,  1.6272e-03,\n",
       "          -1.8945e-03, -4.6235e-04],\n",
       "         [-1.4873e-03, -2.7568e-03, -1.1896e-03,  ..., -7.8023e-04,\n",
       "          -3.7375e-04,  9.4782e-04],\n",
       "         [-2.8558e-05, -7.9340e-04, -4.4632e-04,  ..., -2.1984e-04,\n",
       "           6.8614e-04, -1.2313e-03]]),\n",
       " tensor([ 6.0521e-04,  8.9097e-04, -2.4112e-04,  ..., -8.8662e-05,\n",
       "          2.3818e-04, -1.3047e-05]),\n",
       " tensor([[-7.9543e-04, -1.3018e-03,  2.7335e-04,  ...,  4.8677e-04,\n",
       "           2.5995e-04, -3.3743e-03],\n",
       "         [ 1.4975e-03,  1.2867e-03, -2.6440e-04,  ..., -1.2649e-03,\n",
       "          -8.5901e-04, -1.4304e-03],\n",
       "         [ 8.4486e-04,  6.8447e-04,  4.8601e-04,  ...,  3.5212e-04,\n",
       "          -5.2258e-04, -1.8678e-03],\n",
       "         ...,\n",
       "         [ 1.4823e-03, -3.2037e-03,  1.5299e-03,  ..., -9.4220e-04,\n",
       "           1.1208e-03,  6.2528e-04],\n",
       "         [-1.3402e-03,  1.6084e-05, -9.9720e-04,  ...,  5.8991e-04,\n",
       "          -1.1762e-03, -2.1220e-03],\n",
       "         [ 6.5457e-04,  4.2125e-04, -1.0915e-03,  ...,  1.3799e-03,\n",
       "           3.9956e-03,  1.8168e-03]]),\n",
       " tensor([-5.7684e-05, -2.1215e-05, -1.8314e-04,  4.7673e-04,  2.1801e-04,\n",
       "          6.3697e-04,  2.1168e-04, -2.3783e-04, -7.8220e-05,  7.2215e-05,\n",
       "          5.6291e-04,  3.1108e-05,  7.1695e-04, -4.4793e-04,  5.9178e-04,\n",
       "         -1.1049e-03, -9.6456e-05,  6.2621e-04,  2.5412e-04, -8.5659e-04,\n",
       "         -3.3022e-04, -2.0074e-05, -3.8792e-04,  1.4273e-03, -1.8636e-06,\n",
       "         -3.6089e-04, -1.3970e-04,  8.3644e-05,  1.8193e-04,  3.9149e-04,\n",
       "         -1.5544e-04,  3.6085e-04,  1.5867e-04, -4.8060e-04, -4.0215e-04,\n",
       "         -4.5851e-04,  1.6406e-04, -7.4331e-04, -3.5510e-04, -9.4211e-05,\n",
       "          5.4491e-04,  7.0824e-04, -1.2318e-04, -3.9648e-04, -5.5396e-04,\n",
       "          2.7939e-04,  8.5389e-04,  1.4893e-04,  7.5584e-05,  5.3431e-04,\n",
       "         -4.4531e-04,  2.8786e-04, -2.5093e-05, -1.9249e-04, -7.7603e-04,\n",
       "          4.1298e-04, -8.0354e-04, -2.3683e-04,  2.4552e-04, -2.0771e-04,\n",
       "          1.9607e-05, -1.1322e-03,  3.7234e-04, -2.0918e-04,  3.8167e-04,\n",
       "         -4.8839e-04,  5.1149e-05,  3.9039e-04,  5.0309e-04,  3.5454e-04,\n",
       "          6.8060e-04, -4.5873e-04,  5.9104e-04, -9.6614e-04,  6.3737e-04,\n",
       "         -2.1737e-04,  5.2867e-05,  4.8537e-04,  6.2435e-04, -1.1066e-04,\n",
       "         -3.5259e-04, -5.1384e-04,  1.8531e-04, -1.4430e-03, -2.8547e-04,\n",
       "          1.3046e-04,  3.1417e-04,  2.1086e-04,  5.3331e-04,  2.9048e-05,\n",
       "          2.5882e-04,  8.4360e-05, -2.4243e-04, -5.9034e-04,  1.3157e-04,\n",
       "         -4.9625e-04, -1.0501e-03, -1.7543e-04, -4.5395e-04, -1.3871e-04,\n",
       "          5.3294e-04, -3.4541e-04, -9.8872e-04, -4.2702e-04, -6.2222e-04,\n",
       "          2.6177e-04,  1.6808e-04,  1.0748e-03,  1.9879e-04, -6.0068e-04,\n",
       "          2.2419e-04,  5.8844e-04,  4.5916e-05, -1.3100e-04, -2.8735e-04,\n",
       "         -2.5981e-04,  1.0978e-04,  3.2487e-04, -5.8963e-04, -5.0039e-04,\n",
       "          4.2131e-04,  4.3572e-04, -4.8573e-04, -4.7657e-04, -1.3894e-03,\n",
       "         -6.1917e-04, -2.0815e-04,  1.2888e-04,  3.0927e-04,  1.5883e-04,\n",
       "         -5.3966e-04, -6.8101e-04, -6.0392e-04, -7.7025e-05, -1.3292e-04,\n",
       "          3.9818e-04,  8.7885e-04, -1.8942e-04, -8.4026e-05, -3.9181e-04,\n",
       "         -6.7469e-04,  9.9303e-05,  2.1048e-04, -4.7726e-04, -4.8794e-04,\n",
       "         -1.2766e-04, -3.1299e-04,  1.6064e-04,  2.2901e-04,  3.7858e-04,\n",
       "          3.1052e-04,  1.3937e-04, -2.9979e-04, -5.5869e-04, -5.1667e-05,\n",
       "         -8.4726e-04,  1.7866e-04,  8.2610e-05,  9.9973e-05,  5.7156e-04,\n",
       "         -1.5953e-04, -8.9250e-04,  2.5133e-04,  6.7845e-04, -9.5196e-04,\n",
       "          6.7746e-04,  8.5966e-04,  5.4155e-04,  1.0956e-04,  2.2499e-04,\n",
       "         -5.3665e-04,  5.8282e-04,  6.0054e-04,  4.3410e-04,  4.5032e-04,\n",
       "         -1.0761e-04,  1.0507e-04,  2.3263e-04, -1.3445e-04, -3.1389e-04,\n",
       "          5.2982e-04,  6.9367e-04,  5.6745e-04,  1.1309e-03, -1.5499e-03,\n",
       "         -1.0315e-04,  9.4476e-05,  3.8815e-04, -9.0316e-05, -7.7110e-05,\n",
       "          3.0789e-04, -2.9608e-04, -4.8093e-06,  5.3434e-04,  3.7157e-04,\n",
       "         -2.3182e-04,  8.8414e-04, -6.1139e-05, -4.8305e-04,  3.3198e-04,\n",
       "         -6.7772e-04,  5.0159e-04,  1.2769e-03, -6.2674e-04,  2.9973e-04,\n",
       "         -7.6156e-04, -3.8918e-04, -1.4183e-04, -4.0016e-04,  3.0178e-04,\n",
       "          3.1710e-04, -1.7677e-04,  7.2965e-04, -3.8759e-05, -4.5073e-04,\n",
       "         -2.2113e-04, -8.2871e-04, -7.4878e-04, -1.9227e-04,  5.1855e-04,\n",
       "         -8.8764e-06,  4.8976e-04, -1.6799e-04,  3.6215e-04, -3.1994e-04,\n",
       "         -4.9119e-04, -1.8545e-05, -1.5071e-03, -4.7277e-04, -1.8513e-04,\n",
       "         -1.8371e-04, -1.1002e-04,  3.3876e-04,  8.6597e-04,  4.3167e-04,\n",
       "          8.1735e-05,  7.4327e-05, -2.8046e-04,  3.3992e-04,  2.5123e-04,\n",
       "         -6.5432e-04,  1.6840e-03,  4.4055e-04,  6.3392e-05, -9.7228e-04,\n",
       "          4.5419e-04, -8.7475e-04,  6.0304e-04, -1.1251e-04, -3.4983e-04,\n",
       "         -2.2348e-04,  4.7623e-05,  7.2396e-04,  5.6345e-07,  5.2463e-04,\n",
       "         -2.8839e-04,  4.3552e-04,  1.3502e-04, -1.3298e-04, -4.2188e-04,\n",
       "          3.5602e-04, -4.5922e-04, -2.0140e-04,  4.1136e-05, -2.3501e-04,\n",
       "         -3.1424e-04, -9.1906e-04, -6.7360e-04, -2.8397e-04,  1.5745e-04,\n",
       "         -8.0575e-04,  8.9383e-04,  2.5433e-04, -9.5777e-04,  5.2331e-04,\n",
       "         -6.5714e-04,  7.7235e-04,  4.6604e-04, -1.3186e-04, -5.1058e-04,\n",
       "         -6.4031e-04, -3.2369e-04, -2.6264e-04,  1.5533e-03,  9.1216e-04,\n",
       "          3.0323e-04,  1.0581e-03, -5.7294e-04,  1.2053e-04, -1.3877e-04,\n",
       "         -8.1442e-05,  3.7996e-04, -5.4886e-05,  3.3553e-04,  8.3076e-06,\n",
       "          4.6520e-05,  2.5083e-04,  1.6110e-03, -1.1028e-03, -2.6407e-04,\n",
       "          1.8376e-04, -5.1155e-04, -1.0871e-03,  2.1073e-04, -6.7276e-04,\n",
       "          2.4262e-04, -1.0492e-04, -9.8294e-04,  6.5447e-04,  2.3149e-05,\n",
       "          8.6544e-05, -9.6872e-04,  2.7152e-04,  3.1610e-05,  2.1233e-04,\n",
       "         -5.5586e-05,  4.3258e-04,  3.7041e-04, -2.1798e-04,  5.0623e-04,\n",
       "          4.3814e-05, -3.7541e-04, -4.7586e-04, -5.0577e-04,  2.3511e-04,\n",
       "         -4.7206e-04, -8.7778e-04,  4.1350e-04, -9.0298e-04,  3.0149e-04,\n",
       "         -1.3060e-04,  3.4129e-04,  4.7863e-04, -7.8693e-05,  1.4821e-04,\n",
       "         -8.6755e-04, -2.4178e-04,  7.6005e-04, -2.0978e-04,  1.6717e-04,\n",
       "         -2.9515e-04, -8.6604e-05,  8.7697e-04,  8.4418e-05,  2.8402e-04,\n",
       "         -3.8113e-05,  9.5250e-04,  2.2222e-04, -1.3016e-04,  1.6039e-04,\n",
       "          3.5437e-04,  1.9158e-04, -4.5805e-04, -2.7032e-06, -2.3024e-04,\n",
       "          4.5572e-04,  2.1820e-04,  8.2378e-04,  7.5904e-05,  5.5904e-04,\n",
       "         -6.9315e-05,  1.5473e-04, -2.3906e-04, -2.5572e-05, -8.6700e-04,\n",
       "          2.9561e-04,  9.0748e-05,  5.3521e-04,  2.1303e-04, -5.1943e-04,\n",
       "          2.6140e-04,  4.5091e-04, -1.4102e-04, -1.7254e-04, -5.1484e-05,\n",
       "          3.3636e-04,  2.0717e-04,  1.1028e-04,  5.8438e-05,  7.5758e-04,\n",
       "         -2.2120e-04,  2.6061e-04,  4.2869e-04, -8.6624e-04, -3.5111e-04,\n",
       "          1.6669e-04, -2.5794e-04, -3.5957e-04, -5.6679e-04,  6.4696e-04,\n",
       "         -1.0728e-03,  3.7837e-04,  2.8166e-04, -2.5749e-05,  4.7933e-05,\n",
       "         -3.8534e-04,  1.5100e-04, -1.7345e-04, -5.2397e-05,  4.4672e-04,\n",
       "         -2.4462e-04, -4.2214e-04,  5.4447e-04,  4.1203e-04,  5.1479e-04,\n",
       "         -4.1710e-04,  2.1845e-04,  5.1396e-04, -2.2367e-04, -4.1046e-04,\n",
       "         -4.1167e-05,  4.7802e-04, -9.1844e-04, -2.6429e-04,  4.1842e-04,\n",
       "         -4.1704e-04,  2.4999e-06, -6.6790e-04, -7.1623e-04, -2.8239e-04,\n",
       "         -5.7274e-05,  5.1829e-04,  6.0504e-04,  3.1048e-04, -2.3526e-05,\n",
       "          2.5096e-04, -1.0952e-04, -2.0996e-04, -5.4399e-04,  5.7544e-04,\n",
       "         -1.2061e-04,  3.8095e-05, -4.7383e-04,  1.8573e-04, -1.6107e-04,\n",
       "          2.8413e-04,  1.8950e-04, -1.1265e-03, -1.5977e-05, -3.0096e-04,\n",
       "          1.1674e-03,  5.1708e-04,  2.6094e-04, -5.4866e-04, -5.9297e-04,\n",
       "         -2.2868e-04, -5.8545e-04,  4.0577e-04,  4.6954e-04, -9.4042e-04,\n",
       "          3.1282e-03, -2.1631e-04,  1.7041e-04,  7.1362e-04,  2.7617e-04,\n",
       "          5.5982e-04,  3.1766e-04, -3.8814e-04, -2.5794e-04, -4.4207e-04,\n",
       "         -3.1388e-04, -5.1245e-04, -2.7289e-05, -4.5676e-04, -1.3949e-04,\n",
       "          2.9222e-04,  4.6813e-04, -8.5252e-04,  7.1161e-05, -2.5017e-04,\n",
       "         -3.0963e-04,  1.5553e-04,  4.0173e-04, -5.4351e-05, -3.2908e-04,\n",
       "          5.8924e-04,  2.6811e-04,  4.2792e-04, -3.3603e-04,  5.1372e-06,\n",
       "          8.8554e-04, -9.2982e-04,  4.2716e-04, -1.6469e-04, -2.3998e-04,\n",
       "          8.4098e-07, -9.9089e-05, -3.6737e-05, -1.0345e-04,  2.8070e-04,\n",
       "         -3.1109e-04, -5.8200e-04, -1.0621e-04,  7.4133e-07,  1.8464e-04,\n",
       "          3.2872e-04, -2.0049e-04,  7.1981e-04,  8.9779e-05,  3.7439e-06,\n",
       "         -6.1745e-04, -3.2200e-04, -4.8118e-04, -3.3556e-04, -1.5060e-04,\n",
       "          3.3487e-04,  4.3566e-04, -2.2188e-04,  4.2129e-04,  6.4585e-04,\n",
       "          5.8309e-04, -3.3359e-04]),\n",
       " tensor([-2.7294e-03, -3.2571e-03,  2.9302e-03, -1.8440e-03,  1.2612e-03,\n",
       "          4.1634e-04, -1.2800e-03, -6.7902e-04, -1.9480e-03, -2.2979e-03,\n",
       "         -2.8767e-03, -1.3034e-03, -8.1456e-04,  2.3285e-03, -3.5858e-04,\n",
       "         -2.4090e-03,  2.3270e-04,  3.2842e-04, -8.6051e-04, -3.4636e-04,\n",
       "         -1.2957e-03,  1.7039e-03, -3.2408e-03,  1.3281e-03, -2.0718e-03,\n",
       "         -3.4939e-03,  2.7454e-04, -4.6015e-05, -1.3732e-03, -1.2441e-03,\n",
       "         -1.4197e-03, -5.9593e-04, -1.2447e-03,  8.7452e-04, -1.7912e-03,\n",
       "         -1.6489e-03, -1.2046e-03, -1.5212e-03, -3.1923e-03, -2.5098e-03,\n",
       "          8.4996e-05,  3.6132e-04, -1.3031e-03, -3.4767e-04, -1.0895e-03,\n",
       "          8.9562e-04, -2.6082e-03,  3.4267e-04, -2.2973e-03,  2.6119e-03,\n",
       "         -1.9884e-03,  1.1096e-03,  2.0260e-03, -3.2153e-03, -2.8591e-03,\n",
       "         -1.6890e-03,  8.6629e-04, -7.5382e-04,  6.4462e-04,  1.7565e-04,\n",
       "          9.9635e-04, -1.5792e-03, -7.6205e-04, -1.8763e-03, -2.4859e-03,\n",
       "         -3.8584e-03, -3.7193e-05, -3.9148e-04, -2.5252e-03, -7.3206e-04,\n",
       "         -9.9957e-05, -4.2164e-04, -3.3641e-04,  2.3675e-04,  8.7297e-04,\n",
       "         -1.4673e-03, -1.0442e-03, -2.4084e-03, -2.7043e-03, -8.9705e-04,\n",
       "         -5.4336e-04, -3.0874e-03, -9.6679e-04, -2.1533e-03,  2.2577e-03,\n",
       "         -9.6583e-04, -8.9020e-04, -8.4436e-04, -1.3082e-03, -1.1204e-03,\n",
       "         -6.3014e-04, -2.6076e-03, -4.8405e-04, -1.2196e-03,  1.6551e-03,\n",
       "         -1.2616e-03, -1.2723e-03,  1.1096e-03, -9.7030e-04,  8.9872e-04,\n",
       "         -1.0790e-03,  2.2097e-03,  4.8983e-04, -2.0782e-03, -1.1801e-03,\n",
       "          4.5937e-04, -6.8820e-04, -1.3802e-03, -4.6668e-03, -1.7350e-03,\n",
       "         -1.2413e-03, -8.4043e-04, -5.0735e-04, -8.1980e-04,  1.0669e-05,\n",
       "         -1.2850e-03,  6.0129e-04, -2.9758e-03,  1.4609e-04,  1.6710e-03,\n",
       "          1.8042e-04,  6.1142e-04,  9.6118e-04, -1.1032e-03, -1.2547e-04,\n",
       "         -7.7248e-04, -6.9213e-04, -2.7006e-03,  2.1701e-03, -5.5307e-03,\n",
       "         -1.7011e-04, -2.3032e-03,  1.8010e-03,  5.8401e-04, -2.1052e-03,\n",
       "         -7.1234e-04, -3.2003e-03, -1.1256e-03, -7.5459e-05, -9.3412e-04,\n",
       "         -1.3094e-03,  3.2175e-04, -1.7747e-03,  9.1457e-04,  8.6671e-04,\n",
       "         -1.6348e-03,  1.3798e-03, -1.9190e-03, -6.1285e-04, -1.5781e-03,\n",
       "         -3.8946e-04, -1.5860e-03, -2.3629e-03, -5.0557e-04, -2.1937e-03,\n",
       "          9.5093e-04, -4.1592e-04,  3.9202e-04, -5.3859e-04, -2.1548e-03,\n",
       "         -1.7750e-03, -1.3404e-03, -1.9432e-03,  6.9964e-04, -1.0111e-03,\n",
       "         -1.5059e-03, -2.9624e-03,  1.5467e-04, -2.7895e-04, -2.2045e-03,\n",
       "          6.2323e-04, -8.6319e-04, -3.6208e-03, -9.0551e-04,  6.7651e-04,\n",
       "         -2.6408e-03,  6.5446e-04,  3.0693e-03, -2.7259e-03, -9.0784e-04,\n",
       "         -8.1617e-04, -1.0164e-03, -1.9748e-03, -1.0898e-03, -1.1185e-03,\n",
       "          4.6897e-04, -2.0753e-03,  5.9456e-04,  1.7159e-03,  3.4869e-05,\n",
       "         -1.3199e-03, -2.8737e-03, -1.2404e-03, -3.7638e-03, -1.5092e-04,\n",
       "         -5.9605e-07, -3.8290e-04, -1.8427e-03, -1.6747e-03,  7.7516e-04,\n",
       "         -6.3747e-04, -3.9434e-04, -1.8345e-03, -1.2665e-03, -5.7292e-04,\n",
       "          6.7389e-04,  5.8568e-04, -7.6437e-04, -1.4509e-03, -1.1145e-03,\n",
       "         -3.3394e-03,  1.2273e-04, -9.3186e-04, -1.4657e-03,  1.7470e-04,\n",
       "          3.9223e-03,  1.2153e-03, -2.2436e-03, -1.2867e-03, -3.4857e-04,\n",
       "         -2.0549e-03,  4.7666e-04, -5.2506e-04,  3.8469e-04, -1.3714e-03,\n",
       "          1.9646e-04, -1.2577e-03, -1.5798e-03, -1.4907e-03, -5.2381e-04,\n",
       "         -2.9583e-03, -9.4217e-04, -1.5843e-04,  1.2068e-03, -8.9049e-04,\n",
       "         -1.8370e-04, -4.9728e-04, -1.5570e-03,  2.7066e-03, -1.1617e-03,\n",
       "         -4.0101e-03, -2.2487e-03,  1.6325e-03,  7.7796e-04, -1.3307e-03,\n",
       "          8.4591e-04, -4.2589e-03,  5.9927e-04, -4.3737e-03, -2.1525e-03,\n",
       "          4.0472e-05, -2.8064e-03,  9.3973e-04, -2.4664e-04, -9.0772e-04,\n",
       "         -8.0514e-04,  5.1188e-04,  2.7680e-03, -2.0472e-03, -8.3470e-04,\n",
       "         -2.3305e-04, -1.2019e-03, -2.9966e-03, -7.1239e-04,  1.8203e-04,\n",
       "          3.0833e-04, -2.2820e-03, -1.6136e-03, -7.0298e-04, -5.1951e-03,\n",
       "          1.8550e-03,  8.2290e-04,  8.7333e-04,  1.9324e-03, -2.9942e-03,\n",
       "         -2.8056e-04, -8.4460e-04,  1.1181e-03, -5.8922e-03, -1.1796e-03,\n",
       "          1.8058e-03, -8.6570e-04, -3.1819e-03, -7.4297e-04, -2.2984e-03,\n",
       "         -2.4635e-03, -1.2672e-03,  8.2761e-04, -9.6130e-04,  1.5645e-03,\n",
       "         -1.9991e-03, -4.9776e-04,  1.8334e-03, -2.2306e-03, -2.6919e-03,\n",
       "         -6.8915e-04,  5.9879e-04,  1.0733e-03,  2.4296e-03, -1.3149e-04,\n",
       "         -3.2113e-03,  5.2643e-04, -1.4858e-03, -1.8662e-03, -3.0828e-04,\n",
       "         -6.0087e-04,  3.1495e-04,  3.3054e-03,  5.5599e-04, -1.8477e-03,\n",
       "         -5.7074e-03,  2.2185e-04, -3.9297e-04,  3.1520e-03, -1.4780e-03,\n",
       "          1.2858e-03, -1.2017e-03,  2.0217e-03,  1.5516e-03, -1.7508e-03,\n",
       "          3.0031e-03,  3.3308e-03,  8.9240e-04,  1.3626e-04, -4.9442e-04,\n",
       "         -3.1984e-04, -6.4307e-04, -3.7557e-04, -2.1889e-03, -1.6574e-03,\n",
       "         -7.8726e-04,  1.1419e-03,  1.1650e-03, -2.2864e-03,  8.6164e-04,\n",
       "         -3.3485e-03,  2.6369e-03,  1.7822e-05,  6.7890e-05, -1.1098e-04,\n",
       "         -5.5027e-04,  5.4055e-04, -9.2959e-04, -2.0900e-03,  2.7597e-04,\n",
       "          1.0563e-03, -2.2376e-04, -5.6124e-04,  4.8995e-04,  3.2771e-04,\n",
       "         -1.1564e-03, -2.5333e-03, -5.2285e-04, -2.3207e-03,  9.6023e-04,\n",
       "         -1.8505e-03, -3.1796e-03, -2.4447e-03,  1.2800e-03, -8.9180e-04,\n",
       "          3.1697e-03, -3.4910e-03, -3.1422e-03, -6.3634e-04,  5.2333e-05,\n",
       "         -3.2017e-03,  9.5105e-04,  2.0124e-03, -4.4310e-04,  2.5761e-04,\n",
       "         -3.7913e-03,  1.1464e-03, -9.9969e-04, -1.8892e-03,  4.4990e-04,\n",
       "          8.9204e-04, -1.8233e-03, -1.6803e-04, -1.8848e-03, -8.0276e-04,\n",
       "         -4.1159e-03, -2.3476e-03,  2.8163e-04,  5.2512e-05,  1.6773e-04,\n",
       "          5.3102e-04, -1.3363e-03, -1.4096e-03, -3.3507e-03,  9.6101e-04,\n",
       "         -2.1302e-03, -2.0186e-03, -1.7331e-03, -1.3021e-03, -4.0209e-04,\n",
       "         -1.2472e-03, -5.0151e-04,  3.5238e-04, -4.7195e-04, -2.5979e-03,\n",
       "          1.2264e-03, -2.5830e-03,  5.8830e-04, -1.4293e-04, -7.1955e-04,\n",
       "         -2.0803e-03, -1.1564e-03, -2.0627e-03,  7.8094e-04, -1.3837e-03,\n",
       "         -1.2294e-03,  2.2137e-04,  1.6112e-03, -2.6059e-04,  1.8017e-03,\n",
       "         -1.6817e-03, -3.0403e-03,  1.2515e-03,  3.7265e-04, -8.7643e-04,\n",
       "          1.2338e-04,  1.0868e-03, -6.8051e-04, -2.6430e-03,  2.0742e-05,\n",
       "          5.9414e-04,  1.2642e-04,  4.7278e-04, -7.8410e-04,  1.3520e-03,\n",
       "         -1.3896e-03,  7.4530e-04,  2.9213e-03,  1.8724e-03,  1.0091e-03,\n",
       "          4.4051e-03, -1.0979e-03, -5.1677e-05, -1.6262e-03,  1.4300e-03,\n",
       "          7.9405e-04,  1.0279e-03,  3.6782e-04, -3.5262e-04, -2.1498e-03,\n",
       "         -1.3974e-03, -4.4048e-04,  1.4178e-03, -3.4254e-03,  7.1615e-04,\n",
       "          3.4848e-03, -2.5532e-03, -2.2225e-03, -1.2218e-03,  2.1631e-04,\n",
       "         -1.9107e-03,  1.9850e-03, -3.9430e-03,  1.3541e-03,  1.2577e-04,\n",
       "          6.3795e-04, -1.7119e-03,  2.0304e-03, -1.5611e-03, -1.7438e-03,\n",
       "         -3.0988e-04, -6.8069e-05, -1.2466e-03, -1.1790e-04, -2.8412e-03,\n",
       "         -7.8356e-04,  8.7994e-04, -3.1289e-03, -4.0281e-04, -9.1559e-04,\n",
       "          2.8351e-03, -4.8411e-04, -1.0799e-03,  1.4360e-03, -1.2333e-03,\n",
       "         -1.5486e-03, -3.8992e-03,  2.0176e-03, -8.4543e-04, -1.7104e-03,\n",
       "         -1.4744e-03,  1.6952e-03, -1.1302e-03, -3.9697e-04, -1.0995e-03,\n",
       "         -1.1285e-03,  2.6476e-04, -2.4518e-03, -4.6372e-05, -9.0146e-04,\n",
       "         -7.2694e-04,  5.5009e-04, -4.3184e-04, -1.0142e-03, -4.5538e-05,\n",
       "         -1.1206e-05, -2.2317e-03, -3.9216e-03, -1.2612e-04, -2.6565e-03,\n",
       "          7.9513e-04, -2.2674e-04, -2.0372e-03, -1.8495e-03,  2.5092e-03,\n",
       "          4.4584e-05, -1.8603e-04]),\n",
       " tensor([ 3.4873e-04,  3.3353e-04,  7.5912e-04,  6.3390e-04, -1.7958e-04,\n",
       "         -1.2647e-03, -2.2089e-04,  1.2455e-03,  1.9418e-04,  6.9726e-04,\n",
       "         -1.5672e-03, -4.3998e-04,  6.7647e-04,  8.2615e-04, -1.4559e-04,\n",
       "         -3.6389e-04,  7.8960e-04, -6.7769e-04,  1.4357e-05,  1.7759e-04,\n",
       "          9.6660e-04,  8.6984e-04,  7.5303e-04, -1.5549e-03, -7.8642e-04,\n",
       "         -8.1184e-04, -1.0729e-03,  1.4174e-03,  2.9928e-04, -3.3435e-04,\n",
       "          6.3740e-04, -4.3192e-04,  4.1762e-04, -7.5195e-04,  5.6789e-04,\n",
       "          1.4219e-03,  8.7162e-04,  5.2965e-04, -4.7551e-04, -8.4568e-05,\n",
       "          5.3194e-04, -1.1353e-03, -1.0446e-05, -1.3197e-04, -2.1901e-04,\n",
       "         -3.3678e-04,  1.8963e-03, -6.8019e-04,  1.8792e-04,  5.4581e-04,\n",
       "          3.6056e-04, -1.6389e-03,  9.3596e-04, -1.5049e-03,  9.1961e-04,\n",
       "          2.0345e-04,  2.8124e-03, -3.6400e-04, -1.2554e-03,  5.9377e-04,\n",
       "          1.0476e-03,  6.5351e-04,  2.8200e-04, -5.1087e-04, -1.0646e-03,\n",
       "         -7.4408e-04,  3.3960e-04, -2.4649e-04, -1.0333e-03, -1.2494e-03,\n",
       "         -4.3433e-04,  1.0121e-03, -7.8777e-04,  1.2461e-03, -6.2353e-04,\n",
       "         -4.1465e-04,  4.9864e-04, -2.3176e-04, -5.2272e-04, -1.8726e-04,\n",
       "          1.0559e-03,  3.0111e-04, -1.8744e-04,  4.0488e-04, -6.8155e-04,\n",
       "          8.5227e-04, -1.8575e-03,  1.5543e-03,  6.0067e-04,  1.4999e-03,\n",
       "         -5.1735e-04, -2.1340e-04,  1.1925e-03,  2.7684e-04,  5.3031e-04,\n",
       "         -1.6681e-03,  9.0494e-04,  2.3838e-04,  5.7845e-04,  1.3874e-03,\n",
       "         -1.2293e-03,  8.5150e-04,  1.1778e-03, -6.3283e-04, -5.1670e-04,\n",
       "         -5.9423e-04, -1.0074e-03, -2.0506e-04, -3.5999e-04, -7.3483e-04,\n",
       "         -1.6550e-04, -8.0763e-04,  4.2899e-05, -1.2492e-03, -5.0255e-04,\n",
       "         -8.7796e-04,  4.9765e-04,  2.9361e-04,  1.4219e-03, -6.2708e-05,\n",
       "         -1.1637e-03, -2.2576e-03, -1.2767e-04, -1.4387e-03,  1.2858e-03,\n",
       "          5.1230e-05, -7.0064e-04,  9.7653e-04, -1.7867e-03,  2.6881e-04,\n",
       "          7.4337e-04, -1.4212e-04,  8.6438e-04,  6.1236e-04,  4.6382e-04,\n",
       "         -8.4168e-04,  4.3194e-04, -9.8016e-04, -7.8412e-04, -1.1650e-03,\n",
       "         -3.9267e-04, -7.5547e-04, -2.4363e-03,  8.7754e-04,  1.0667e-03,\n",
       "         -2.0735e-03, -7.9103e-05,  3.6856e-04, -2.5675e-04, -4.0447e-04,\n",
       "          9.0564e-04, -9.4161e-04,  1.2849e-03, -1.0943e-04, -3.3153e-04,\n",
       "          1.6982e-03,  1.9288e-04,  1.5460e-03,  1.5855e-03, -1.5163e-03,\n",
       "          3.7759e-04,  5.0026e-04,  3.4346e-04, -5.4118e-04,  1.6914e-04,\n",
       "         -1.0669e-03, -9.0577e-04, -4.0969e-04, -7.6536e-06,  9.9159e-04,\n",
       "          3.4687e-04,  1.2464e-04,  6.1580e-04, -7.1533e-04, -1.1667e-03,\n",
       "          1.9992e-04, -1.1524e-03, -2.5582e-05, -9.6609e-04, -4.2222e-04,\n",
       "          6.8498e-04,  6.9626e-06, -3.0920e-04, -5.4918e-04,  1.8022e-03,\n",
       "          1.7931e-03, -3.3283e-04, -6.0790e-04, -1.4012e-04, -1.1711e-03,\n",
       "         -1.0276e-03,  6.9808e-05, -9.3125e-05, -1.0056e-03,  1.1242e-03,\n",
       "          5.6625e-04,  4.0395e-04,  3.5910e-04,  1.4651e-03,  7.4296e-04,\n",
       "          1.3216e-03,  6.1184e-04, -4.6015e-04,  1.1160e-03,  3.8350e-04,\n",
       "          9.2443e-05,  5.7861e-04, -2.8148e-04, -3.7581e-04,  5.3316e-04,\n",
       "         -1.4333e-03,  1.2800e-04, -2.7794e-04,  1.1592e-04,  1.7573e-03,\n",
       "         -2.4299e-04, -7.1952e-04, -1.3538e-04,  5.4035e-04, -4.8366e-04,\n",
       "         -4.1671e-05, -7.9788e-04, -2.4696e-04,  8.7313e-04,  9.9718e-04,\n",
       "          1.1004e-03,  6.7501e-04, -1.9854e-04,  6.3094e-04, -1.4162e-04,\n",
       "         -1.4381e-04,  4.2880e-04,  9.1862e-04, -1.3761e-03,  6.9998e-04,\n",
       "         -5.1148e-04,  2.7010e-04,  3.6488e-04, -1.6903e-03, -4.4979e-04,\n",
       "          2.4648e-03, -2.4173e-04,  3.9112e-04,  6.8698e-05,  1.7374e-03,\n",
       "          4.2231e-04, -9.4162e-04, -1.0917e-03, -8.6215e-04,  1.5995e-03,\n",
       "          7.8911e-04,  5.8693e-04, -1.0034e-03, -1.0324e-03,  1.4417e-05,\n",
       "         -3.4982e-04,  7.4828e-04, -1.0548e-03,  6.4611e-05,  7.0699e-04,\n",
       "         -1.1576e-04,  6.7347e-04,  5.4261e-04,  1.6794e-04,  8.3939e-04,\n",
       "         -3.7537e-04, -6.0886e-04,  2.7547e-04,  8.6841e-04,  1.0039e-03,\n",
       "         -5.0082e-04,  2.9857e-04, -9.6472e-04,  2.1656e-04,  6.8069e-04,\n",
       "          1.3504e-03, -8.7417e-04, -4.1656e-04, -3.8750e-04,  5.3632e-04,\n",
       "          1.0800e-03, -1.2250e-03,  2.3328e-03, -9.4328e-04,  3.6783e-04,\n",
       "         -5.9607e-04, -3.4300e-04,  7.8908e-04,  8.8960e-06,  9.4511e-04,\n",
       "         -9.7262e-04,  8.6625e-04, -5.4295e-04,  2.8937e-04, -2.0843e-05,\n",
       "         -8.8490e-04,  3.3330e-05, -6.3863e-04,  1.0489e-03, -4.9406e-04,\n",
       "         -9.9775e-04,  1.3238e-03, -1.4728e-04, -2.6559e-04, -1.0336e-03,\n",
       "          1.2985e-03, -5.1612e-04, -3.5357e-04,  2.8749e-04,  1.8131e-04,\n",
       "          8.2216e-04,  2.1526e-03, -6.5012e-04, -2.0989e-03,  9.9576e-04,\n",
       "         -2.8828e-04, -5.0981e-04,  2.0434e-03, -6.7920e-04, -3.8825e-04,\n",
       "         -1.7428e-04,  9.1361e-04,  1.9715e-03, -3.2216e-04, -1.2957e-03,\n",
       "         -1.4448e-04,  4.2954e-05, -1.0125e-04,  8.8712e-04,  6.7180e-04,\n",
       "         -1.1090e-03,  1.6058e-03, -4.4583e-04,  7.4981e-04,  4.6737e-04,\n",
       "         -9.3063e-05, -3.2184e-04,  1.2760e-03,  1.0734e-03, -1.9758e-04,\n",
       "         -4.6474e-04,  1.3971e-03, -2.1141e-03, -5.6751e-04, -1.0922e-03,\n",
       "          3.5988e-04, -4.9626e-05, -3.0685e-04,  3.9333e-04, -4.8630e-04,\n",
       "          2.3450e-04,  9.3099e-05,  1.5595e-03,  5.9847e-04, -1.3839e-04,\n",
       "          7.8082e-06, -5.3205e-04,  1.1401e-03, -3.1496e-04, -2.8704e-04,\n",
       "         -8.3486e-04,  1.1209e-03, -1.1750e-03, -8.4920e-05,  3.2808e-04,\n",
       "         -5.9581e-04,  5.6391e-04, -1.1588e-03,  3.6759e-04,  2.1323e-03,\n",
       "          1.0973e-03, -3.9556e-04,  2.1800e-04, -3.3040e-04,  1.1124e-03,\n",
       "         -9.6539e-04, -1.8441e-03, -9.8423e-04,  5.8629e-04, -2.4196e-03,\n",
       "          8.1022e-04,  1.2674e-04,  4.9189e-04, -2.9393e-05, -7.6504e-04,\n",
       "          2.1559e-04, -6.2410e-05, -7.3289e-04, -1.2418e-03, -1.4432e-03,\n",
       "          8.0246e-05,  1.6823e-03, -4.2075e-04,  4.7325e-04, -2.1109e-04,\n",
       "          5.1252e-04, -7.6418e-04,  1.8588e-04,  1.2308e-03, -2.5444e-04,\n",
       "          1.4430e-03,  3.3113e-04, -6.1047e-04, -4.0678e-04,  1.3001e-03,\n",
       "         -6.8629e-04, -4.6096e-04,  4.4006e-04, -1.8773e-03,  1.0407e-03,\n",
       "          5.5216e-05,  5.2728e-04,  5.8097e-04,  1.6420e-04,  1.8976e-04,\n",
       "         -1.3827e-03, -5.9851e-04,  1.2938e-03,  1.4559e-04,  1.9709e-04,\n",
       "          4.4972e-04,  2.9270e-05, -2.4430e-05, -9.2428e-04,  4.9799e-04,\n",
       "          1.0207e-03, -4.9824e-04,  6.8460e-05,  6.2807e-04, -8.5353e-04,\n",
       "          1.9436e-04, -2.3946e-05,  1.5443e-03,  1.3822e-03, -5.3808e-05,\n",
       "         -5.5449e-04, -1.0267e-04, -4.8995e-05,  1.4807e-03,  1.5478e-03,\n",
       "         -1.0019e-03, -2.1685e-03, -1.1503e-03, -2.2929e-04,  1.1922e-03,\n",
       "          6.6396e-04, -1.4483e-03, -7.0397e-04, -1.0835e-03,  6.6442e-04,\n",
       "         -3.3690e-03,  8.3088e-04,  1.6856e-04,  7.3109e-04, -1.7983e-03,\n",
       "         -4.6852e-04, -7.6639e-04, -3.6075e-04,  3.1351e-04, -1.0910e-03,\n",
       "          6.4467e-04, -7.8775e-04,  7.0053e-04,  4.6201e-04,  1.8641e-04,\n",
       "         -1.8574e-04, -1.2886e-03,  8.6516e-04, -1.9847e-03, -7.2091e-04,\n",
       "         -5.7147e-04,  9.9234e-05, -8.0506e-04, -3.4906e-06, -5.2255e-05,\n",
       "          1.0740e-05,  9.8301e-04,  1.0406e-03,  5.6066e-04,  2.6226e-04,\n",
       "         -2.0151e-03,  7.5076e-04, -1.5106e-03, -7.6080e-04,  1.7593e-04,\n",
       "         -4.8669e-04, -2.1712e-03, -6.1655e-05, -1.1557e-03,  1.8138e-04,\n",
       "          6.2440e-04,  7.9445e-04,  1.2013e-03, -1.3019e-03,  4.9479e-04,\n",
       "         -1.9946e-03, -1.2973e-04, -1.3973e-03, -5.9811e-04, -1.2422e-04,\n",
       "         -4.5632e-04,  8.7558e-04,  9.4206e-04,  5.6691e-04,  9.0133e-04,\n",
       "          4.5388e-04, -1.2341e-03,  3.0581e-04,  8.2950e-04, -5.5338e-04,\n",
       "         -2.8362e-04,  8.2424e-04]),\n",
       " tensor([[ 4.0477e-04, -2.5295e-03,  6.7619e-03,  ..., -9.1969e-04,\n",
       "          -1.4983e-03, -7.3601e-04],\n",
       "         [ 3.7890e-04, -9.2853e-05,  2.7635e-03,  ..., -4.0562e-04,\n",
       "           3.5259e-03, -1.5370e-03],\n",
       "         [ 5.1772e-03, -6.7754e-04, -5.5901e-04,  ...,  4.0819e-03,\n",
       "          -2.3902e-03,  8.2457e-04],\n",
       "         ...,\n",
       "         [ 2.5452e-03,  4.9308e-03, -2.7019e-03,  ..., -1.2706e-03,\n",
       "          -1.3912e-03,  8.8314e-04],\n",
       "         [ 2.9761e-03,  2.5472e-03,  2.2172e-03,  ...,  2.5787e-03,\n",
       "          -3.5869e-03, -1.4714e-03],\n",
       "         [-3.0883e-03, -1.5700e-03, -9.0703e-04,  ..., -3.4368e-03,\n",
       "          -2.6027e-03, -1.6458e-03]]),\n",
       " tensor([-0.0013,  0.0003,  0.0023,  ...,  0.0018, -0.0010,  0.0010]),\n",
       " tensor([[-1.3893e-03,  2.1503e-03, -6.3930e-04,  ..., -1.4309e-03,\n",
       "          -7.9958e-04,  2.2324e-03],\n",
       "         [-5.1189e-03, -8.3378e-04,  8.7478e-04,  ...,  5.8411e-04,\n",
       "           1.8378e-03, -1.0745e-03],\n",
       "         [ 2.3939e-03, -7.8837e-04,  4.0534e-03,  ..., -2.2120e-03,\n",
       "           2.0544e-03,  3.0748e-04],\n",
       "         ...,\n",
       "         [ 3.8390e-03,  8.4026e-05,  6.7154e-04,  ..., -4.7610e-03,\n",
       "           1.1007e-04, -3.1869e-03],\n",
       "         [-3.1481e-03,  1.2686e-03,  8.6362e-04,  ..., -8.0771e-04,\n",
       "          -6.1601e-04,  9.0026e-04],\n",
       "         [-2.7282e-04,  1.0123e-03,  5.0063e-04,  ...,  1.8587e-03,\n",
       "          -4.5947e-04,  8.7609e-04]]),\n",
       " tensor([-1.6548e-04, -1.1469e-04, -4.1737e-04,  3.5296e-04,  2.6529e-04,\n",
       "          8.1004e-04,  2.5388e-04, -4.8501e-04, -1.5397e-04, -1.7237e-04,\n",
       "          8.5627e-04,  9.0996e-05,  5.5921e-04, -6.5698e-04,  6.8883e-04,\n",
       "         -9.8497e-04, -2.2370e-04,  8.2571e-04,  1.6848e-04, -8.2362e-04,\n",
       "         -5.3921e-04, -4.9648e-04, -4.7289e-04,  1.8206e-03,  2.5208e-04,\n",
       "         -8.5061e-05,  4.2894e-05, -4.2325e-04,  1.7357e-04,  3.8697e-04,\n",
       "         -3.9961e-04,  4.3698e-04,  1.4439e-04, -3.2481e-04, -4.7421e-04,\n",
       "         -8.3857e-04,  6.0130e-05, -7.6826e-04, -3.2835e-04, -5.5142e-05,\n",
       "          4.6823e-04,  8.4921e-04, -9.5490e-05, -4.8042e-04, -4.8390e-04,\n",
       "          4.1175e-04,  1.2712e-04,  2.7145e-04,  1.1439e-04,  4.1814e-04,\n",
       "         -5.8836e-04,  5.3337e-04, -1.1815e-04,  4.6205e-05, -9.6826e-04,\n",
       "          3.1168e-04, -1.5169e-03, -1.9431e-04,  6.6614e-04, -2.8396e-04,\n",
       "         -2.3119e-04, -1.3508e-03,  2.2054e-04, -1.0186e-04,  6.5434e-04,\n",
       "         -3.4278e-04,  5.9977e-05,  3.8352e-04,  6.6471e-04,  6.1604e-04,\n",
       "          6.4735e-04, -6.2048e-04,  7.9755e-04, -1.2951e-03,  8.0069e-04,\n",
       "         -7.0672e-05, -6.5417e-05,  6.0553e-04,  7.3161e-04, -5.1731e-05,\n",
       "         -5.2931e-04, -5.8794e-04,  3.8806e-04, -1.4881e-03, -7.4129e-05,\n",
       "          2.2039e-05,  5.3650e-04, -8.5671e-05,  3.7864e-04, -3.3841e-04,\n",
       "          3.3332e-04,  1.1168e-04, -3.8516e-04, -6.0964e-04, -3.8244e-05,\n",
       "         -1.6555e-04, -1.1639e-03, -1.6429e-04, -5.1135e-04, -3.8525e-04,\n",
       "          8.8309e-04, -5.1421e-04, -1.2555e-03, -1.9424e-04, -4.2327e-04,\n",
       "          3.5385e-04,  4.5185e-04,  1.1088e-03,  1.3213e-04, -4.7059e-04,\n",
       "          2.4700e-04,  6.3906e-04,  8.9573e-05,  1.9742e-05, -3.2838e-04,\n",
       "          1.0967e-04, -1.0244e-04,  2.4803e-04, -8.1609e-04, -5.0304e-04,\n",
       "          6.6031e-04,  1.0118e-03, -4.4003e-04, -2.4884e-04, -1.5314e-03,\n",
       "         -6.5902e-04, -7.7411e-05, -3.8611e-05,  7.5540e-04,  1.2580e-04,\n",
       "         -6.0731e-04, -7.1369e-04, -6.9745e-04, -1.5124e-04, -2.4944e-04,\n",
       "          6.5152e-04,  7.2437e-04,  7.9859e-05,  1.0813e-04,  1.3180e-04,\n",
       "         -5.7145e-04,  1.6445e-04,  8.5141e-04, -6.5467e-04, -6.7332e-04,\n",
       "          6.5149e-04, -1.7776e-04,  1.1470e-05,  2.9677e-04,  3.4649e-04,\n",
       "          1.0774e-04,  2.6590e-04, -6.3898e-04, -5.8586e-04,  1.6576e-04,\n",
       "         -1.1124e-03,  1.8835e-04, -2.1907e-04, -3.4017e-04,  1.0017e-03,\n",
       "         -2.1872e-04, -9.4349e-04,  1.1077e-04,  6.3756e-04, -9.2583e-04,\n",
       "          8.7997e-04,  8.8583e-04,  7.9164e-04,  2.0515e-04, -2.4959e-06,\n",
       "         -6.2600e-04,  5.3273e-04,  5.0710e-04,  5.5061e-04,  5.8172e-04,\n",
       "         -1.3236e-04,  3.3995e-04,  1.2866e-04,  4.2684e-05, -1.9481e-04,\n",
       "          2.8674e-04,  5.8733e-04,  5.3053e-04,  1.3056e-03, -2.2166e-03,\n",
       "         -4.5190e-04,  1.2923e-04,  4.9069e-04, -1.7004e-05,  2.1414e-04,\n",
       "          5.8784e-04, -2.1969e-04, -3.5191e-05,  6.7765e-04,  1.4843e-04,\n",
       "         -3.9818e-04,  8.2944e-04, -1.2883e-04, -7.7254e-04,  5.2592e-05,\n",
       "         -9.4345e-04,  4.0107e-04,  1.3029e-03, -7.6494e-04,  1.1974e-04,\n",
       "         -7.1918e-04, -5.1814e-04, -8.3653e-05, -3.6528e-04,  2.0410e-04,\n",
       "          7.4134e-04, -2.5727e-04,  6.8409e-04, -8.4702e-05, -7.3184e-04,\n",
       "         -2.7917e-04, -6.2941e-04, -7.8464e-04, -2.5743e-04,  7.0071e-04,\n",
       "          2.0059e-05,  6.1394e-04, -1.7517e-04,  1.8143e-04, -5.2748e-04,\n",
       "         -8.0768e-04, -1.1978e-04, -1.1997e-03, -6.0597e-04, -1.8728e-04,\n",
       "         -1.2870e-04, -1.2048e-04,  3.6247e-05,  1.0852e-03,  2.4991e-04,\n",
       "          1.6492e-04, -7.7128e-05, -3.7513e-04,  7.1006e-04,  3.2030e-04,\n",
       "         -1.1989e-03,  1.7806e-03,  3.3252e-04,  6.3675e-05, -1.2900e-03,\n",
       "          2.7723e-04, -5.9035e-04,  8.3485e-04,  7.4634e-05, -6.4374e-04,\n",
       "         -3.6369e-04, -1.0554e-04,  8.7106e-04,  1.7831e-04,  4.9693e-04,\n",
       "         -9.8295e-05,  2.5922e-04,  3.1702e-04, -7.6232e-05, -5.2721e-04,\n",
       "          2.7874e-04, -7.6479e-04, -3.9562e-04, -2.2507e-05, -3.0414e-04,\n",
       "         -2.2401e-04, -8.5187e-04, -6.4401e-04, -4.4634e-04, -2.3088e-04,\n",
       "         -6.6274e-04,  3.0487e-04,  4.5568e-04, -9.6313e-04,  4.1371e-04,\n",
       "         -9.0537e-04,  9.5208e-04,  5.2579e-04,  4.2691e-05, -5.3011e-04,\n",
       "         -7.0707e-04, -7.7154e-05, -6.1622e-04,  1.8014e-03,  4.8728e-04,\n",
       "          5.2957e-04,  1.0166e-03, -7.0738e-04,  5.1284e-05, -2.9046e-04,\n",
       "          1.2011e-04,  1.6392e-04,  8.9461e-05,  3.0374e-04, -1.5054e-05,\n",
       "          2.7267e-04,  2.2458e-04,  1.5725e-03, -1.3056e-03, -1.1402e-04,\n",
       "          3.9292e-04, -7.9586e-04, -1.0559e-03,  3.0731e-04, -3.3001e-04,\n",
       "          5.0315e-05, -8.3074e-06, -8.2878e-04,  5.6728e-04,  1.5918e-05,\n",
       "          6.0701e-06, -1.3612e-03,  4.0514e-04,  3.4492e-04,  5.6354e-06,\n",
       "          2.7324e-05,  5.3413e-04, -2.0473e-04,  5.3427e-05,  5.2531e-04,\n",
       "          9.9314e-05, -5.6552e-04, -1.0634e-03, -4.2893e-04,  4.4169e-04,\n",
       "         -3.2868e-04, -7.9673e-04,  3.8874e-04, -1.0153e-03,  1.9382e-04,\n",
       "          1.8371e-04, -1.4088e-04,  4.9965e-04, -4.1375e-04,  6.5751e-05,\n",
       "         -7.0789e-04, -2.2542e-04,  5.0320e-04, -4.8867e-04,  2.0177e-04,\n",
       "         -2.4531e-04, -3.4107e-04,  1.2106e-03,  2.7117e-04,  4.9114e-04,\n",
       "         -5.4441e-05,  9.4416e-04,  2.5371e-04, -1.9932e-04,  1.7479e-04,\n",
       "          4.1943e-04,  1.5281e-04, -7.0332e-04, -1.9137e-04, -2.4525e-04,\n",
       "          5.8097e-04,  2.6401e-04,  1.5355e-04,  1.2488e-04,  5.4509e-04,\n",
       "          1.0340e-04, -7.4269e-05,  8.3484e-05,  8.7550e-05, -8.5725e-04,\n",
       "          3.8933e-04, -6.4307e-05,  7.4861e-04, -7.4093e-05, -9.1802e-04,\n",
       "         -8.0870e-05,  5.1894e-04, -1.7500e-04, -1.1978e-04, -2.8429e-04,\n",
       "          5.3933e-04,  5.0615e-04,  3.4804e-04, -1.0842e-04,  1.1029e-03,\n",
       "         -3.3073e-04,  1.2981e-04,  3.0589e-04, -8.8330e-04, -2.0755e-04,\n",
       "          1.0611e-04, -2.0440e-04, -9.3651e-05, -1.3194e-04,  8.4944e-04,\n",
       "         -1.0525e-03,  4.6715e-06,  2.8964e-04, -1.5930e-04,  1.1222e-05,\n",
       "         -4.6328e-04,  4.0922e-04, -1.6053e-04, -2.3987e-04,  5.4224e-04,\n",
       "         -5.2143e-04, -4.8181e-04,  5.2286e-04,  5.4305e-04,  2.3609e-04,\n",
       "         -2.3293e-04,  3.8179e-04,  4.2762e-05,  8.4672e-05, -6.0210e-04,\n",
       "         -8.1779e-05,  4.0180e-04, -9.7203e-04, -2.1860e-04,  3.5036e-04,\n",
       "         -1.1858e-04,  1.3579e-04, -9.4275e-04, -7.0085e-04, -1.8761e-04,\n",
       "         -1.3687e-04,  5.5496e-04,  5.7501e-04,  4.8417e-04, -5.7340e-05,\n",
       "         -1.0805e-04, -2.4587e-05, -1.8502e-04, -5.4058e-04,  7.7175e-04,\n",
       "         -1.6388e-04, -6.6031e-06, -9.0380e-04, -1.4251e-04, -1.0115e-04,\n",
       "          3.5389e-04,  2.6492e-04, -1.0356e-03, -2.2149e-04, -5.7008e-04,\n",
       "          1.4889e-03,  8.1088e-04,  5.4197e-04, -4.8913e-04, -8.6846e-04,\n",
       "         -2.7163e-04, -1.9269e-04,  6.9569e-04,  6.7216e-04, -9.9777e-04,\n",
       "          4.4726e-03, -3.6833e-04,  1.0471e-04,  4.7274e-04,  5.2436e-04,\n",
       "          6.0369e-04,  4.1176e-04, -3.3723e-04, -4.1152e-04, -9.2335e-05,\n",
       "         -3.9912e-04, -2.2352e-04, -1.7436e-04, -5.4020e-04, -2.9243e-04,\n",
       "          2.8503e-04,  7.2817e-04, -9.5421e-04,  5.4846e-04, -8.9586e-05,\n",
       "         -1.8070e-04,  9.4042e-05,  5.4154e-04, -3.5848e-05, -3.2403e-04,\n",
       "          5.7460e-04,  3.3583e-05,  1.5308e-04, -3.9914e-04,  3.1069e-06,\n",
       "          1.2500e-03, -9.8414e-04,  7.1181e-04,  5.2630e-05, -2.6990e-04,\n",
       "          1.3385e-04,  5.1950e-04,  7.5836e-05,  2.5553e-04,  2.2088e-04,\n",
       "         -4.5417e-04, -7.2402e-04, -2.4808e-04,  1.8868e-04,  1.4730e-04,\n",
       "          7.0820e-04, -1.6239e-04,  1.1845e-03,  1.9122e-04,  5.5907e-05,\n",
       "         -5.4379e-04, -5.0113e-04, -6.4772e-04, -5.5074e-04, -2.9422e-04,\n",
       "          2.8730e-04,  6.1175e-04, -2.3278e-04,  2.3010e-04,  6.9439e-04,\n",
       "          6.1216e-04, -4.4066e-04]),\n",
       " tensor([ 2.1846e-03, -8.2374e-05,  1.2648e-04, -2.8074e-04, -3.2432e-03,\n",
       "         -3.3760e-04,  1.9491e-03,  2.2861e-03, -3.7173e-03, -1.8053e-03,\n",
       "         -2.0666e-03, -1.6229e-03,  1.1765e-03, -5.1336e-03, -8.2171e-04,\n",
       "          8.8882e-04,  1.0077e-03, -3.1600e-03, -3.7122e-04, -4.6874e-03,\n",
       "          5.4073e-04, -5.2214e-03,  1.4784e-03,  1.9588e-03, -7.7456e-03,\n",
       "         -1.6078e-03, -4.8769e-04, -2.8651e-03, -1.9968e-04, -7.9882e-04,\n",
       "         -4.5012e-03,  2.4498e-04, -4.9102e-04, -1.4037e-03,  1.0036e-03,\n",
       "         -3.6597e-05,  1.0353e-03,  3.8612e-04,  1.6426e-03,  2.0009e-03,\n",
       "         -2.2969e-03, -7.6985e-04, -1.6671e-03, -1.9176e-03,  7.1931e-04,\n",
       "          1.9681e-04,  3.1245e-04, -1.3926e-03, -8.8453e-05, -1.1677e-03,\n",
       "          7.9775e-04,  6.5362e-04,  1.6422e-03, -1.0408e-03, -4.7112e-04,\n",
       "          2.6357e-04, -3.2818e-03, -1.4172e-03, -1.5090e-03, -3.3700e-04,\n",
       "         -2.0455e-03,  3.5715e-03, -3.3224e-04,  1.1311e-03, -1.2821e-03,\n",
       "         -4.4751e-04,  5.0628e-04, -1.1152e-03,  1.9857e-03,  2.5754e-03,\n",
       "          7.5531e-04,  2.3650e-03,  4.7302e-04, -9.7036e-04,  2.5847e-03,\n",
       "         -4.7988e-03, -1.4286e-03, -1.5640e-04, -1.3967e-03,  1.5032e-04,\n",
       "         -2.7646e-03,  2.3842e-06,  2.5785e-04, -4.1604e-05,  2.2147e-03,\n",
       "         -7.3206e-04, -5.4836e-06, -2.2256e-04, -1.9411e-03,  1.2846e-03,\n",
       "          2.0623e-03,  8.9920e-04,  2.0529e-03,  9.2542e-04,  1.7846e-03,\n",
       "          1.1070e-03,  1.2532e-03,  1.8055e-03, -1.7439e-03,  9.8705e-05,\n",
       "          1.7346e-03,  8.6260e-04,  1.3328e-03, -9.1994e-04, -5.5993e-04,\n",
       "         -1.7111e-03, -6.3241e-04,  1.2363e-03, -3.8993e-04, -1.0123e-03,\n",
       "          2.7657e-04, -1.6845e-03,  6.2478e-04, -2.0409e-04, -8.0316e-03,\n",
       "         -2.6242e-03,  6.3515e-04, -6.6996e-05,  4.3714e-04,  6.0904e-04,\n",
       "          1.1694e-04, -2.4786e-03,  1.6963e-04,  7.5901e-04, -5.0962e-03,\n",
       "         -1.9979e-04,  1.4482e-03, -2.1768e-04, -4.0400e-04,  3.6812e-04,\n",
       "          1.2245e-03, -1.7674e-03,  1.1917e-03, -1.6556e-03,  9.9206e-04,\n",
       "         -2.2756e-03, -2.2492e-03,  1.5430e-03,  9.0873e-04, -1.3260e-03,\n",
       "          1.0566e-03,  2.5320e-04, -8.5306e-04, -2.0000e-03,  6.7949e-05,\n",
       "         -5.7876e-04, -4.6667e-03, -2.3913e-04, -2.9901e-03,  7.7248e-05,\n",
       "         -1.0523e-03,  2.7802e-03,  1.9217e-03, -3.2649e-03, -3.2117e-03,\n",
       "         -2.0859e-03,  1.9091e-03, -6.2656e-04, -8.3256e-04, -1.1421e-03,\n",
       "          1.5450e-03, -5.4284e-03,  2.4611e-03,  2.3401e-04,  2.7529e-03,\n",
       "         -4.1997e-04,  3.9780e-04, -5.2760e-03, -1.8369e-03, -3.1439e-03,\n",
       "          3.9256e-04, -2.4190e-03,  8.9836e-04,  1.4794e-04,  2.4021e-04,\n",
       "          1.0519e-03, -2.6941e-03,  9.4032e-04,  2.1635e-03, -1.0786e-03,\n",
       "         -5.5339e-03, -1.4185e-03,  1.2691e-03,  4.3428e-04, -3.8699e-03,\n",
       "         -1.0909e-03,  1.1443e-03, -6.3562e-04, -2.1571e-03,  6.8593e-04,\n",
       "         -1.8549e-04,  7.3230e-04,  1.5478e-03, -1.2810e-03, -4.3004e-03,\n",
       "         -2.0825e-03,  9.8300e-04,  4.5562e-04, -4.5832e-03, -1.2515e-03,\n",
       "          9.6345e-04, -3.2449e-04, -1.1230e-03, -2.9612e-04,  7.7474e-04,\n",
       "         -3.7596e-03,  2.1362e-04,  6.9976e-05, -2.8737e-03, -2.7397e-03,\n",
       "         -5.1361e-03, -1.2488e-03,  3.1435e-04, -6.8235e-04, -1.0643e-03,\n",
       "         -3.1852e-03,  1.3490e-03,  7.7558e-04,  4.3571e-04, -7.5316e-04,\n",
       "          1.1879e-03,  1.8003e-03,  6.7246e-04, -4.7231e-04, -1.3837e-03,\n",
       "          2.2054e-05,  2.5642e-04, -2.5879e-03, -1.7583e-03,  8.1980e-04,\n",
       "         -4.2045e-04,  9.2888e-04, -3.5298e-04, -1.3863e-03, -2.7670e-03,\n",
       "          2.0843e-03, -4.1232e-03,  3.1576e-03, -1.8913e-03,  5.7065e-04,\n",
       "         -7.9918e-04, -6.7663e-04, -5.8413e-05, -2.3282e-04, -2.5201e-04,\n",
       "          9.1791e-05,  1.2968e-03,  1.1265e-04,  2.7895e-05,  2.7680e-03,\n",
       "          5.4121e-05, -6.7902e-04, -2.6037e-03,  1.3791e-03, -9.1553e-05,\n",
       "         -1.3132e-03, -4.5550e-03, -1.0848e-04,  1.6463e-04, -5.2130e-04,\n",
       "         -3.7849e-04, -2.1207e-03, -3.7215e-03,  3.6120e-05,  7.3195e-04,\n",
       "          3.3475e-03,  7.8583e-04,  8.9228e-04, -2.5486e-03, -2.0696e-03,\n",
       "         -2.6393e-03, -2.0441e-03,  3.9732e-04,  1.7807e-03,  2.2045e-03,\n",
       "         -7.0429e-04,  3.2091e-04, -1.8600e-03,  5.0986e-04, -2.7418e-04,\n",
       "         -2.7107e-03, -2.5116e-03, -1.8907e-03,  1.3790e-03, -1.4752e-03,\n",
       "          1.4031e-04,  6.4766e-04, -6.1798e-04, -3.2607e-03,  4.0531e-05,\n",
       "          1.1182e-04, -9.3007e-04, -1.9501e-03,  4.4060e-04, -2.4478e-03,\n",
       "          8.2862e-04,  5.8889e-05, -1.5036e-03,  3.2043e-04, -2.5904e-04,\n",
       "          2.5249e-04,  2.2655e-03, -2.2464e-03, -8.8334e-04, -3.0254e-03,\n",
       "          2.0550e-03,  8.6689e-04,  1.3337e-03,  1.1425e-03,  8.3494e-04,\n",
       "          3.1102e-04,  1.3250e-03,  2.3183e-03,  3.2640e-04, -8.3423e-04,\n",
       "          1.8194e-03,  2.9758e-03,  4.5741e-04, -3.6113e-03,  1.5233e-03,\n",
       "         -5.4872e-04,  7.7856e-04, -7.2360e-05, -9.0253e-04,  8.1074e-04,\n",
       "          3.3648e-03, -3.0749e-03, -1.1888e-03, -3.5347e-03,  1.5407e-03,\n",
       "          1.2859e-03, -5.1697e-03,  7.6997e-04, -4.4560e-04, -6.7425e-04,\n",
       "         -5.7828e-03,  7.9298e-04, -1.5439e-03,  9.4962e-04, -4.1103e-04,\n",
       "         -2.5868e-05,  5.5814e-04,  1.1350e-03,  1.7014e-03, -5.8293e-05,\n",
       "         -1.3314e-03, -1.3046e-03, -2.4521e-03,  1.7581e-03,  1.0502e-03,\n",
       "          3.4368e-04, -1.1069e-03,  1.9437e-03, -9.2530e-04, -1.1650e-03,\n",
       "          7.1144e-04, -2.2173e-04, -1.0288e-03, -3.4122e-03, -1.7277e-03,\n",
       "         -6.4695e-04,  4.5300e-04, -3.2985e-04, -4.8196e-04,  1.6457e-03,\n",
       "          5.3871e-04, -6.7580e-04,  7.7939e-04, -6.3858e-03,  1.4927e-03,\n",
       "         -5.3813e-03, -3.1376e-04,  5.9986e-04, -6.0737e-04, -1.8163e-03,\n",
       "         -2.1625e-04,  1.3989e-03, -2.7678e-03, -3.2806e-04, -3.5930e-04,\n",
       "          3.0363e-04, -1.9888e-03, -1.5031e-03,  3.2641e-03, -3.5536e-04,\n",
       "          4.0519e-04,  9.3281e-04, -3.4548e-03, -5.3571e-03,  7.1383e-04,\n",
       "         -1.5129e-03, -1.6360e-03, -3.2985e-03,  3.1228e-03,  2.4559e-03,\n",
       "         -4.2582e-04,  3.0482e-04,  2.0802e-03,  3.6955e-03,  9.6548e-04,\n",
       "          4.0674e-04,  1.8146e-03, -1.0380e-03, -2.3628e-03, -2.4438e-05,\n",
       "         -1.1809e-03, -1.0080e-03, -6.9463e-04, -5.7387e-04, -1.7685e-03,\n",
       "          7.3791e-05, -3.7706e-04,  2.2781e-04, -3.3920e-03,  4.8614e-04,\n",
       "         -4.7958e-04,  4.8256e-04,  3.4094e-05,  1.4919e-03, -1.9556e-03,\n",
       "         -1.0474e-03, -1.3269e-03,  1.0703e-03, -1.8741e-03,  9.8097e-04,\n",
       "         -2.0388e-03, -4.3213e-04,  4.8327e-04, -5.0670e-03,  7.5436e-04,\n",
       "         -2.0826e-03,  1.0531e-03, -2.4409e-03, -1.6146e-03, -5.8329e-04,\n",
       "         -8.3244e-04, -3.0047e-03, -1.6494e-03, -2.4003e-03, -8.7130e-04,\n",
       "         -1.4147e-03,  4.3154e-05, -2.4080e-05,  5.3465e-04, -2.9645e-03,\n",
       "         -1.4980e-03, -2.9868e-03,  3.2353e-04,  4.7684e-07, -2.9796e-03,\n",
       "         -2.7688e-03, -1.4081e-03,  7.9155e-04, -4.6742e-04, -6.6590e-04,\n",
       "          3.9804e-04, -1.9252e-04,  1.3270e-03,  3.8159e-04, -2.6385e-03,\n",
       "          3.1935e-03, -3.8922e-04, -8.2386e-04, -3.6123e-03,  1.1004e-03,\n",
       "          7.0965e-04, -1.6713e-04,  1.3566e-04,  7.4232e-04,  3.7110e-04,\n",
       "         -3.1644e-03,  9.9897e-05, -4.9794e-04,  1.1462e-03,  1.3984e-03,\n",
       "          9.8073e-04, -5.0936e-03,  9.0468e-04, -1.4198e-03, -2.7096e-04,\n",
       "          9.0504e-04, -6.5695e-03, -1.7570e-03,  2.3047e-03,  7.3433e-04,\n",
       "         -9.3281e-04, -2.2546e-03,  8.9645e-04,  5.5456e-04, -4.1568e-04,\n",
       "          8.2433e-04,  2.5977e-03, -7.5781e-04,  1.0383e-04, -2.0593e-03,\n",
       "          6.1238e-04,  1.2537e-03,  7.0715e-04,  8.4925e-04,  2.4807e-04,\n",
       "          1.6983e-03, -5.8901e-04, -2.0367e-03, -4.0007e-04,  8.6403e-04,\n",
       "          1.8239e-05,  1.0362e-03,  1.0232e-03,  1.0382e-03, -3.3524e-03,\n",
       "         -1.3909e-03,  8.2922e-04]),\n",
       " tensor([-3.2258e-04, -8.2908e-05, -4.1115e-04,  3.0130e-04,  2.3030e-04,\n",
       "          7.9454e-04,  7.6922e-05, -5.5439e-04, -3.9547e-04, -4.9577e-04,\n",
       "          8.3895e-04,  2.6549e-05,  3.0212e-04, -7.3706e-04,  4.9698e-04,\n",
       "         -1.2339e-03, -4.4796e-04,  6.7063e-04,  1.0910e-04, -9.8763e-04,\n",
       "         -5.9605e-04, -4.5494e-04, -5.4502e-04,  1.5415e-03, -1.9403e-05,\n",
       "         -1.7248e-05, -2.7623e-05, -7.7158e-04, -1.9047e-05,  3.4970e-04,\n",
       "         -3.4120e-04,  5.3480e-04, -1.6572e-04, -5.9142e-04, -8.1822e-04,\n",
       "         -8.3577e-04, -2.4305e-04, -1.0690e-03, -2.8913e-04, -6.1616e-06,\n",
       "          1.8566e-04,  9.1289e-04, -3.6454e-04, -5.0941e-04, -4.6344e-04,\n",
       "          4.7796e-04, -4.1652e-05,  2.4141e-04,  5.0057e-05,  5.0127e-04,\n",
       "         -5.5532e-04,  5.8607e-04, -2.8322e-04,  1.9807e-05, -1.1635e-03,\n",
       "          2.4464e-04, -1.3853e-03, -3.1376e-04,  5.2028e-04, -2.6155e-04,\n",
       "         -3.0099e-04, -1.2447e-03,  2.1008e-04, -9.0631e-05,  6.3131e-04,\n",
       "         -6.3511e-04, -4.1302e-04,  4.0230e-04,  6.1062e-04,  6.7282e-04,\n",
       "          8.0939e-04, -7.9948e-04,  5.8189e-04, -1.3386e-03,  5.9628e-04,\n",
       "         -3.6201e-04,  1.9250e-05,  4.6989e-04,  5.5520e-04, -1.3322e-04,\n",
       "         -6.6775e-04, -4.3665e-04, -6.8799e-05, -1.5130e-03, -1.3222e-04,\n",
       "         -4.6717e-04,  5.6665e-04, -9.4634e-05,  2.2412e-04, -4.7386e-04,\n",
       "          2.4225e-04,  1.7018e-04, -5.0237e-04, -8.3665e-04, -2.0580e-05,\n",
       "         -5.2642e-05, -1.4656e-03, -4.8177e-04, -8.0832e-04, -7.5709e-04,\n",
       "          9.9577e-04, -8.5995e-04, -1.5643e-03, -3.6938e-04, -2.7201e-04,\n",
       "          3.8020e-04,  2.9668e-04,  1.0163e-03,  1.1920e-04, -5.4910e-04,\n",
       "          3.3961e-05,  7.2694e-04,  1.9584e-05, -2.2681e-04, -2.2770e-04,\n",
       "         -1.9931e-04,  9.5747e-05,  3.0761e-04, -1.0097e-03, -6.2952e-04,\n",
       "          4.5916e-04,  9.8978e-04, -6.3062e-04, -2.9644e-04, -1.7416e-03,\n",
       "         -5.8562e-04, -3.3849e-04, -1.3182e-04,  8.7833e-04,  3.3928e-05,\n",
       "         -9.1843e-04, -5.1310e-04, -1.0029e-03, -5.8481e-04, -1.0950e-04,\n",
       "          4.4733e-04,  7.1801e-04,  1.6694e-04, -8.6221e-05,  1.8588e-04,\n",
       "         -7.5752e-04,  1.1362e-04,  6.8565e-04, -1.0020e-03, -8.2586e-04,\n",
       "          6.6752e-04, -3.9336e-04, -1.3304e-04,  2.2149e-04,  3.6989e-04,\n",
       "          2.5656e-04,  1.6746e-04, -8.7978e-04, -4.3289e-04, -5.3607e-06,\n",
       "         -1.3780e-03,  1.0678e-04, -1.7112e-04, -3.5877e-04,  1.3097e-03,\n",
       "         -2.5014e-04, -9.9937e-04,  2.0109e-04,  6.7105e-04, -1.0596e-03,\n",
       "          8.7290e-04,  1.3211e-03,  1.8889e-04, -5.1036e-05,  1.2458e-04,\n",
       "         -7.2894e-04,  2.8004e-04,  3.8857e-04,  5.7251e-04,  9.3394e-04,\n",
       "         -4.9006e-06,  4.7793e-04,  2.8399e-04,  1.4715e-06, -2.5369e-04,\n",
       "          2.5450e-04,  4.3333e-04,  4.3491e-04,  1.0937e-03, -1.8573e-03,\n",
       "         -6.5439e-04,  9.4630e-05,  4.6541e-04, -8.7276e-05,  2.3968e-05,\n",
       "          5.4730e-04, -6.9433e-04, -3.6523e-05,  4.2352e-04, -1.5972e-04,\n",
       "         -3.1195e-04,  4.5723e-04, -3.9975e-04, -8.9692e-04,  6.5186e-05,\n",
       "         -1.2248e-03,  2.3651e-04,  1.3339e-03, -9.0118e-04,  2.3847e-04,\n",
       "         -7.7152e-04, -6.8648e-04, -1.3075e-04, -6.9089e-05, -1.6908e-05,\n",
       "          6.5051e-04, -1.7367e-04,  8.0973e-04, -1.3413e-04, -5.6309e-04,\n",
       "         -1.1990e-04, -6.9245e-04, -4.8931e-04, -3.6046e-04,  5.5230e-04,\n",
       "         -2.0491e-04,  5.1146e-04, -4.5020e-05,  1.6436e-04, -5.7688e-04,\n",
       "         -6.1288e-04, -4.0883e-04, -1.8012e-03, -5.7345e-04, -3.6486e-04,\n",
       "         -5.2496e-04, -7.4355e-04, -1.5229e-05,  9.2459e-04,  1.3516e-04,\n",
       "          1.5634e-04,  7.0471e-05, -4.2889e-04,  6.9766e-04,  2.5884e-04,\n",
       "         -1.1148e-03,  1.3293e-03,  7.7553e-05, -2.0979e-04, -1.4664e-03,\n",
       "         -2.9534e-05, -1.0423e-03,  7.9686e-04,  4.1084e-04, -6.6199e-04,\n",
       "         -5.0643e-04, -1.3530e-04,  1.0480e-03,  2.4905e-04,  6.8278e-04,\n",
       "         -3.4903e-04, -9.1735e-05,  2.8887e-04,  2.1917e-04, -7.7144e-04,\n",
       "          5.1568e-04, -7.5329e-04, -2.0541e-05, -8.3130e-06, -6.6422e-04,\n",
       "         -1.8604e-04, -1.1258e-03, -8.1933e-04, -5.6961e-04, -4.0039e-04,\n",
       "         -5.1743e-04,  5.8460e-04,  5.1692e-04, -9.8643e-04,  4.1685e-04,\n",
       "         -1.1461e-03,  5.7610e-04,  2.8084e-04, -8.9129e-05, -9.7703e-04,\n",
       "         -6.2609e-04,  4.9403e-05, -5.7976e-04,  2.0816e-03,  5.9773e-04,\n",
       "          1.4538e-04,  9.8827e-04, -6.0961e-04, -1.7190e-05, -2.5051e-04,\n",
       "          9.1482e-05, -4.4964e-05,  1.8252e-04,  2.1638e-04,  3.9952e-05,\n",
       "          1.7974e-04,  8.9086e-05,  1.6815e-03, -1.3275e-03, -1.9816e-04,\n",
       "          2.8783e-04, -1.0501e-03, -6.6105e-04, -1.3770e-04, -7.6342e-04,\n",
       "         -7.7143e-05,  1.0008e-04, -1.0765e-03,  1.4061e-04,  2.1260e-05,\n",
       "          2.2955e-04, -1.6755e-03,  2.8926e-04,  1.0483e-04, -3.4591e-04,\n",
       "         -2.1491e-06,  4.6598e-04, -6.1842e-04, -1.4507e-04,  5.9532e-04,\n",
       "          3.5455e-05, -6.0607e-04, -1.2700e-03, -5.5242e-04,  5.9755e-04,\n",
       "         -5.0569e-04, -1.0657e-03,  3.5831e-04, -1.1595e-03, -7.0248e-05,\n",
       "          8.4097e-05, -8.4091e-05,  4.7760e-04, -2.1446e-04, -3.1114e-04,\n",
       "         -8.5998e-04, -8.8234e-06,  2.6079e-04, -5.2034e-04, -9.4848e-05,\n",
       "         -5.6972e-04, -7.0119e-04,  1.2173e-03,  3.6786e-05,  5.5961e-04,\n",
       "         -2.5572e-04,  4.8824e-04,  4.0386e-04, -2.9795e-04,  2.1733e-04,\n",
       "         -1.2085e-04, -3.1864e-05, -7.6022e-04, -3.0766e-04, -5.5861e-05,\n",
       "          2.1545e-04,  3.0249e-04,  7.4569e-05, -2.0333e-04,  6.2065e-04,\n",
       "          9.1117e-05,  3.1259e-04, -7.7762e-05, -2.5105e-04, -1.1132e-03,\n",
       "          3.3696e-04, -1.1480e-04,  7.4773e-04,  8.6799e-07, -1.0742e-03,\n",
       "         -2.8863e-04,  3.6477e-04, -1.4285e-04, -6.0506e-05, -5.3820e-04,\n",
       "          5.2906e-04,  5.5631e-04,  4.8816e-04, -1.7119e-05,  1.0455e-03,\n",
       "         -5.1572e-04,  1.8456e-04,  1.4089e-04, -7.9191e-04, -2.3416e-04,\n",
       "          1.2954e-04, -6.4208e-04, -1.9735e-04, -2.6399e-04,  5.7885e-04,\n",
       "         -1.0101e-03, -1.6711e-04,  1.2252e-04, -1.0612e-04,  1.2284e-04,\n",
       "         -5.7841e-04,  2.5998e-04, -2.6046e-04, -3.2714e-04,  1.5988e-04,\n",
       "         -7.4042e-04, -6.8698e-04,  6.6870e-04,  8.4472e-05, -1.0974e-04,\n",
       "         -4.3607e-04,  3.3876e-04,  2.3377e-04,  1.6550e-04, -8.9188e-04,\n",
       "         -2.3214e-05,  4.6388e-04, -1.2938e-03, -5.1827e-04,  3.8683e-04,\n",
       "         -2.7357e-04, -3.4749e-04, -1.1754e-03, -7.1966e-04, -4.7537e-04,\n",
       "         -5.6420e-04,  2.5852e-04,  6.1048e-04,  2.1144e-04, -2.7905e-04,\n",
       "         -1.2630e-04, -3.1346e-05, -1.8721e-04, -9.2570e-04,  1.0455e-03,\n",
       "         -4.0810e-04, -2.6159e-05, -9.8737e-04, -9.7458e-05, -4.7209e-05,\n",
       "          3.1387e-04,  7.5622e-05, -1.2113e-03, -1.7870e-04, -6.0125e-04,\n",
       "          1.2523e-03,  8.2043e-04,  5.1812e-04, -5.1792e-04, -7.7130e-04,\n",
       "         -6.2530e-04, -4.8133e-04,  4.7897e-04,  6.5509e-04, -8.9822e-04,\n",
       "          2.9411e-03, -4.5617e-04,  1.7144e-04,  7.2712e-05,  5.1922e-04,\n",
       "          5.7197e-04,  4.8729e-04, -3.7841e-04, -3.0774e-04, -1.8911e-04,\n",
       "         -6.8362e-04, -5.3486e-04, -3.5877e-04, -9.9107e-04, -2.5772e-04,\n",
       "          3.3458e-04,  6.5085e-04, -8.7719e-04,  5.5327e-04,  4.4696e-05,\n",
       "         -1.1064e-06,  3.1928e-04,  4.4348e-04, -9.8241e-05, -3.2001e-04,\n",
       "          5.3469e-04, -1.6879e-04, -1.4943e-04, -7.7624e-04, -3.6396e-04,\n",
       "          1.1366e-03, -1.1861e-03,  5.6620e-04,  1.2259e-04, -3.2104e-04,\n",
       "          5.0362e-05,  9.2034e-04, -2.6159e-04,  9.0409e-05,  5.0144e-05,\n",
       "         -5.2273e-04, -6.6462e-04, -1.9785e-04, -6.9017e-05, -2.4829e-04,\n",
       "          5.8115e-04, -9.0660e-05,  1.1836e-03,  7.3673e-04, -1.8881e-04,\n",
       "         -6.6965e-04, -6.8758e-04, -8.6615e-04, -6.2192e-04, -4.2845e-04,\n",
       "         -5.2221e-05,  8.5558e-04, -5.7923e-04,  3.2085e-04,  7.5804e-04,\n",
       "          5.4331e-04, -5.4282e-04])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diffs(arg=\"encoder.blocks.0.attn.qkv.weight\"):\n",
    "  f = lambda trg: checkpoint['state_dict'][f'{trg}{arg}']\n",
    "  return f(\"target_\") - f(\"\")\n",
    "\n",
    "[diffs(x) for x in list(filter(lambda k: k.startswith(\"encoder\"), checkpoint['state_dict'].keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517eec47",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EEGPTClassifier.__init__() missing 1 required positional argument: 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mEEGPTClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.load_state_dict(checkpoint, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: EEGPTClassifier.__init__() missing 1 required positional argument: 'num_classes'"
     ]
    }
   ],
   "source": [
    "EEGPTClassifier().load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-eegpt-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "CHECKPOINT_PATH = Path('./model_checkpoints/25866970/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt')\n",
    "\n",
    "# Standard 58-channel EEG montage (subset of CHANNEL_DICT)\n",
    "USE_CHANNELS = list(CHANNEL_DICT.keys())[:58]\n",
    "print(f'Using {len(USE_CHANNELS)} channels: {USE_CHANNELS[:10]}...')\n",
    "\n",
    "# Initialize model for feature extraction (num_classes=0)\n",
    "model = EEGPTClassifier(\n",
    "    num_classes=0,  # For feature extraction only\n",
    "    in_channels=58,\n",
    "    img_size=[58, 2000],  # 58 channels, 2000 time points\n",
    "    use_channels_names=USE_CHANNELS,\n",
    "    use_chan_conv=False\n",
    ")\n",
    "\n",
    "# Load pretrained weights\n",
    "try:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "    # Handle different checkpoint formats\n",
    "    if 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    elif 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Load with non-strict mode (some keys might not match)\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    print(f'✅ Model loaded successfully!')\n",
    "    if missing_keys:\n",
    "        print(f'Missing keys: {len(missing_keys)} (classification head expected)')\n",
    "    if unexpected_keys:\n",
    "        print(f'Unexpected keys: {len(unexpected_keys)}')\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print('❌ Checkpoint not found. Please download the pretrained model.')\n",
    "    print(f'Expected location: {CHECKPOINT_PATH}')\n",
    "    model = None\n",
    "except Exception as e:\n",
    "    print(f'❌ Error loading checkpoint: {e}')\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy EEG data (batch_size=2, channels=58, samples=2000)\n",
    "    # Represents ~8 seconds of EEG at 250Hz sampling rate\n",
    "    dummy_eeg = torch.randn(2, 58, 2000)  # Random EEG-like data\n",
    "    print(f'Input shape: {dummy_eeg.shape}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Extract features using forward_features (not classification)\n",
    "        features = model.forward_features(dummy_eeg)\n",
    "        print(f'Feature tensor shape: {features.shape}')\n",
    "        print(f'Feature range: [{features.min():.4f}, {features.max():.4f}]')\n",
    "        print(f'First few feature values: {features[0, 0, :5].numpy()}')\n",
    "    \n",
    "    print('🎯 Model ready for feature extraction!')\n",
    "else:\n",
    "    print('⚠️  Model not available - cannot test features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-connection",
   "metadata": {},
   "source": [
    "## Connection to Music Decoding Architecture\n",
    "\n",
    "The EEGPT model serves as **Model A** in our neural music decoding pipeline:\n",
    "\n",
    "```\n",
    "EEG Signal → [EEGPT Feature Extractor] → EEG Features → [Diffusion Model] → Audio\n",
    "   (58, T)              Model A               (?, D)        Model B        (1, T_audio)\n",
    "```\n",
    "\n",
    "### Integration Requirements:\n",
    "\n",
    "1. **Feature Conditioning**: The output features need projection/reshaping to match the diffusion model's conditioning requirements\n",
    "2. **Temporal Alignment**: EEG features must be temporally aligned with target audio segments\n",
    "3. **Cross-Modal Bridge**: May need learned projection layers to map EEG semantic space to audio semantic space\n",
    "\n",
    "### Useful Data Processing from EEGPT:\n",
    "\n",
    "- **Channel Standardization**: `CHANNEL_DICT` mapping for consistent electrode ordering\n",
    "- **Temporal Interpolation**: Resample EEG to consistent lengths\n",
    "- **Patchification**: Convert time series to patch-based representations\n",
    "- **Voltage Scaling**: Standardized µV ↔ V conversions\n",
    "\n",
    "These preprocessing utilities can be adapted for our music-listening datasets (BCMI, NMED-T, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your-project-name",
   "language": "python",
   "name": "your-project-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
